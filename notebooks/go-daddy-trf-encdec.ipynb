{
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.7.12",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  }
 },
 "nbformat_minor": 4,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "markdown",
   "source": "# Setup\n",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": "import math\nimport os\n\nimport numpy as np\nimport torch\n#\nROOT_DIR=os.path.dirname(\"/kaggle/working\")\nDATA_DIR=os.path.join(\"/kaggle/input/godaddy/godaddy-microbusiness-density-forecasting\") ##Directory of dataset\n# ROOT_DIR=os.path.join(\".\",\"..\")\n# DATA_DIR=os.path.join(ROOT_DIR,\"data\",\"godaddy-microbusiness-density-forecasting\") ##Directory of dataset\n\nEXPERIMENTS_DIR=os.path.join(ROOT_DIR, \"logs/experiments\")\n\n\n# Copy pretrained model\nimport shutil\nsrc_dir=\"/kaggle/input/local-trt-seq-5/trf_ae_dv_ed.128_dce.4_nl.6_nh.8_df.512_sl.15_ss.1_lr.0.0001_bs.16_do.0.0\"\nshutil.copytree(src_dir,os.path.join(EXPERIMENTS_DIR,os.path.basename(src_dir)))\n\n\nuse_cuda = torch .cuda.is_available()\nDEVICE = torch.device(\"cuda\" if use_cuda else \"cpu\")\n\nCENSUS_FEATURES = ['pct_bb', 'pct_college', 'pct_foreign_born', 'pct_it_workers', 'median_hh_inc','active']\nN_CENSUS_FEATURES= len(CENSUS_FEATURES) #pct_bb,pct_college,pct_foreign_born,pct_it_workers,median_hh_inc,active\n#cfips is not considered as a feature we use a one-hot encoding for it\n\n\nUSE_CENSUS= False #Without census features\n\nAE_LATENT_DIM= 32\n\nLSTM_HIDDEN_DIM = 8\n\nSEQ_LEN=6\nSEQ_STRIDE= 1\n\nN_COUNTY=3142\nN_DIMS_COUNTY_ENCODING=  math.ceil(math.log(N_COUNTY,2))\n\nFEATURES_AE_CENSUS_DIR=os.path.join(EXPERIMENTS_DIR, \"features_ae_2_dims\")\nFEATURES_AE_LATENT_DIM= 2\n\nTRAIN_FILE= os.path.join(DATA_DIR, \"train.csv\")\nTEST_FILE= os.path.join(DATA_DIR, \"test.csv\")\n\nCENSUS_FILE =os.path.join(DATA_DIR, \"census_interpolated.csv\")\n\nNB_FUTURES= 10 #Number of days to predict\n\n\n#Scaling factors for microbusiness density\nMEAN_MB= 3.817671\nSTD_MB= 4.991087\n\nMAX_MB= 300\nMIN_MB= 0.0",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "execution": {
     "iopub.status.busy": "2023-02-21T20:40:02.159485Z",
     "iopub.execute_input": "2023-02-21T20:40:02.159984Z",
     "iopub.status.idle": "2023-02-21T20:40:05.380020Z",
     "shell.execute_reply.started": "2023-02-21T20:40:02.159878Z",
     "shell.execute_reply": "2023-02-21T20:40:05.378185Z"
    },
    "trusted": true
   },
   "execution_count": 1,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "",
   "metadata": {
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Utils",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import json\n",
    "import os\n",
    "from enum import Enum\n",
    "from itertools import islice\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import logging\n",
    "import os\n",
    "import sys\n",
    "from time import strftime\n",
    "def setup_logger(args):\n",
    "    formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "    a_logger = logging.getLogger()\n",
    "    a_logger.setLevel(args.log_level)\n",
    "    log_dir=os.path.join(ROOT_DIR,\"logs\",\"output_logs\")\n",
    "    if not os.path.exists(log_dir):\n",
    "        os.makedirs(log_dir)\n",
    "    output_file_handler = logging.FileHandler(os.path.join(log_dir,strftime(\"log_%d_%m_%Y_%H_%M.log\")))\n",
    "    stdout_handler = logging.StreamHandler(sys.stdout)\n",
    "    stdout_handler.setFormatter(formatter)\n",
    "    a_logger.propagate=False\n",
    "    a_logger.addHandler(output_file_handler)\n",
    "    a_logger.addHandler(stdout_handler)\n",
    "\n",
    "def read_json(path_json):\n",
    "    with open(path_json, encoding='utf8') as json_file:\n",
    "        return json.load(json_file)\n",
    "def softmax(x):\n",
    "    return np.exp(x) / np.sum(np.exp(x))\n",
    "def chunks(data, SIZE):\n",
    "    \"\"\"Split a dictionnary into parts of max_size =SIZE\"\"\"\n",
    "    it = iter(data)\n",
    "    for _ in range(0, len(data), SIZE):\n",
    "        yield {k: data[k] for k in islice(it, SIZE)}\n",
    "\n",
    "def sorted_dict(x, ascending=True):\n",
    "    \"\"\"\n",
    "    Sort dict according to value.\n",
    "    x must be a primitive type: int,float, str...\n",
    "    @param x:\n",
    "    @return:\n",
    "    \"\"\"\n",
    "    return dict(sorted(x.items(), key=lambda item: (1 if ascending else -1) * item[1]))\n",
    "def reverse_dict(input_dict):\n",
    "    \"\"\"\n",
    "    Reverse a dictonary\n",
    "    Args:\n",
    "        input_dict:\n",
    "\n",
    "    Returns:\n",
    "\n",
    "    \"\"\"\n",
    "    inv_dict = {}\n",
    "    for k, v in input_dict.items():\n",
    "        inv_dict[v] = inv_dict.get(v, []) + [k]\n",
    "\n",
    "    return inv_dict\n",
    "\n",
    "def save_matrix(matrix,filename):\n",
    "    with open(filename,'wb') as output:\n",
    "        np.save(output,matrix)\n",
    "def load_matrix(filename,auto_delete=False):\n",
    "    with open(filename,'rb') as input:\n",
    "        matrix=np.load(input)\n",
    "\n",
    "    if auto_delete:\n",
    "        os.remove(filename)\n",
    "    return matrix\n",
    "\n",
    "\n",
    "\n",
    "class Averager:\n",
    "    def __init__(self):\n",
    "        self.current_total = 0.0\n",
    "        self.iterations = 0.0\n",
    "\n",
    "    def send(self, value):\n",
    "        self.current_total += value\n",
    "        self.iterations += 1\n",
    "\n",
    "    @property\n",
    "    def value(self):\n",
    "        if self.iterations == 0:\n",
    "            return 0\n",
    "        else:\n",
    "            return 1.0 * self.current_total / self.iterations\n",
    "\n",
    "    def reset(self):\n",
    "        self.current_total = 0.0\n",
    "        self.iterations = 0.0\n",
    "\n",
    "\n",
    "\n",
    "from enum import Enum\n",
    "class DatasetType(Enum):\n",
    "    TRAIN=\"train\"\n",
    "    VALID=\"valid\"\n",
    "    TEST=\"test\"\n",
    "\n",
    "\n",
    "def extract_census_features(row, cfips_index ,single_row=True):\n",
    "    \"\"\"\n",
    "\n",
    "    @param row: Row of the dataframe\n",
    "    @param cfips_index: Index of cfips\n",
    "    @return:\n",
    "    \"\"\"\n",
    "    ##If series :\n",
    "    if single_row:\n",
    "        features_tensor = torch.tensor( [\n",
    "                                        row[CENSUS_FEATURES[0]],\n",
    "                                        row[CENSUS_FEATURES[1]],\n",
    "                                        row[CENSUS_FEATURES[2]],\n",
    "                                        row[CENSUS_FEATURES[3]],\n",
    "                                        row[CENSUS_FEATURES[4]],\n",
    "                                        row[CENSUS_FEATURES[5]]\n",
    "                                        ], dtype=torch.float32)\n",
    "\n",
    "        cfips= row['cfips']\n",
    "        cfips_one_hot = get_cfips_encoding(cfips, cfips_index)\n",
    "        # Min-max normalization\n",
    "        features_tensor[0] = (features_tensor[0] - 24.5) / (97.6 - 24.5)\n",
    "        features_tensor[1] = (features_tensor[1] / 48)\n",
    "        features_tensor[2] = (features_tensor[2] / 54)\n",
    "        features_tensor[3] = (features_tensor[3] / 17.4)\n",
    "        features_tensor[4] = (features_tensor[4] - 17109) / (1586821 - 17109)\n",
    "        features_tensor[5] = (features_tensor[5] / 1167744)\n",
    "\n",
    "    else :\n",
    "        features_tensor= torch.from_numpy(row[CENSUS_FEATURES].values)\n",
    "        cfips= row['cfips'].iloc[0]\n",
    "        cfips_one_hot= get_cfips_encoding(cfips,cfips_index).repeat(features_tensor.shape[0],1)\n",
    "        # cfips_one_hot = torch.stack(row_one_hots)\n",
    "        #Min-max normalization\n",
    "        features_tensor[:,0] = (features_tensor[:,0]- 24.5)/ (97.6-24.5)\n",
    "        features_tensor[:,1] = (features_tensor[:,1] /48)\n",
    "        features_tensor[:,2] = (features_tensor[:,2]/ 54)\n",
    "        features_tensor[:,3] = (features_tensor[:,3] / 17.4)\n",
    "        features_tensor[:,4] = (features_tensor[:,4]- 17109)/(1586821-17109)\n",
    "        features_tensor[:,5] = (features_tensor[:,5]/1167744)\n",
    "\n",
    "    ##Add one-hot encoding of cfips\n",
    "    if single_row:\n",
    "        features_tensor = torch.cat((cfips_one_hot, features_tensor))\n",
    "    else:\n",
    "        features_tensor = torch.cat((cfips_one_hot,features_tensor), 1)\n",
    "\n",
    "    return features_tensor.float()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def get_cfips_index():\n",
    "    \"\"\"\n",
    "    Return a dictionary with key=cfips and value=index for using a one-hot encoding\n",
    "    \"\"\"\n",
    "    df= pd.read_csv(os.path.join(DATA_DIR, \"census_interpolated.csv\"))\n",
    "    cfips = df['cfips'].unique()\n",
    "    cfips.sort()\n",
    "    #Sort cfips\n",
    "    return {cfips[i]: i for i in range(len(cfips))}\n",
    "\n",
    "\n",
    "def get_cfips_encoding(cfips,cfips_index):\n",
    "    \"\"\"\n",
    "     return the base 2 encoding of cfips\n",
    "    \"\"\"\n",
    "\n",
    "    #n_dims is the number of bits needed to represent the cfips\n",
    "\n",
    "    bin_index=np.binary_repr(cfips_index[cfips],width=N_DIMS_COUNTY_ENCODING)\n",
    "    enc = torch.tensor([int(x) for x in bin_index],dtype=torch.float32)\n",
    "    return enc\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "execution": {
     "iopub.status.busy": "2023-02-21T20:40:05.383086Z",
     "iopub.execute_input": "2023-02-21T20:40:05.383914Z",
     "iopub.status.idle": "2023-02-21T20:40:05.424370Z",
     "shell.execute_reply.started": "2023-02-21T20:40:05.383861Z",
     "shell.execute_reply": "2023-02-21T20:40:05.422418Z"
    },
    "trusted": true
   },
   "execution_count": 2,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "# Network",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": "import json\nimport logging\nimport os\n\nimport numpy as np\nimport torch\nfrom torch import nn\n\nclass PositionalEncoding(nn.Module):\n    def __init__(self, d_model, max_len=5000):\n        # Sin/Cos positional encoding\n        super().__init__()\n        self.d_model = d_model\n        self.max_len = max_len\n        self.pe = torch.zeros(self.max_len, self.d_model)\n        position = torch.arange(0, self.max_len, dtype=torch.float).unsqueeze(1)\n        div_term = torch.exp(torch.arange(0, self.d_model, 2).float() * (-np.log(10000.0) / self.d_model))\n        self.pe[:, 0::2] = torch.sin(position * div_term)\n        self.pe[:, 1::2] = torch.cos(position * div_term)\n        self.pe = self.pe.unsqueeze(0).transpose(0, 1).squeeze(1)\n        # COnvert to nn.Parameter\n        self.pe = nn.Parameter(self.pe, requires_grad=False)\n\n\n\n    def forward(self, x):\n        # Add positional encoding to the input (Pay attention to the dimensions (the pe does not have the batch dimension))\n        x = x + self.pe[:x.size(1), :]\n        return x\n\n\nclass TransformerPredictor(nn.Module):\n\n    def __init__(self,\n                 emb_dim=32,\n                 n_layers=3,\n                 n_head=8,\n                 max_seq_len=100,\n                 dim_feedforward=128,\n                 use_derivative=True,\n                 use_census=USE_CENSUS,\n                 census_emb_dim=4,\n                 dropout_rate=0.01,\n                 experiment_dir=\"my_model\", reset=False, load_best=True):\n        \"\"\"\n        @param features_encoder :\n        @param input_dim:\n        @param hidden_dim:\n        @param ues_encoder:²\n        @param experiment_dir:\n        @param reset:\n        @param load_best:\n        \"\"\"\n\n        super(TransformerPredictor, self).__init__()\n        self.variante_num = 4\n        self.emb_dim = emb_dim\n        self.n_layers = n_layers\n        self.n_head = n_head\n        self.dim_feedforward = dim_feedforward\n        self.use_census = use_census\n        self.max_seq_len = max_seq_len\n        self.census_features_encoder = None\n        self.census_emb_dim = census_emb_dim\n        self.input_dim = 1\n        self.use_derivative = use_derivative\n        self.dropout_rate = dropout_rate\n        if self.use_derivative:\n            self.input_dim += 2  # 2 for the first order derivatives\n            self.input_dim += 2  # 2 for the second order derivatives\n\n        if self.use_census:\n            self.input_dim = self.input_dim + self.census_emb_dim\n\n        self.experiment_dir = experiment_dir\n        self.model_name = os.path.basename(self.experiment_dir)\n        self.reset = reset\n        self.load_best = load_best\n        self.setup_dirs()\n        self.setup_network()\n\n\n        if not reset: self.load_state()\n\n    def format_model_name(use_census, use_derivative, emb_dim, census_emb_dim, n_layers, n_head, dim_feedforward, seq_len, seq_stride, learning_rate, batch_size, dropout_rate):\n        model_name = f\"trf_{'ae_' if use_census else ''}{'dv_' if use_derivative else ''}ed.{emb_dim}{'_dce.' + str(census_emb_dim) if use_census else ''}_nl.{n_layers}_nh.{n_head}_df.{dim_feedforward}_sl.{seq_len}_ss.{seq_stride}_lr.{learning_rate}_bs.{batch_size}_do.{dropout_rate}\"\n        return model_name\n\n    def build_from_config(config, experiment_dir, reset=False, load_best=True):\n        return TransformerPredictor(\n            emb_dim=config[\"emb_dim\"],\n            n_layers=config[\"n_layers\"],\n            n_head=config[\"n_head\"],\n            max_seq_len=config[\"seq_len\"]-1,\n            dim_feedforward=config[\"dim_feedforward\"],\n            use_derivative=config[\"use_derivative\"],\n            use_census=config[\"use_census\"],\n            dropout_rate=config[\"dropout_rate\"],\n            census_emb_dim=config[\"census_emb_dim\"],\n            reset=reset,\n            load_best=load_best,\n            experiment_dir=experiment_dir,\n        )\n\n\n\n    ##1. Defining network architecture\n    def setup_network(self):\n        \"\"\"\n        Initialize the network  architecture here\n        @return:\n        \"\"\"\n        # Input encoder from self.input_dim to self.emb_dim along with positional encoding\n        if self.use_census:\n            self.query_encoder = nn.Sequential(nn.Linear(N_DIMS_COUNTY_ENCODING + N_CENSUS_FEATURES, self.emb_dim))\n            self.census_features_encoder = nn.Sequential(\n                nn.Linear(N_CENSUS_FEATURES, self.census_emb_dim),\n            )\n\n        self.input_embedding = nn.Sequential(\n            nn.Linear(self.input_dim, self.emb_dim),\n        )\n\n        ##Positional encoding\n        self.positional_encoding = PositionalEncoding(self.emb_dim, max_len=self.max_seq_len)\n        self.dropout = nn.Dropout(p=self.dropout_rate)\n        self.transformer_encoder = nn.TransformerEncoder(\n            nn.TransformerEncoderLayer(d_model=self.emb_dim, nhead=self.n_head, dim_feedforward=self.dim_feedforward,\n                                       batch_first=True,dropout=0.05),\n            num_layers=self.n_layers\n        )\n        self.transformer_decoder = nn.TransformerDecoder(\n            nn.TransformerDecoderLayer(d_model=self.emb_dim, nhead=self.n_head, dim_feedforward=self.dim_feedforward,\n                                       batch_first=True,dropout=0.05),\n            num_layers=self.n_layers,\n        )\n\n\n\n        self.regressor = nn.Sequential(\n            nn.Linear(2*self.emb_dim, 2048),\n            nn.ReLU(),\n            self.dropout,\n            nn.Linear(2048,  1)\n        )\n        if self.use_census:\n            pass\n\n        else:\n            self.regressor = nn.Sequential(\n                nn.Linear(self.emb_dim, 1)\n            )\n\n        self.config={\n            \"use_census\":self.use_census,\n            \"use_derivative\":self.use_derivative,\n            \"emb_dim\":self.emb_dim,\n            \"census_emb_dim\":self.census_emb_dim,\n            \"n_layers\":self.n_layers,\n            \"n_head\":self.n_head,\n            \"dim_feedforward\":self.dim_feedforward,\n            \"seq_len\":self.max_seq_len+1,\n            \"dropout_rate\":self.dropout_rate,\n            \"experiment_dir\":self.experiment_dir,\n            \"variante\":self.variante_num,\n            \"input_dim\":self.input_dim,\n        }\n\n    ##2. Model Saving/Loading\n    def load_state(self, best=False):\n        \"\"\"\n        Load model\n        :param self:\n        :return:\n        \"\"\"\n        if best and os.path.exists(self.save_best_file):\n            logging.info(f\"Loading best model state : {self.save_file}\")\n            self.load_state_dict(torch.load(self.save_file, map_location=DEVICE))\n            return\n\n        if os.path.exists(self.save_file):\n            logging.info(f\"Loading model state : {self.save_file}\")\n            self.load_state_dict(torch.load(self.save_file, map_location=DEVICE))\n\n    def save_state(self, best=False):\n        if best:\n            logging.info(\"Saving best model\")\n            torch.save(self.state_dict(), self.save_best_file)\n        torch.save(self.state_dict(), self.save_file)\n\n    ##3. Setupping directories for weights /logs ... etc\n    def setup_dirs(self):\n        \"\"\"\n        Checking and creating directories for weights storage\n        @return:\n        \"\"\"\n        self.save_file = os.path.join(self.experiment_dir, f\"{self.model_name}.pt\")\n        self.save_best_file = os.path.join(self.experiment_dir, f\"{self.model_name}_best.pt\")\n        if not os.path.exists(self.experiment_dir):\n            os.makedirs(self.experiment_dir)\n\n    # 4. Forward call\n    def forward(self, X_input):\n        \"\"\"\n        +Forward call here.\n        It a time series, so we need the full sequence output (strided by 1)\n        @param X:\n        @return:\n        \"\"\"\n        # 0. Preparing the input (Removing the target from the input)\n        X = X_input[:, :-1, :]  # Removing the target from the input (Only required when using census features)\n\n        if self.use_derivative:\n\n            #1. First order derivative\n            d_left = torch.zeros((X.shape[0], X.shape[1], 1), device=DEVICE)\n            d_left[:, 1:, -1] = X[:, 1:, -1] - X[:, :-1, -1]\n\n            d_right = torch.zeros((X.shape[0], X.shape[1], 1), device=DEVICE)\n            d_right[:, :-1, -1] = X[:, 1:, -1] - X[:, :-1, -1]\n\n            #2. Second order derivative\n            d2_left = torch.zeros((X.shape[0], X.shape[1], 1), device=DEVICE)\n            d2_left[:, 2:, -1] = d_left[:, 2:, -1] - d_left[:, 1:-1, -1]\n\n            d2_right = torch.zeros((X.shape[0], X.shape[1], 1), device=DEVICE)\n            d2_right[:, :-2, -1] = d_left[:, 2:, -1] - d_left[:, 1:-1, -1]\n\n\n\n            X = torch.cat((X, d_left,  d_right,d2_left, d2_right), dim=-1)  ## Adding the derivative to the input as a new feature\n\n        if self.use_census:\n            target = X[:, -1, :]  # Last element of the sequence is the target .\n            query = self.query_encoder(target[:, :N_DIMS_COUNTY_ENCODING + N_CENSUS_FEATURES])\n\n            enc_census = self.census_features_encoder(\n                X[:, :, N_DIMS_COUNTY_ENCODING:N_CENSUS_FEATURES + N_DIMS_COUNTY_ENCODING])\n            X = torch.cat((X[:, :, N_CENSUS_FEATURES + N_DIMS_COUNTY_ENCODING:], enc_census), dim=-1)\n\n        # 2. Apply the input encoder to the input\n        X = self.input_embedding(X)\n\n        # 3. Add the positional encoding\n        X = self.positional_encoding(X)\n\n        # 4. Add a query token to the input.\n        if self.use_census:\n            X = torch.cat((query.unsqueeze(1), X), dim=1)\n        # 4. Apply the transformer encoder to get the memory\n\n        X = self.transformer_encoder(X)\n\n        if self.use_census:\n            query_enc = X[:, 0, :]\n            X = X[:, 1:, :]  # Removing the query token\n\n        # .5 Apply the transformer decoder to get the next item in the sequence\n        tgt_sequence = torch.zeros(X.shape[0], 1, X.shape[-1]).to(DEVICE)\n        tgt_mask = torch.ones(1, 1).to(DEVICE)\n\n        # 6. Then apply the transformer to get the next item in the sequence\n        output = self.transformer_decoder(tgt_sequence, memory=X,\n                                          tgt_mask=tgt_mask)  # We want to predict the next item in the sequence\n\n        # 7.We only want the last output of the sequence\n        output = output[:, -1, :]\n        if self.use_census:\n            output = torch.cat((output, query_enc), dim=-1)\n\n        # 3. Finally apply the regressor to get the predictions.\n        output = self.regressor(output)\n\n        return output\n\n",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "execution": {
     "iopub.status.busy": "2023-02-21T20:40:05.426416Z",
     "iopub.execute_input": "2023-02-21T20:40:05.426874Z",
     "iopub.status.idle": "2023-02-21T20:40:05.487870Z",
     "shell.execute_reply.started": "2023-02-21T20:40:05.426841Z",
     "shell.execute_reply": "2023-02-21T20:40:05.485486Z"
    },
    "trusted": true
   },
   "execution_count": 3,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "# Dataset",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": "import os\n\nimport pandas as pd\nimport torch\nfrom torch.utils.data import Dataset\nclass LstmDataset(Dataset):\n    def __init__(self, type, seq_len, stride=1):\n        self.type = type\n        self.seq_len = seq_len\n        self.stride = stride\n\n        self.file = os.path.join(DATA_DIR, f\"train_with_census_{'train' if type==DatasetType.TRAIN else 'val' if type==DatasetType.VALID  else 'test'}.csv\")\n        self.load_data()\n\n    def init_transforms(self):\n        \"\"\"\n        Initialize transforms.Might be different for each dataset type\n        \"\"\"\n\n    def load_data(self):\n        \"\"\"\n        Load data from the data items if necessary\n        \"\"\"\n        self.data = pd.read_csv(self.file)\n        self.data['first_day_of_month'] = pd.to_datetime(self.data['first_day_of_month'])\n\n    def __len__(self):\n        return len(self.data) // self.stride\n\n    def __getitem__(self, item):\n        \"\"\"\n        Retrieving seq_len data\n        1. The county (CFIPS) should be the same\n        2. And the difference between the date(first_day_of_month) should be at most 3 months\n        \"\"\"\n        i = item * self.stride\n        county = self.data.iloc[i]['cfips']\n\n        rows_data=self.data.iloc[i:i+self.seq_len]\n\n        #Check if the county is the same\n        is_valid = len(rows_data)==self.seq_len and (rows_data['cfips'].unique()[0]==county) and (rows_data['first_day_of_month'].diff().max()<pd.Timedelta(days=90))\n\n        if not is_valid:\n            ##Find a random item that is valid\n            return self.__getitem__(torch.randint(0, len(self), (1,)).item())\n\n        #Taking seq_len rows and considering the following features\n        #pct_bb,pct_college,pct_foreign_born,pct_it_workers,median_hh_inc, active,microbusiness_density\n        features_tensor = torch.tensor(\n            rows_data[['pct_bb', 'pct_college', 'pct_foreign_born', 'pct_it_workers', 'median_hh_inc','year', 'active',\n                        'microbusiness_density']].values, dtype=torch.float32)\n\n        #return the iterator\n        return features_tensor\n\nimport json\nimport os\nfrom unicodedata import category\nimport numpy as np\nimport pandas as pd\nimport torch\nfrom PIL import Image\nfrom matplotlib import pyplot as plt\nfrom torch.utils.data import Dataset\nfrom torchvision.transforms import transforms\nfrom tqdm import tqdm\nfrom enum import Enum\nclass CensusDataset(Dataset):\n    def __init__(self, type):\n        self.type=type\n        self.load_data()\n        pass\n\n    def load_data(self):\n        \"\"\"\n        Load data from the data items if necessary\n        Returns:\n\n        \"\"\"\n        self.data_file=os.path.join(DATA_DIR,f\"train_with_census_ae_{'train' if self.type == DatasetType.TRAIN else 'test'}.csv\")\n        self.data = pd.read_csv(self.data_file)\n\n\n\n\n    def __len__(self):\n        return len(self.data)\n\n\n    def __getitem__(self, idx):\n        \"\"\"\n        pct_bb,pct_college,pct_foreign_born,pct_it_workers,median_hh_inc,year .\n        Retrieve the following features from the dataset and return the corresponding tensor\n\n        Returns:\n        \"\"\"\n        row=self.data.iloc[idx]\n        features_tensor=torch.tensor([row['pct_bb'],row['pct_college'],row['pct_foreign_born'],\\\n                                      row['pct_it_workers'],row['median_hh_inc'],row['year']],dtype=torch.float32)\n        return features_tensor\n\n\n\n\n\n",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "execution": {
     "iopub.status.busy": "2023-02-21T20:40:05.490871Z",
     "iopub.execute_input": "2023-02-21T20:40:05.491425Z",
     "iopub.status.idle": "2023-02-21T20:40:05.896685Z",
     "shell.execute_reply.started": "2023-02-21T20:40:05.491357Z",
     "shell.execute_reply": "2023-02-21T20:40:05.895435Z"
    },
    "trusted": true
   },
   "execution_count": 4,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "# Loss and metrics",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": "import torch\nfrom torch import nn\n\n\nclass SmapeCriterion(nn.Module):\n    \"\"\"\n    Class to compute the SMAPE loss.\n    \"\"\"\n    def __init__(self):\n        super(SmapeCriterion, self).__init__()\n\n    def forward(self, y_pred, y_true):\n        \"\"\"\n        @param y_pred: Predicted values\n        @param y_true: True values\n        @return: SMAPE loss\n        \"\"\"\n        eps = 1e-8\n        return 100*torch.mean(2 * torch.abs(y_pred - y_true) / (torch.abs(y_pred) + torch.abs(y_true) + eps))\n\n    def __str__(self):\n        return \"SMAPE\"\n\n    def __repr__(self):\n        return str(self)\n\n",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "execution": {
     "iopub.status.busy": "2023-02-21T20:40:05.898440Z",
     "iopub.execute_input": "2023-02-21T20:40:05.899865Z",
     "iopub.status.idle": "2023-02-21T20:40:05.909275Z",
     "shell.execute_reply.started": "2023-02-21T20:40:05.899814Z",
     "shell.execute_reply": "2023-02-21T20:40:05.907993Z"
    },
    "trusted": true
   },
   "execution_count": 5,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "# Dataset",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": "import logging\nimport os\nimport random\n\nimport pandas as pd\nimport torch\nfrom torch.utils.data import Dataset\nfrom tqdm import tqdm\n\nEVAL_START_DATE = \"2022-10-01\"\nTEST_START_DATE =  \"2022-11-01\"\n\nSEED=42\nclass MicroDensityDataset(Dataset):\n    def __init__(self, type, seq_len, stride=1,use_census=USE_CENSUS):\n        self.type = type\n        self.seq_len = seq_len\n        self.stride = stride if type == DatasetType.TRAIN else 1\n        self.use_census = use_census\n        self.load_data()\n        self.prepare_sequences()\n\n        self.tensor_list = dict()\n        if type!=DatasetType.TEST:\n            self.prepare_tensors()\n\n    def init_transforms(self):\n        \"\"\"\n        Initialize transforms.Might be different for each dataset type\n        \"\"\"\n\n    def load_data(self):\n        \"\"\"\n        Load data from the data items if necessary\n        \"\"\"\n\n        self.main_file = os.path.join(DATA_DIR, \"train.csv\")\n        self.main_df = pd.read_csv(self.main_file)\n\n        if self.type == DatasetType.TEST:\n            self.test_df = pd.read_csv(TEST_FILE)\n            self.test_df[\"microbusiness_density\"] = [0 for _ in range(len(self.test_df))]\n            self.test_df[\"county\"] =[\"NAN\" for _ in range(len(self.test_df))]\n            self.test_df[\"state\"] =[\"NAN\" for _ in range(len(self.test_df))]\n\n            self.main_df = pd.concat([self.main_df, self.test_df], ignore_index=True)\n\n            self.test_df =self.test_df.sort_values(by=[\"cfips\",\"first_day_of_month\"])\n            self.test_df = self.test_df.reset_index(drop=True)\n\n            # Fill The missing value of active with last known value (for each cfips)\n            self.main_df[\"active\"] = self.main_df.groupby(\"cfips\")[\"active\"].apply(lambda x: x.fillna(method=\"ffill\"))\n            pass\n\n        if self.use_census:\n            #Merge the census features\n            self.cfips_index=get_cfips_index()\n            self.census_df = pd.read_csv(CENSUS_FILE)\n\n            self.main_df=pd.merge(self.main_df,self.census_df,on=[\"cfips\",\"first_day_of_month\"],how=\"left\")\n\n\n\n        ##Group by cfips and sort by date\n        self.main_df=self.main_df.sort_values(by=[\"cfips\",\"first_day_of_month\"])\n        self.main_df[\"id\"] =list(range(len(self.main_df)))\n\n\n\n    def prepare_sequences(self):\n        \"\"\"\n        Prepare the sequences for the LSTM:\n        Build a list of (id(offset), id(seq_len+offset)) tuples\n        \"\"\"\n        self.sequences=[]\n\n        if self.type == DatasetType.TRAIN:\n            ##Train data are dates before EVAL_START_DATE\n            df=self.main_df[self.main_df['first_day_of_month']<EVAL_START_DATE]\n\n            for i in tqdm(range(0, len(df)-self.seq_len, self.stride), desc=\"Preparing sequences of dataset of type train\"):\n\n                ##The cfips should be the same for the whole sequence(just check the first and last rows)\n                if df.iloc[i][\"cfips\"] != df.iloc[i + self.seq_len - 1][\"cfips\"]:\n                    continue\n\n                if i + self.seq_len > len(df) :\n                    break\n\n                #Get the corresponding ids\n                self.sequences.append((df.iloc[i][\"id\"], df.iloc[i][\"id\"]+ self.seq_len))\n\n\n\n        else :\n\n\n            if self.type == DatasetType.VALID:\n                df = self.main_df[self.main_df['first_day_of_month'] >= EVAL_START_DATE]\n\n            else:\n                df = self.main_df[self.main_df['first_day_of_month'] >= TEST_START_DATE]\n\n\n            for i in tqdm(range(0, len(df),self.stride), desc=\"Preparing sequences of dataset of type {}\".format(\"eval\" if self.type == DatasetType.VALID else \"test\")):\n                ## In eval and test sequences, the step to predict should always be the last one of the sequence\n\n                ##Find the offest of the start in the main df\n\n\n                offset=df.iloc[i][\"id\"]\n\n                offset = offset - self.seq_len+1  # The step to predict is the last one of the sequence\n\n\n                ##check if the cfips is the same\n                if self.main_df.iloc[offset][\"cfips\"] != self.main_df.iloc[offset + self.seq_len - 1][\"cfips\"]:\n                    #Warning\n                    print(\"Warning: cfips is not the same for the whole sequence . Offsets :\",offset,offset + self.seq_len - 1)\n\n                self.sequences.append((offset, offset + self.seq_len))\n\n\n    def prepare_tensors(self):\n\n        for i in tqdm(range(len(self.sequences)), desc=\"Prefetching tensors...\"):\n            start,end=self.sequences[i]\n            self.tensor_list[(start,end)]=self.__getitem__(i)\n\n\n\n    def __len__(self):\n        return len(self.sequences)\n\n    def __getitem__(self, item):\n        \"\"\"\n        Retrieving seq_len data\n        1. The county (CFIPS) should be the same\n        2. And the difference between the date(first_day_of_month) should be at most 3 months\n        \"\"\"\n        start,end=self.sequences[item]\n\n        if (start,end) in self.tensor_list:\n            return self.tensor_list[(start,end)]\n\n        rows_data=self.main_df.iloc[start:end]\n\n\n        #ensure unique cfips\n        # assert len(rows_data[\"cfips\"].unique())==1\n\n        tensor = torch.tensor(rows_data[['microbusiness_density']].values,\n                                       dtype=torch.float32)  # Not considering the census features\n\n        #FEatures scaling\n\n\n        if self.use_census:\n            censur_features_tensor = extract_census_features(rows_data, cfips_index=self.cfips_index,single_row=False)\n            tensor = torch.cat((censur_features_tensor,tensor), dim=1)\n\n        return tensor\n\n\n\n    def mix_with(self, other_dataset, size=0.8):\n        \"\"\"\n        Combine two datasets exemple a train dataset and test dataset\n        @param other_dataset:\n        @param size:\n        @return:\n        \"\"\"\n\n        all_sequences= self.sequences + other_dataset.sequences\n        random.shuffle(all_sequences)\n        self.sequences=all_sequences[:int(len(all_sequences)*size)]\n        other_dataset.sequences=all_sequences[int(len(all_sequences)*size):]\n        logging.info(\"Combined dataset: {} sequences for train and {} sequences for test\".format(len(self.sequences),len(other_dataset.sequences)))\n",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "execution": {
     "iopub.status.busy": "2023-02-21T20:40:05.911237Z",
     "iopub.execute_input": "2023-02-21T20:40:05.911758Z",
     "iopub.status.idle": "2023-02-21T20:40:05.950311Z",
     "shell.execute_reply.started": "2023-02-21T20:40:05.911714Z",
     "shell.execute_reply": "2023-02-21T20:40:05.949007Z"
    },
    "trusted": true
   },
   "execution_count": 6,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "# Trainer",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": "import csv\nimport json\nimport logging\nimport os\nimport shutil\nimport warnings\n\nimport numpy as np\nimport pandas as pd\nimport torch\nfrom torch.utils.tensorboard import SummaryWriter\nfrom tqdm import tqdm\nclass TrainerTransformerPredictor:\n    \"\"\"\n    Class to manage the full training pipeline\n    \"\"\"\n    def __init__(self, network: TransformerPredictor,\n                 criterion,\n                 optimizer,\n                 scheduler=None,\n                 nb_epochs=10, batch_size=128, reset=False):\n        \"\"\"\n        @param network:\n        @param dataset_name:\n        @param images_dirs:\n        @param loss:\n        @param optimizer:\n        @param nb_epochs:\n        @param nb_workers: Number of worker for the dataloader\n        \"\"\"\n        self.network = network\n        self.batch_size = batch_size\n        self.loss_fn=criterion\n\n        self.optimizer = optimizer\n        self.scheduler =scheduler if scheduler else\\\n            torch.optim.lr_scheduler.ReduceLROnPlateau(self.optimizer, mode='min', factor=0.2, patience=10,min_lr=1e-5)\n\n        self.nb_epochs = nb_epochs\n        self.experiment_dir = self.network.experiment_dir\n        self.model_info_file = os.path.join(self.experiment_dir, \"model.json\")\n        self.model_info_best_file = os.path.join(self.experiment_dir, \"model_best.json\")\n\n        if reset:\n            if os.path.exists(self.experiment_dir):\n                shutil.rmtree(self.experiment_dir)\n        if not os.path.exists(self.experiment_dir):\n            os.makedirs(self.experiment_dir)\n\n        self.start_epoch = 0\n        if not reset and os.path.exists(self.model_info_file):\n            with open(self.model_info_file, \"r\") as f:\n                self.start_epoch = json.load(f)[\"epoch\"] + 1\n                self.nb_epochs += self.start_epoch\n                logging.info(\"Resuming from epoch {}\".format(self.start_epoch))\n\n\n    def save_model_info(self, infos, best=False):\n        json.dump(infos, open(self.model_info_file, 'w'),indent=4)\n        if best: json.dump(infos, open(self.model_info_best_file, 'w'),indent=4)\n\n    def fit(self,train_dataloader,val_dataloader):\n        logging.info(\"Launch training on {}\".format(DEVICE))\n        if self.network.use_census:\n            logging.info(\"Using encoder census data\")\n\n        self.summary_writer = SummaryWriter(log_dir=self.experiment_dir)\n        itr = self.start_epoch * len(train_dataloader) * self.batch_size  ##Global counter for steps\n\n        #Save model graph\n        # self.summary_writer.add_graph(self.network, next(iter(train_dataloader)).to(DEVICE)[:,:-1,:])\n\n        self.best_val_loss = 1e20  # infinity\n        if os.path.exists(self.model_info_file):\n            with open(self.model_info_file, \"r\") as f:\n                model_info = json.load(f)\n                lr=model_info[\"lr\"]\n                logging.info(f\"Setting lr to {lr}\")\n                for g in self.optimizer.param_groups:\n                    g['lr'] = lr\n\n        if os.path.exists(self.model_info_best_file):\n            with open(self.model_info_best_file, \"r\") as f:\n                best_model_info = json.load(f)\n                self.best_val_loss = best_model_info[\"val_loss\"]\n\n\n        for epoch in range(self.start_epoch, self.nb_epochs):  # Training loop\n            self.network.train()\n            \"\"\"\"\n            0. Initialize loss and other metrics\n            \"\"\"\n            running_loss=Averager()\n            pbar = tqdm(train_dataloader, desc=f\"Epoch {epoch + 1}/{self.nb_epochs}\")\n            for _, batch in enumerate(pbar):\n                \"\"\"\n                Training lopp\n                \"\"\"\n                self.optimizer.zero_grad(\n\n                )\n                itr += self.batch_size\n                \"\"\"\n                1.Forward pass\n                \"\"\"\n                batch = batch.to(DEVICE)\n\n                y_pred = self.network(batch)\n                ## The output is the values of the density for each time step\n\n                \"\"\"\n                2.Loss computation and other metrics\n                \"\"\"\n                # The density is the last item of the batch\n                y_true = batch[:,:,-1].to(DEVICE)\n                loss = self.loss_fn(y_pred, y_true[:, -1:])\n\n                \"\"\"\n                3.Optimizing\n                \"\"\"\n                loss.backward()\n                self.optimizer.step()\n                running_loss.send(loss.cpu().item())\n                pbar.set_postfix(current_loss=loss.cpu().item(), current_mean_loss=running_loss.value)\n\n                \"\"\"\n                4.Writing logs and tensorboard data, loss and other metrics\n                \"\"\"\n                self.summary_writer.add_scalar(\"Train/loss\", loss.item(), itr)\n\n\n\n            epoch_val_loss =self.eval(val_dataloader,epoch)\n\n            #If step lr scheduler and currrent lr is not lower than 1e-5\n            current_lr= self.optimizer.param_groups[0]['lr']\n            if current_lr>=2e-6:\n                if isinstance(self.scheduler,torch.optim.lr_scheduler.StepLR):\n                    self.scheduler.step()\n                else:\n                    self.scheduler.step(epoch_val_loss.value)\n\n            infos = {\n                \"epoch\": epoch,\n                \"train_loss\":running_loss.value,\n                \"val_loss\":epoch_val_loss.value,\n                \"lr\": self.optimizer.param_groups[0]['lr'],\n                \"batch_size\": train_dataloader.batch_size,\n                \"stride\": train_dataloader.dataset.stride,\n\n            }\n            infos.update(self.network.config)\n\n\n            logging.info(\"Epoch {} - Train loss: {:.4f} - Val loss: {:.4f}\".format(epoch, running_loss.value, epoch_val_loss.value))\n\n            if epoch_val_loss.value < self.best_val_loss:\n                self.best_val_loss = epoch_val_loss.value\n                best = True\n            else:\n                best = False\n\n            self.network.save_state(best=best)\n            self.save_model_info(infos, best=best)\n\n\n            self.summary_writer.add_scalar(\"Epoch_train/loss\", running_loss.value, epoch)\n            self.summary_writer.add_scalar(\"Epoch_val/loss\", epoch_val_loss.value, epoch)\n\n\n\n    def eval(self, val_dataloader,epoch):\n        \"\"\"\n        Compute loss and metrics on a validation dataloader\n        @return:\n        \"\"\"\n        with torch.no_grad():\n            self.network.eval()\n            running_loss=Averager()\n            pbar = tqdm(val_dataloader, desc=f\"Validation Epoch {epoch + 1}/{self.nb_epochs}\")\n            for _, batch in enumerate(pbar):\n\n                \"\"\"\n                Training lopp\n                \"\"\"\n                \"\"\"\n                1.Forward pass\n                \"\"\"\n                batch=batch.to(DEVICE)\n                y_pred = self.network(batch)\n                \"\"\"\n                2.Loss computation and other metrics\n                \"\"\"\n                y_true = batch[:,:,-1]\n\n\n                loss = self.loss_fn(y_pred, y_true[:, -1:])\n\n                running_loss.send(loss.item())\n\n                pbar.set_postfix(current_loss=loss.item(), current_mean_loss=running_loss.value)\n\n\n        return running_loss\n\n\n\n    def run_test(self, test_dataloader):\n        \"\"\"\n        Compute loss and metrics on a validation dataloader\n        @return:\n        \"\"\"\n        assert test_dataloader.batch_size == 1, \"Batch size must be 1 for test\"\n        predictions = []\n        pred_by_cfips={} #For each cfips, we store the predictions, we keep only the first prediction\n        row_ids = []\n        with torch.no_grad():\n            self.network.eval()\n            for i, input in enumerate(tqdm(test_dataloader,\" Running tests for submission\")):\n                input = input.to(DEVICE)\n                y_pred = self.network(input.to(DEVICE)).cpu().squeeze().item()\n\n                # Denormalize. MEAN_MB, STD_MB (if noramlized)\n                # y_pred = y_pred * STD_MB + MEAN_MB\n                \"\"\"\n                2.Loss computation and other metrics\n                \"\"\"\n\n\n                ##Update all microbusiness_den isty column\n                row_id=test_dataloader.dataset.test_df.loc[i,\"row_id\"]\n                row_ids.append(row_id)\n\n                cfips=test_dataloader.dataset.test_df.loc[i,\"cfips\"]\n                if cfips not in pred_by_cfips:\n                    pred_by_cfips[cfips]=y_pred\n                else:\n                    y_pred=pred_by_cfips[cfips]#We keep only the first prediction\n\n                predictions.append(y_pred)\n\n                test_dataloader.dataset.main_df.loc[test_dataloader.dataset.main_df[\"row_id\"]==row_id,\"microbusiness_density\"]=y_pred\n\n\n        #Merge predictions\n        predictions=np.array(predictions)\n\n\n        #Update all microbusiness_denisty column\n\n        pred_test_df = pd.DataFrame(\n            {\n                \"row_id\":row_ids,\n                 \"microbusiness_density\":predictions}\n\n                                )\n        pred_test_df.to_csv(os.path.join(self.experiment_dir,\"submission.csv\"),index=False)\n\n        return pred_test_df\n\n",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "execution": {
     "iopub.status.busy": "2023-02-21T20:40:05.952468Z",
     "iopub.execute_input": "2023-02-21T20:40:05.953057Z",
     "iopub.status.idle": "2023-02-21T20:40:06.485429Z",
     "shell.execute_reply.started": "2023-02-21T20:40:05.953008Z",
     "shell.execute_reply": "2023-02-21T20:40:06.483949Z"
    },
    "trusted": true
   },
   "execution_count": 7,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "# Runner",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": "from dataclasses import dataclass\nimport argparse\nimport logging\nimport os\nimport pickle\nimport torch.utils.data\n\n@dataclass\nclass Arguments:\n    reset: bool = False\n    learning_rate: float = 0.0001\n    nb_epochs: int = 200\n    model_name: str = \"trf_ae_dv_ed.128_dce.4_nl.6_nh.8_df.512_sl.15_ss.1_lr.0.0001_bs.16_do.0.0\"\n    num_workers: int = 0\n    batch_size: int = 16\n    log_level: str = \"INFO\"\n    autorun_tb: bool = True\n    use_census: bool = True\n    use_derivative: bool = True\n    seq_len: int = 15\n    dropout_rate=0\n    seq_stride: int = 1\n    emb_dim: int = 128\n    n_layers: int =6\n    n_head: int = 8\n    census_emb_dim: int = 4\n    dim_feedforward: int = 512\n\ndef cli():\n    return Arguments()\n\ndef main(args):\n\n    #Format the model name\n    if args.model_name is None:\n        model_name = TransformerPredictor.format_model_name(\n            use_census=args.use_census,\n            use_derivative=args.use_derivative,\n            emb_dim=args.emb_dim,\n            census_emb_dim=args.census_emb_dim,\n            n_layers=args.n_layers,\n            n_head=args.n_head,\n            dim_feedforward=args.dim_feedforward,\n            seq_len=args.seq_len,\n            seq_stride=args.seq_stride,\n            learning_rate=args.learning_rate,\n            batch_size=args.batch_size,\n            dropout_rate=args.dropout_rate\n        )\n    else :\n        model_name=args.model_name\n\n\n\n    experiment_dir = os.path.join(EXPERIMENTS_DIR, model_name)\n\n    network =TransformerPredictor(\n                                    experiment_dir=experiment_dir,\n                                    emb_dim=args.emb_dim,\n                                      n_layers=args.n_layers,\n                                      n_head=args.n_head,\n                                      dim_feedforward=args.dim_feedforward,\n                                      use_census=args.use_census,\n                                    dropout_rate=args.dropout_rate,\n                                    census_emb_dim=args.census_emb_dim,\n                                    max_seq_len=args.seq_len-1,\n                                    reset=args.reset\n                ).to(DEVICE)\n    \n    #Using multiple gpus\n    \n    #Adam optimizer\n    optimizer = torch.optim.Adam(network.parameters(), lr=args.learning_rate)\n\n    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=int(20*args.batch_size/32),gamma=0.5, verbose=True)\n\n    criterion= SmapeCriterion().to(DEVICE)\n\n\n    logging.info(\"Training : \"+model_name)\n    trainer = TrainerTransformerPredictor(network,\n                      criterion,\n                      optimizer=optimizer,\n                      scheduler=scheduler,\n                      nb_epochs= args.nb_epochs,\n                      batch_size=args.batch_size,\n                      reset=args.reset,\n                      )\n\n    # Save  the dataset according to type, seq_len_stride and use_census: using pickle\n\n    if not os.path.exists(os.path.join(ROOT_DIR,\"dataset\",\"pickle\")):\n        os.makedirs(os.path.join(ROOT_DIR,\"dataset\",\"pickle\"))\n\n    datasets_pickle_path = os.path.join(ROOT_DIR,\"dataset\",\"pickle\",f\"all_dataset_{args.seq_len}_{args.seq_stride}_{args.use_census}.pickle\")\n\n\n    if not os.path.exists(datasets_pickle_path):\n        train_dataset = MicroDensityDataset(type=DatasetType.TRAIN, seq_len=args.seq_len, stride=args.seq_stride,\n                                            use_census=args.use_census)\n        val_dataset = MicroDensityDataset(type=DatasetType.VALID, seq_len=args.seq_len, stride=args.seq_stride,\n                                          use_census=args.use_census)\n\n        # train_dataset.mix_with(val_dataset,size=0.8) #Mix train and val dataset to avoid disparity between the two in terms of dates distribution\n\n        test_dataset = MicroDensityDataset(type=DatasetType.TEST, seq_len=args.seq_len, stride=args.seq_stride,\n                                           use_census=args.use_census)\n\n        with open(datasets_pickle_path,\"wb\") as f:\n            logging.info(f\"Saving datasets to {datasets_pickle_path}\")\n            pickle.dump((train_dataset,val_dataset,test_dataset),f)\n    else:\n        with open(datasets_pickle_path,\"rb\") as f:\n            logging.info(f\"Loading datasets  from {datasets_pickle_path}\")\n            train_dataset,val_dataset,test_dataset = pickle.load(f)\n\n\n\n    logging.info(f\"Nb sequences : Train {len(train_dataset)} - Val {len(val_dataset)} - Test {len(test_dataset)}\")\n\n    train_dataloader=torch.utils.data.DataLoader(train_dataset,batch_size=args.batch_size,num_workers=args.num_workers,shuffle=True,drop_last=False,persistent_workers=args.num_workers>0)\n    val_dataloader = torch.utils.data.DataLoader(val_dataset, batch_size=args.batch_size,num_workers=0,drop_last=False)\n    test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=1,num_workers=0,drop_last=False,shuffle=False)\n\n    ##Train\n    trainer.fit(train_dataloader,val_dataloader)\n\n    ##Load best model\n    trainer.network.load_state(best=True)\n    trainer.run_test(test_dataloader=test_dataloader)\n\n\n\nargs = cli()\nsetup_logger(args)\nmain(args)",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "execution": {
     "iopub.status.busy": "2023-02-21T20:40:06.487475Z",
     "iopub.execute_input": "2023-02-21T20:40:06.487966Z",
     "iopub.status.idle": "2023-02-21T20:43:56.148350Z",
     "shell.execute_reply.started": "2023-02-21T20:40:06.487912Z",
     "shell.execute_reply": "2023-02-21T20:43:56.146261Z"
    },
    "trusted": true
   },
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "text": "2023-02-21 20:40:06,678 - root - INFO - Loading model state : /kaggle/logs/experiments/trf_ae_dv_ed.128_dce.4_nl.6_nh.8_df.512_sl.15_ss.1_lr.0.0001_bs.16_do.0.0/trf_ae_dv_ed.128_dce.4_nl.6_nh.8_df.512_sl.15_ss.1_lr.0.0001_bs.16_do.0.0.pt\nAdjusting learning rate of group 0 to 1.0000e-04.\n2023-02-21 20:40:06,727 - root - INFO - Training : trf_ae_dv_ed.128_dce.4_nl.6_nh.8_df.512_sl.15_ss.1_lr.0.0001_bs.16_do.0.0\n2023-02-21 20:40:06,728 - root - INFO - Resuming from epoch 162\n",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": "Preparing sequences of dataset of type train: 100%|██████████| 119115/119115 [01:00<00:00, 1955.24it/s]\nPrefetching tensors...: 100%|██████████| 75239/75239 [02:09<00:00, 581.45it/s]\nPreparing sequences of dataset of type eval: 100%|██████████| 3135/3135 [00:01<00:00, 2217.66it/s]\nPrefetching tensors...: 100%|██████████| 3135/3135 [00:05<00:00, 598.21it/s]\nPreparing sequences of dataset of type test: 100%|██████████| 25080/25080 [00:11<00:00, 2158.19it/s]",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "2023-02-21 20:43:38,887 - root - INFO - Saving datasets to /kaggle/dataset/pickle/all_dataset_15_1_True.pickle\n",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": "\n",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "2023-02-21 20:43:46,778 - root - INFO - Nb sequences : Train 75239 - Val 3135 - Test 25080\n2023-02-21 20:43:46,779 - root - INFO - Launch training on cpu\n2023-02-21 20:43:46,781 - root - INFO - Using encoder census data\n2023-02-21 20:43:55,564 - root - INFO - Setting lr to 5e-05\n",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": "Epoch 163/362:   0%|          | 0/4703 [00:00<?, ?it/s]\n",
     "output_type": "stream"
    },
    {
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "\u001B[0;32m/tmp/ipykernel_27/1744542708.py\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m    134\u001B[0m \u001B[0margs\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mcli\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    135\u001B[0m \u001B[0msetup_logger\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 136\u001B[0;31m \u001B[0mmain\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[0;32m/tmp/ipykernel_27/1744542708.py\u001B[0m in \u001B[0;36mmain\u001B[0;34m(args)\u001B[0m\n\u001B[1;32m    124\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    125\u001B[0m     \u001B[0;31m##Train\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 126\u001B[0;31m     \u001B[0mtrainer\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mfit\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtrain_dataloader\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0mval_dataloader\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    127\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    128\u001B[0m     \u001B[0;31m##Load best model\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/tmp/ipykernel_27/1356361356.py\u001B[0m in \u001B[0;36mfit\u001B[0;34m(self, train_dataloader, val_dataloader)\u001B[0m\n\u001B[1;32m    106\u001B[0m                 \u001B[0mbatch\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mbatch\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mto\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mDEVICE\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    107\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 108\u001B[0;31m                 \u001B[0my_pred\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mnetwork\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mbatch\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    109\u001B[0m                 \u001B[0;31m## The output is the values of the density for each time step\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    110\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py\u001B[0m in \u001B[0;36m_call_impl\u001B[0;34m(self, *input, **kwargs)\u001B[0m\n\u001B[1;32m   1128\u001B[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001B[1;32m   1129\u001B[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001B[0;32m-> 1130\u001B[0;31m             \u001B[0;32mreturn\u001B[0m \u001B[0mforward_call\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0minput\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1131\u001B[0m         \u001B[0;31m# Do not call functions when jit is used\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1132\u001B[0m         \u001B[0mfull_backward_hooks\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mnon_full_backward_hooks\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m[\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m[\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/tmp/ipykernel_27/1936721707.py\u001B[0m in \u001B[0;36mforward\u001B[0;34m(self, X_input)\u001B[0m\n\u001B[1;32m    240\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    241\u001B[0m         \u001B[0;31m# 2. Apply the input encoder to the input\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 242\u001B[0;31m         \u001B[0mX\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0minput_embedding\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mX\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    243\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    244\u001B[0m         \u001B[0;31m# 3. Add the positional encoding\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py\u001B[0m in \u001B[0;36m_call_impl\u001B[0;34m(self, *input, **kwargs)\u001B[0m\n\u001B[1;32m   1128\u001B[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001B[1;32m   1129\u001B[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001B[0;32m-> 1130\u001B[0;31m             \u001B[0;32mreturn\u001B[0m \u001B[0mforward_call\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0minput\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1131\u001B[0m         \u001B[0;31m# Do not call functions when jit is used\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1132\u001B[0m         \u001B[0mfull_backward_hooks\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mnon_full_backward_hooks\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m[\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m[\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/opt/conda/lib/python3.7/site-packages/torch/nn/modules/container.py\u001B[0m in \u001B[0;36mforward\u001B[0;34m(self, input)\u001B[0m\n\u001B[1;32m    137\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0mforward\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0minput\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    138\u001B[0m         \u001B[0;32mfor\u001B[0m \u001B[0mmodule\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 139\u001B[0;31m             \u001B[0minput\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mmodule\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0minput\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    140\u001B[0m         \u001B[0;32mreturn\u001B[0m \u001B[0minput\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    141\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py\u001B[0m in \u001B[0;36m_call_impl\u001B[0;34m(self, *input, **kwargs)\u001B[0m\n\u001B[1;32m   1128\u001B[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001B[1;32m   1129\u001B[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001B[0;32m-> 1130\u001B[0;31m             \u001B[0;32mreturn\u001B[0m \u001B[0mforward_call\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0minput\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1131\u001B[0m         \u001B[0;31m# Do not call functions when jit is used\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1132\u001B[0m         \u001B[0mfull_backward_hooks\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mnon_full_backward_hooks\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m[\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m[\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/opt/conda/lib/python3.7/site-packages/torch/nn/modules/linear.py\u001B[0m in \u001B[0;36mforward\u001B[0;34m(self, input)\u001B[0m\n\u001B[1;32m    112\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    113\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0mforward\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0minput\u001B[0m\u001B[0;34m:\u001B[0m \u001B[0mTensor\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m->\u001B[0m \u001B[0mTensor\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 114\u001B[0;31m         \u001B[0;32mreturn\u001B[0m \u001B[0mF\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mlinear\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0minput\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mweight\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mbias\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    115\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    116\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0mextra_repr\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m->\u001B[0m \u001B[0mstr\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mRuntimeError\u001B[0m: mat1 and mat2 shapes cannot be multiplied (224x8 and 9x128)"
     ],
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (224x8 and 9x128)",
     "output_type": "error"
    }
   ]
  },
  {
   "cell_type": "code",
   "source": "",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  }
 ]
}