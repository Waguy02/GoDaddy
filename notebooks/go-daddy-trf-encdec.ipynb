{
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.7.12",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  }
 },
 "nbformat_minor": 4,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Setup\n"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import math\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "ROOT_DIR=os.path.dirname(\"/kaggle/working\")\n",
    "DATA_DIR=os.path.join(\"/kaggle/input/go-daddy/godaddy-microbusiness-density-forecasting\") ##Directory of dataset\n",
    "\n",
    "EXPERIMENTS_DIR=os.path.join(ROOT_DIR, \"logs/experiments\")\n",
    "use_cuda = torch .cuda.is_available()\n",
    "DEVICE = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "\n",
    "\n",
    "N_CENSUS_FEATURES= 5 #pct_bb,pct_college,pct_foreign_born,pct_it_workers,median_hh_inc\n",
    "#cfips is not considered as a feature we use a one-hot encoding for it\n",
    "\n",
    "\n",
    "USE_CENSUS= False #Without census features\n",
    "\n",
    "AE_LATENT_DIM= 32\n",
    "\n",
    "LSTM_HIDDEN_DIM = 8\n",
    "\n",
    "SEQ_LEN=6\n",
    "SEQ_STRIDE= 1\n",
    "\n",
    "N_COUNTY=3142\n",
    "N_DIMS_COUNTY_ENCODING=  math.ceil(math.log(N_COUNTY,2))\n",
    "\n",
    "FEATURES_AE_CENSUS_DIR=os.path.join(EXPERIMENTS_DIR, \"features_ae_2_dims\")\n",
    "FEATURES_AE_LATENT_DIM= 2\n",
    "\n",
    "TRAIN_FILE= os.path.join(DATA_DIR, \"train.csv\")\n",
    "TEST_FILE= os.path.join(DATA_DIR, \"test.csv\")\n",
    "\n",
    "CENSUS_FILE =os.path.join(DATA_DIR, \"census_interpolated.csv\")\n",
    "\n",
    "NB_FUTURES= 10 #Number of days to predict\n",
    "\n",
    "\n",
    "#Scaling factors for microbusiness density\n",
    "MEAN_MB= 3.817671\n",
    "STD_MB= 4.991087\n",
    "\n",
    "MAX_MB= 300\n",
    "MIN_MB= 0.0"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "execution": {
     "iopub.status.busy": "2023-02-12T15:45:07.787272Z",
     "iopub.execute_input": "2023-02-12T15:45:07.788088Z",
     "iopub.status.idle": "2023-02-12T15:45:07.859041Z",
     "shell.execute_reply.started": "2023-02-12T15:45:07.788051Z",
     "shell.execute_reply": "2023-02-12T15:45:07.857772Z"
    },
    "trusted": true
   },
   "execution_count": 2,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Utils"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import json\n",
    "import os\n",
    "from enum import Enum\n",
    "from itertools import islice\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import logging\n",
    "import os\n",
    "import sys\n",
    "from time import strftime\n",
    "def setup_logger(args):\n",
    "    formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "    a_logger = logging.getLogger()\n",
    "    a_logger.setLevel(args.log_level)\n",
    "    log_dir=os.path.join(ROOT_DIR,\"logs\",\"output_logs\")\n",
    "    if not os.path.exists(log_dir):\n",
    "        os.makedirs(log_dir)\n",
    "    output_file_handler = logging.FileHandler(os.path.join(log_dir,strftime(\"log_%d_%m_%Y_%H_%M.log\")))\n",
    "    stdout_handler = logging.StreamHandler(sys.stdout)\n",
    "    stdout_handler.setFormatter(formatter)\n",
    "    a_logger.propagate=False\n",
    "    a_logger.addHandler(output_file_handler)\n",
    "    a_logger.addHandler(stdout_handler)\n",
    "\n",
    "def read_json(path_json):\n",
    "    with open(path_json, encoding='utf8') as json_file:\n",
    "        return json.load(json_file)\n",
    "def softmax(x):\n",
    "    return np.exp(x) / np.sum(np.exp(x))\n",
    "def chunks(data, SIZE):\n",
    "    \"\"\"Split a dictionnary into parts of max_size =SIZE\"\"\"\n",
    "    it = iter(data)\n",
    "    for _ in range(0, len(data), SIZE):\n",
    "        yield {k: data[k] for k in islice(it, SIZE)}\n",
    "\n",
    "def sorted_dict(x, ascending=True):\n",
    "    \"\"\"\n",
    "    Sort dict according to value.\n",
    "    x must be a primitive type: int,float, str...\n",
    "    @param x:\n",
    "    @return:\n",
    "    \"\"\"\n",
    "    return dict(sorted(x.items(), key=lambda item: (1 if ascending else -1) * item[1]))\n",
    "def reverse_dict(input_dict):\n",
    "    \"\"\"\n",
    "    Reverse a dictonary\n",
    "    Args:\n",
    "        input_dict:\n",
    "\n",
    "    Returns:\n",
    "\n",
    "    \"\"\"\n",
    "    inv_dict = {}\n",
    "    for k, v in input_dict.items():\n",
    "        inv_dict[v] = inv_dict.get(v, []) + [k]\n",
    "\n",
    "    return inv_dict\n",
    "\n",
    "def save_matrix(matrix,filename):\n",
    "    with open(filename,'wb') as output:\n",
    "        np.save(output,matrix)\n",
    "def load_matrix(filename,auto_delete=False):\n",
    "    with open(filename,'rb') as input:\n",
    "        matrix=np.load(input)\n",
    "\n",
    "    if auto_delete:\n",
    "        os.remove(filename)\n",
    "    return matrix\n",
    "\n",
    "\n",
    "\n",
    "class Averager:\n",
    "    def __init__(self):\n",
    "        self.current_total = 0.0\n",
    "        self.iterations = 0.0\n",
    "\n",
    "    def send(self, value):\n",
    "        self.current_total += value\n",
    "        self.iterations += 1\n",
    "\n",
    "    @property\n",
    "    def value(self):\n",
    "        if self.iterations == 0:\n",
    "            return 0\n",
    "        else:\n",
    "            return 1.0 * self.current_total / self.iterations\n",
    "\n",
    "    def reset(self):\n",
    "        self.current_total = 0.0\n",
    "        self.iterations = 0.0\n",
    "\n",
    "\n",
    "\n",
    "from enum import Enum\n",
    "class DatasetType(Enum):\n",
    "    TRAIN=\"train\"\n",
    "    VALID=\"valid\"\n",
    "    TEST=\"test\"\n",
    "\n",
    "\n",
    "\n",
    "def extract_census_features(row,cfips_index,single_row=True):\n",
    "    \"\"\"\n",
    "\n",
    "    @param row: Row of the dataframe\n",
    "    @param cfips_index: index of the cfips for one-hot encoding\n",
    "    @return:\n",
    "    \"\"\"\n",
    "    ##If series :\n",
    "\n",
    "\n",
    "    if single_row:\n",
    "        features_tensor = torch.tensor( [row['pct_bb'],\n",
    "                                        row['pct_college'],\n",
    "                                        row['pct_foreign_born'],\n",
    "                                        row['pct_it_workers'],\n",
    "                                        row['median_hh_inc']\n",
    "                                        ], dtype=torch.float32)\n",
    "        cfips_one_hot = get_cfips_encoding(row['cfips'], cfips_index)\n",
    "        # Min-max normalization\n",
    "        features_tensor[ 0] = (features_tensor[ 0] - 24.5) / (97.6 - 24.5)\n",
    "        features_tensor[ 1] = (features_tensor[ 1] / 48)\n",
    "        features_tensor[ 2] = (features_tensor[ 2] / 54)\n",
    "        features_tensor[ 3] = (features_tensor[ 3] / 17.4)\n",
    "        features_tensor[ 4] = (features_tensor[ 4] - 17109) / (1586821 - 17109)\n",
    "\n",
    "    else :\n",
    "        features_tensor= torch.from_numpy(row[['pct_bb', 'pct_college', 'pct_foreign_born', 'pct_it_workers', 'median_hh_inc']].values)\n",
    "        row_one_hots= [get_cfips_encoding(cfips,cfips_index) for cfips in row['cfips']]\n",
    "        cfips_one_hot = torch.stack(row_one_hots)\n",
    "        #Min-max normalization\n",
    "        features_tensor[:,0] = (features_tensor[:,0]- 24.5)/ (97.6-24.5)\n",
    "        features_tensor[:,1] = (features_tensor[:,1] /48)\n",
    "        features_tensor[:,2] = (features_tensor[:,2]/ 54)\n",
    "        features_tensor[:,3] = (features_tensor[:,3] / 17.4)\n",
    "        features_tensor[:,4] = (features_tensor[:,4]- 17109)/(1586821-17109)\n",
    "\n",
    "\n",
    "    ##Add one-hot encoding of cfips\n",
    "    if single_row:\n",
    "        features_tensor = torch.cat((cfips_one_hot, features_tensor))\n",
    "    else:\n",
    "        features_tensor = torch.cat((cfips_one_hot,features_tensor), 1)\n",
    "\n",
    "    return features_tensor.float()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def get_cfips_index():\n",
    "    \"\"\"\n",
    "    Return a dictionary with key=cfips and value=index for using a one-hot encoding\n",
    "    \"\"\"\n",
    "    df= pd.read_csv(os.path.join(DATA_DIR, \"census_interpolated.csv\"))\n",
    "    cfips = df['cfips'].unique()\n",
    "    cfips.sort()\n",
    "    #Sort cfips\n",
    "    return {cfips[i]: i for i in range(len(cfips))}\n",
    "\n",
    "\n",
    "def get_cfips_encoding(cfips,cfips_index):\n",
    "    \"\"\"\n",
    "     return the base 2 encoding of cfips\n",
    "    \"\"\"\n",
    "\n",
    "    #n_dims is the number of bits needed to represent the cfips\n",
    "\n",
    "    bin_index=np.binary_repr(cfips_index[cfips],width=N_DIMS_COUNTY_ENCODING)\n",
    "    enc = torch.tensor([int(x) for x in bin_index],dtype=torch.float32)\n",
    "    return enc\n",
    "\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "execution": {
     "iopub.status.busy": "2023-02-12T15:45:07.863832Z",
     "iopub.execute_input": "2023-02-12T15:45:07.866164Z",
     "iopub.status.idle": "2023-02-12T15:45:07.892541Z",
     "shell.execute_reply.started": "2023-02-12T15:45:07.866133Z",
     "shell.execute_reply": "2023-02-12T15:45:07.891621Z"
    },
    "trusted": true
   },
   "execution_count": 3,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Network"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import json\n",
    "import logging\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=5000):\n",
    "        #Sin/Cos positional encoding\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.max_len = max_len\n",
    "        self.pe = torch.zeros(self.max_len, self.d_model)\n",
    "        position = torch.arange(0, self.max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, self.d_model, 2).float() * (-np.log(10000.0) / self.d_model))\n",
    "        self.pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        self.pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        self.pe = self.pe.unsqueeze(0).transpose(0, 1).squeeze(1)\n",
    "        #COnvert to nn.Parameter\n",
    "        self.pe = nn.Parameter(self.pe, requires_grad=False)\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        #Add positional encoding to the input (Pay attention to the dimensions (the pe does not have the batch dimension))\n",
    "        x = x + self.pe[:x.size(0), :]\n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "class TransformerPredictor(nn.Module):\n",
    "\n",
    "    def __init__(self,\n",
    "                 emb_dim=32,\n",
    "                 n_layers=3,\n",
    "                 n_head=8,\n",
    "                 max_seq_len=100,\n",
    "                 dim_feedforward=128,\n",
    "                 use_derivative=True,\n",
    "                 use_census=USE_CENSUS,\n",
    "                 n_dims_census_emb=2,\n",
    "                 experiment_dir=\"my_model\", reset=False, load_best=True):\n",
    "        \"\"\"\n",
    "        @param features_encoder :\n",
    "        @param input_dim:\n",
    "        @param hidden_dim:\n",
    "        @param ues_encoder:Â²\n",
    "        @param experiment_dir:\n",
    "        @param reset:\n",
    "        @param load_best:\n",
    "        \"\"\"\n",
    "\n",
    "        super(TransformerPredictor, self).__init__()\n",
    "        self.variante_num=4\n",
    "        self.emb_dim = emb_dim\n",
    "        self.n_layers = n_layers\n",
    "        self.n_head = n_head\n",
    "        self.dim_feedforward = dim_feedforward\n",
    "        self.use_census = use_census\n",
    "        self.max_seq_len = max_seq_len\n",
    "        self.census_features_encoder = None\n",
    "        self.n_dims_census_emb = n_dims_census_emb\n",
    "        self.input_dim =1\n",
    "        self.use_derivative = use_derivative\n",
    "        if self.use_derivative:\n",
    "            self.input_dim += 2 # 2 for derivative\n",
    "\n",
    "        if self.use_census:\n",
    "            self.input_dim = self.input_dim  + self.n_dims_census_emb\n",
    "\n",
    "\n",
    "\n",
    "        self.experiment_dir = experiment_dir\n",
    "        self.model_name = os.path.basename(self.experiment_dir)\n",
    "        self.reset = reset\n",
    "        self.load_best = load_best\n",
    "        self.setup_dirs()\n",
    "        self.setup_network()\n",
    "\n",
    "\n",
    "        if not reset: self.load_state()\n",
    "\n",
    "    ##1. Defining network architecture\n",
    "    def setup_network(self):\n",
    "        \"\"\"\n",
    "        Initialize the network  architecture here\n",
    "        @return:\n",
    "        \"\"\"\n",
    "        #Input encoder from self.input_dim to self.emb_dim along with positional encoding\n",
    "        if self.use_census:\n",
    "            self.query_encoder = nn.Sequential(nn.Linear(N_DIMS_COUNTY_ENCODING + N_CENSUS_FEATURES, self.emb_dim))\n",
    "            self.census_features_encoder= nn.Sequential(\n",
    "                nn.Linear(N_CENSUS_FEATURES,self.n_dims_census_emb),\n",
    "            )\n",
    "\n",
    "        self.input_embedding = nn.Sequential(\n",
    "            nn.Linear(self.input_dim, self.emb_dim),\n",
    "        )\n",
    "\n",
    "        ##Positional encoding\n",
    "        self.positional_encoding = PositionalEncoding(self.emb_dim, max_len=self.max_seq_len)\n",
    "        self.dropout = nn.Dropout(p=0.1)\n",
    "        self.transformer_encoder  = nn.TransformerEncoder(\n",
    "            nn.TransformerEncoderLayer(d_model=self.emb_dim, nhead=self.n_head, dim_feedforward=self.dim_feedforward,\n",
    "                                       dropout=0,\n",
    "                                       batch_first=True),\n",
    "            num_layers=self.n_layers\n",
    "        )\n",
    "        self.transformer_decoder = nn.TransformerDecoder(\n",
    "            nn.TransformerDecoderLayer(d_model=self.emb_dim, nhead=self.n_head, dim_feedforward=self.dim_feedforward,\n",
    "                                       dropout=0,\n",
    "                                       batch_first=True),\n",
    "            num_layers=self.n_layers,\n",
    "            \n",
    "        )\n",
    "\n",
    "        if self.use_census:\n",
    "            self.regressor = nn.Sequential(\n",
    "                nn.Linear(2*self.emb_dim, 1024),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(1024,1)\n",
    "            )\n",
    "\n",
    "        else:\n",
    "            self.regressor = nn.Sequential(\n",
    "                nn.Linear(self.emb_dim, 1)\n",
    "            )\n",
    "\n",
    "\n",
    "    ##2. Model Saving/Loading\n",
    "    def load_state(self, best=False):\n",
    "        \"\"\"\n",
    "        Load model\n",
    "        :param self:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        if best and os.path.exists(self.save_best_file):\n",
    "            logging.info(f\"Loading best model state : {self.save_file}\")\n",
    "            self.load_state_dict(torch.load(self.save_file, map_location=DEVICE))\n",
    "            return\n",
    "\n",
    "        if os.path.exists(self.save_file):\n",
    "            logging.info(f\"Loading model state : {self.save_file}\")\n",
    "            self.load_state_dict(torch.load(self.save_file, map_location=DEVICE))\n",
    "\n",
    "    def save_state(self, best=False):\n",
    "        if best:\n",
    "            logging.info(\"Saving best model\")\n",
    "            torch.save(self.state_dict(), self.save_best_file)\n",
    "        torch.save(self.state_dict(), self.save_file)\n",
    "\n",
    "    ##3. Setupping directories for weights /logs ... etc\n",
    "    def setup_dirs(self):\n",
    "        \"\"\"\n",
    "        Checking and creating directories for weights storage\n",
    "        @return:\n",
    "        \"\"\"\n",
    "        self.save_file = os.path.join(self.experiment_dir, f\"{self.model_name}.pt\")\n",
    "        self.save_best_file = os.path.join(self.experiment_dir, f\"{self.model_name}_best.pt\")\n",
    "        if not os.path.exists(self.experiment_dir):\n",
    "            os.makedirs(self.experiment_dir)\n",
    "\n",
    "    #4. Forward call\n",
    "    def forward(self, X_input):\n",
    "        \"\"\"\n",
    "        +Forward call here.\n",
    "        It a time series, so we need the full sequence output (strided by 1)\n",
    "        @param X:\n",
    "        @return:\n",
    "        \"\"\"\n",
    "        #0. Preparing the input (Removing the target from the input)\n",
    "        X = X_input[:, :-1, :] # Removing the target from the input (Only required when using census features)\n",
    "\n",
    "\n",
    "        if self.use_derivative:\n",
    "            d_left= torch.zeros((X.shape[0], X.shape[1], 1), device=DEVICE)\n",
    "            d_left[:,1:, -1] = X[:, 1:, -1] - X[:, :-1, -1]\n",
    "\n",
    "            d_right= torch.zeros((X.shape[0], X.shape[1], 1), device=DEVICE)\n",
    "            d_right[:,:-1, -1] = X[:, 1:, -1] - X[:, :-1, -1]\n",
    "\n",
    "            X = torch.cat((d_left, X, d_right), dim=-1) ## Adding the derivative to the input as a new feature\n",
    "\n",
    "\n",
    "        if self.use_census:\n",
    "            target = X[:, -1, :]  # Last element of the sequence is the target .\n",
    "            query = self.query_encoder(target[:, :N_DIMS_COUNTY_ENCODING+N_CENSUS_FEATURES])\n",
    "\n",
    "            enc_census = self.census_features_encoder(X[:, :, N_DIMS_COUNTY_ENCODING:N_CENSUS_FEATURES + N_DIMS_COUNTY_ENCODING])\n",
    "            X = torch.cat((X[:, :, N_CENSUS_FEATURES + N_DIMS_COUNTY_ENCODING:], enc_census), dim=-1)\n",
    "\n",
    "\n",
    "        #2. Apply the input encoder to the input\n",
    "        X = self.input_embedding(X)\n",
    "\n",
    "        #3. Add the positional encoding\n",
    "        X = self.positional_encoding(X)\n",
    "\n",
    "\n",
    "        #4. Add a query token to the input. Encoding of the cfips. (It is the same for all the sequence)\n",
    "        if self.use_census:\n",
    "            X = torch.cat((query.unsqueeze(1), X), dim=1)\n",
    "\n",
    "\n",
    "        #4. Apply the transformer encoder to get the memory\n",
    "        X = self.transformer_encoder(X)\n",
    "\n",
    "        if self.use_census:\n",
    "            query_enc= X[:, 0, :]\n",
    "            X = X[:, 1:, :]#Removing the query token\n",
    "\n",
    "\n",
    "\n",
    "        #.5 Apply the transformer decoder to get the next item in the sequence\n",
    "        tgt_sequence = torch.zeros(X.shape[0], 1, X.shape[-1]).to(DEVICE)\n",
    "        tgt_mask = torch.ones(1,1).to(DEVICE)\n",
    "\n",
    "        #6. Then apply the transformer to get the next item in the sequence\n",
    "        output = self.transformer_decoder(tgt_sequence, memory=X, tgt_mask= tgt_mask)#We want to predict the next item in the sequence\n",
    "\n",
    "        #7.We only want the last output of the sequence\n",
    "        output= output[:, -1, :]\n",
    "        if self.use_census:\n",
    "            output= torch.cat((output, query_enc), dim=-1)\n",
    "\n",
    "        #3. Finally apply the regressor to get the predictions.\n",
    "        output = self.regressor(output)\n",
    "\n",
    "        return output\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "execution": {
     "iopub.status.busy": "2023-02-12T15:45:07.920848Z",
     "iopub.execute_input": "2023-02-12T15:45:07.921118Z",
     "iopub.status.idle": "2023-02-12T15:45:07.952922Z",
     "shell.execute_reply.started": "2023-02-12T15:45:07.921093Z",
     "shell.execute_reply": "2023-02-12T15:45:07.951778Z"
    },
    "trusted": true
   },
   "execution_count": 4,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Dataset"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "class LstmDataset(Dataset):\n",
    "    def __init__(self, type, seq_len, stride=1):\n",
    "        self.type = type\n",
    "        self.seq_len = seq_len\n",
    "        self.stride = stride\n",
    "\n",
    "        self.file = os.path.join(DATA_DIR, f\"train_with_census_{'train' if type==DatasetType.TRAIN else 'val' if type==DatasetType.VALID  else 'test'}.csv\")\n",
    "        self.load_data()\n",
    "\n",
    "    def init_transforms(self):\n",
    "        \"\"\"\n",
    "        Initialize transforms.Might be different for each dataset type\n",
    "        \"\"\"\n",
    "\n",
    "    def load_data(self):\n",
    "        \"\"\"\n",
    "        Load data from the data items if necessary\n",
    "        \"\"\"\n",
    "        self.data = pd.read_csv(self.file)\n",
    "        self.data['first_day_of_month'] = pd.to_datetime(self.data['first_day_of_month'])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data) // self.stride\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        \"\"\"\n",
    "        Retrieving seq_len data\n",
    "        1. The county (CFIPS) should be the same\n",
    "        2. And the difference between the date(first_day_of_month) should be at most 3 months\n",
    "        \"\"\"\n",
    "        i = item * self.stride\n",
    "        county = self.data.iloc[i]['cfips']\n",
    "\n",
    "        rows_data=self.data.iloc[i:i+self.seq_len]\n",
    "\n",
    "        #Check if the county is the same\n",
    "        is_valid = len(rows_data)==self.seq_len and (rows_data['cfips'].unique()[0]==county) and (rows_data['first_day_of_month'].diff().max()<pd.Timedelta(days=90))\n",
    "\n",
    "        if not is_valid:\n",
    "            ##Find a random item that is valid\n",
    "            return self.__getitem__(torch.randint(0, len(self), (1,)).item())\n",
    "\n",
    "        #Taking seq_len rows and considering the following features\n",
    "        #pct_bb,pct_college,pct_foreign_born,pct_it_workers,median_hh_inc, active,microbusiness_density\n",
    "        features_tensor = torch.tensor(\n",
    "            rows_data[['pct_bb', 'pct_college', 'pct_foreign_born', 'pct_it_workers', 'median_hh_inc','year', 'active',\n",
    "                        'microbusiness_density']].values, dtype=torch.float32)\n",
    "\n",
    "        #return the iterator\n",
    "        return features_tensor\n",
    "\n",
    "import json\n",
    "import os\n",
    "from unicodedata import category\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from PIL import Image\n",
    "from matplotlib import pyplot as plt\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision.transforms import transforms\n",
    "from tqdm import tqdm\n",
    "from enum import Enum\n",
    "class CensusDataset(Dataset):\n",
    "    def __init__(self, type):\n",
    "        self.type=type\n",
    "        self.load_data()\n",
    "        pass\n",
    "\n",
    "    def load_data(self):\n",
    "        \"\"\"\n",
    "        Load data from the data items if necessary\n",
    "        Returns:\n",
    "\n",
    "        \"\"\"\n",
    "        self.data_file=os.path.join(DATA_DIR,f\"train_with_census_ae_{'train' if self.type == DatasetType.TRAIN else 'test'}.csv\")\n",
    "        self.data = pd.read_csv(self.data_file)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        pct_bb,pct_college,pct_foreign_born,pct_it_workers,median_hh_inc,year .\n",
    "        Retrieve the following features from the dataset and return the corresponding tensor\n",
    "\n",
    "        Returns:\n",
    "        \"\"\"\n",
    "        row=self.data.iloc[idx]\n",
    "        features_tensor=torch.tensor([row['pct_bb'],row['pct_college'],row['pct_foreign_born'],\\\n",
    "                                      row['pct_it_workers'],row['median_hh_inc'],row['year']],dtype=torch.float32)\n",
    "        return features_tensor\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "execution": {
     "iopub.status.busy": "2023-02-12T15:45:07.955142Z",
     "iopub.execute_input": "2023-02-12T15:45:07.955602Z",
     "iopub.status.idle": "2023-02-12T15:45:08.207557Z",
     "shell.execute_reply.started": "2023-02-12T15:45:07.955481Z",
     "shell.execute_reply": "2023-02-12T15:45:08.203181Z"
    },
    "trusted": true
   },
   "execution_count": 5,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Loss and metrics"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "\n",
    "class SmapeCriterion(nn.Module):\n",
    "    \"\"\"\n",
    "    Class to compute the SMAPE loss.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super(SmapeCriterion, self).__init__()\n",
    "\n",
    "    def forward(self, y_pred, y_true):\n",
    "        \"\"\"\n",
    "        @param y_pred: Predicted values\n",
    "        @param y_true: True values\n",
    "        @return: SMAPE loss\n",
    "        \"\"\"\n",
    "        eps = 1e-8\n",
    "        return 100*torch.mean(2 * torch.abs(y_pred - y_true) / (torch.abs(y_pred) + torch.abs(y_true) + eps))\n",
    "\n",
    "    def __str__(self):\n",
    "        return \"SMAPE\"\n",
    "\n",
    "    def __repr__(self):\n",
    "        return str(self)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "execution": {
     "iopub.status.busy": "2023-02-12T15:45:08.209649Z",
     "iopub.execute_input": "2023-02-12T15:45:08.210057Z",
     "iopub.status.idle": "2023-02-12T15:45:08.221990Z",
     "shell.execute_reply.started": "2023-02-12T15:45:08.210023Z",
     "shell.execute_reply": "2023-02-12T15:45:08.218998Z"
    },
    "trusted": true
   },
   "execution_count": 6,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Dataset"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import logging\n",
    "import os\n",
    "import random\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from tqdm import tqdm\n",
    "\n",
    "EVAL_START_DATE = \"2022-05-01\"\n",
    "TEST_START_DATE =  \"2022-11-01\"\n",
    "\n",
    "SEED=42\n",
    "class MicroDensityDataset(Dataset):\n",
    "    def __init__(self, type, seq_len, stride=1,use_census=USE_CENSUS):\n",
    "        self.type = type\n",
    "        self.seq_len = seq_len\n",
    "        self.stride = stride if type == DatasetType.TRAIN else 1\n",
    "        self.use_census = use_census\n",
    "        self.load_data()\n",
    "        self.prepare_sequences()\n",
    "\n",
    "\n",
    "    def init_transforms(self):\n",
    "        \"\"\"\n",
    "        Initialize transforms.Might be different for each dataset type\n",
    "        \"\"\"\n",
    "\n",
    "    def load_data(self):\n",
    "        \"\"\"\n",
    "        Load data from the data items if necessary\n",
    "        \"\"\"\n",
    "\n",
    "        self.main_file = os.path.join(DATA_DIR, \"train.csv\")\n",
    "        self.main_df = pd.read_csv(self.main_file)\n",
    "\n",
    "        if self.type == DatasetType.TEST:\n",
    "            self.test_df = pd.read_csv(TEST_FILE)\n",
    "            self.test_df[\"microbusiness_density\"] = [0 for _ in range(len(self.test_df))]\n",
    "            self.test_df[\"county\"] =[\"NAN\" for _ in range(len(self.test_df))]\n",
    "            self.test_df[\"state\"] =[\"NAN\" for _ in range(len(self.test_df))]\n",
    "\n",
    "            self.main_df = pd.concat([self.main_df, self.test_df], ignore_index=True)\n",
    "\n",
    "            self.test_df =self.test_df.sort_values(by=[\"cfips\",\"first_day_of_month\"])\n",
    "            self.test_df = self.test_df.reset_index(drop=True)\n",
    "\n",
    "        if self.use_census:\n",
    "            #Merge the census features\n",
    "            self.cfips_index=get_cfips_index()\n",
    "            self.census_df = pd.read_csv(CENSUS_FILE)\n",
    "\n",
    "            self.main_df=pd.merge(self.main_df,self.census_df,on=[\"cfips\",\"first_day_of_month\"],how=\"left\")\n",
    "\n",
    "\n",
    "        ##Group by cfips and sort by date\n",
    "        self.main_df=self.main_df.sort_values(by=[\"cfips\",\"first_day_of_month\"])\n",
    "        self.main_df[\"id\"] =list(range(len(self.main_df)))\n",
    "\n",
    "\n",
    "\n",
    "    def prepare_sequences(self):\n",
    "        \"\"\"\n",
    "        Prepare the sequences for the LSTM:\n",
    "        Build a list of (id(offset), id(seq_len+offset)) tuples\n",
    "        \"\"\"\n",
    "        self.sequences=[]\n",
    "\n",
    "        if self.type == DatasetType.TRAIN:\n",
    "            ##Train data are dates before EVAL_START_DATE\n",
    "            df=self.main_df[self.main_df['first_day_of_month']<EVAL_START_DATE]\n",
    "\n",
    "            for i in tqdm(range(0, len(df)-self.seq_len, self.stride), desc=\"Preparing sequences of dataset of type train\"):\n",
    "\n",
    "                ##The cfips should be the same for the whole sequence(just check the first and last rows)\n",
    "                if df.iloc[i][\"cfips\"] != df.iloc[i + self.seq_len - 1][\"cfips\"]:\n",
    "                    continue\n",
    "\n",
    "                if i + self.seq_len > len(df) :\n",
    "                    break\n",
    "\n",
    "                #Get the corresponding ids\n",
    "                self.sequences.append((df.iloc[i][\"id\"], df.iloc[i][\"id\"]+ self.seq_len))\n",
    "\n",
    "\n",
    "\n",
    "        else :\n",
    "\n",
    "\n",
    "            if self.type == DatasetType.VALID:\n",
    "                df = self.main_df[self.main_df['first_day_of_month'] >= EVAL_START_DATE]\n",
    "\n",
    "            else:\n",
    "                df = self.main_df[self.main_df['first_day_of_month'] >= TEST_START_DATE]\n",
    "\n",
    "\n",
    "            for i in tqdm(range(0, len(df),self.stride), desc=\"Preparing sequences of dataset of type {}\".format(\"eval\" if self.type == DatasetType.VALID else \"test\")):\n",
    "                ## In eval and test sequences, the step to predict should always be the last one of the sequence\n",
    "\n",
    "                ##Find the offest of the start in the main df\n",
    "\n",
    "\n",
    "                offset=df.iloc[i][\"id\"]\n",
    "\n",
    "                offset = offset - self.seq_len  # The step to predict is the last one of the sequence\n",
    "\n",
    "\n",
    "                ##check if the cfips is the same\n",
    "                if self.main_df.iloc[offset][\"cfips\"] != self.main_df.iloc[offset + self.seq_len - 1][\"cfips\"]:\n",
    "                    #Warning\n",
    "                    print(\"Warning: cfips is not the same for the whole sequence . Offsets :\",offset,offset + self.seq_len - 1)\n",
    "\n",
    "                self.sequences.append((offset, offset + self.seq_len))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sequences)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        \"\"\"\n",
    "        Retrieving seq_len data\n",
    "        1. The county (CFIPS) should be the same\n",
    "        2. And the difference between the date(first_day_of_month) should be at most 3 months\n",
    "        \"\"\"\n",
    "        start,end=self.sequences[item]\n",
    "        rows_data=self.main_df.iloc[start:end]\n",
    "\n",
    "\n",
    "        #ensure unique cfips\n",
    "        # assert len(rows_data[\"cfips\"].unique())==1\n",
    "\n",
    "        tensor = torch.tensor(rows_data[['microbusiness_density']].values,\n",
    "                                       dtype=torch.float32)  # Not considering the census features\n",
    "\n",
    "        #FEatures scaling\n",
    "\n",
    "\n",
    "        if self.use_census:\n",
    "            censur_features_tensor = extract_census_features(rows_data, cfips_index=self.cfips_index,single_row=False)\n",
    "            tensor = torch.cat((censur_features_tensor,tensor), dim=1)\n",
    "\n",
    "\n",
    "\n",
    "        return tensor\n",
    "\n",
    "\n",
    "\n",
    "    def mix_with(self, other_dataset, size=0.8):\n",
    "        \"\"\"\n",
    "        Combine two datasets exemple a train dataset and test dataset\n",
    "        @param other_dataset:\n",
    "        @param size:\n",
    "        @return:\n",
    "        \"\"\"\n",
    "\n",
    "        all_sequences= self.sequences + other_dataset.sequences\n",
    "        random.shuffle(all_sequences)\n",
    "        self.sequences=all_sequences[:int(len(all_sequences)*size)]\n",
    "        other_dataset.sequences=all_sequences[int(len(all_sequences)*size):]\n",
    "        logging.info(\"Combined dataset: {} sequences for train and {} sequences for test\".format(len(self.sequences),len(other_dataset.sequences)))\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "execution": {
     "iopub.status.busy": "2023-02-12T15:45:08.226218Z",
     "iopub.execute_input": "2023-02-12T15:45:08.226587Z",
     "iopub.status.idle": "2023-02-12T15:45:08.301550Z",
     "shell.execute_reply.started": "2023-02-12T15:45:08.226550Z",
     "shell.execute_reply": "2023-02-12T15:45:08.300210Z"
    },
    "trusted": true
   },
   "execution_count": 7,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Trainer"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import csv\n",
    "import json\n",
    "import logging\n",
    "import os\n",
    "import shutil\n",
    "import warnings\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from tqdm import tqdm\n",
    "class TrainerTransformerPredictor:\n",
    "    \"\"\"\n",
    "    Class to manage the full training pipeline\n",
    "    \"\"\"\n",
    "    def __init__(self, network: TransformerPredictor,\n",
    "                 criterion,\n",
    "                 optimizer,\n",
    "                 scheduler=None,\n",
    "                 nb_epochs=10, batch_size=128, reset=False):\n",
    "        \"\"\"\n",
    "        @param network:\n",
    "        @param dataset_name:\n",
    "        @param images_dirs:\n",
    "        @param loss:\n",
    "        @param optimizer:\n",
    "        @param nb_epochs:\n",
    "        @param nb_workers: Number of worker for the dataloader\n",
    "        \"\"\"\n",
    "        self.network = network\n",
    "        self.batch_size = batch_size\n",
    "        self.loss_fn=criterion\n",
    "\n",
    "        self.optimizer = optimizer\n",
    "        self.scheduler =scheduler if scheduler else\\\n",
    "            torch.optim.lr_scheduler.ReduceLROnPlateau(self.optimizer, mode='min', factor=0.2, patience=10,min_lr=1e-5)\n",
    "\n",
    "        self.nb_epochs = nb_epochs\n",
    "        self.experiment_dir = self.network.experiment_dir\n",
    "        self.model_info_file = os.path.join(self.experiment_dir, \"model.json\")\n",
    "        self.model_info_best_file = os.path.join(self.experiment_dir, \"model_best.json\")\n",
    "\n",
    "        if reset:\n",
    "            if os.path.exists(self.experiment_dir):\n",
    "                shutil.rmtree(self.experiment_dir)\n",
    "        if not os.path.exists(self.experiment_dir):\n",
    "            os.makedirs(self.experiment_dir)\n",
    "\n",
    "        self.start_epoch = 0\n",
    "        if not reset and os.path.exists(self.model_info_file):\n",
    "            with open(self.model_info_file, \"r\") as f:\n",
    "                self.start_epoch = json.load(f)[\"epoch\"] + 1\n",
    "                self.nb_epochs += self.start_epoch\n",
    "                logging.info(\"Resuming from epoch {}\".format(self.start_epoch))\n",
    "\n",
    "\n",
    "    def save_model_info(self, infos, best=False):\n",
    "        json.dump(infos, open(self.model_info_file, 'w'),indent=4)\n",
    "        if best: json.dump(infos, open(self.model_info_best_file, 'w'),indent=4)\n",
    "\n",
    "    def fit(self,train_dataloader,val_dataloader):\n",
    "        logging.info(\"Launch training on {}\".format(DEVICE))\n",
    "        if self.network.use_census:\n",
    "            logging.info(\"Using encoder census data\")\n",
    "\n",
    "        self.summary_writer = SummaryWriter(log_dir=self.experiment_dir)\n",
    "        itr = self.start_epoch * len(train_dataloader) * self.batch_size  ##Global counter for steps\n",
    "\n",
    "        #Save model graph\n",
    "        # self.summary_writer.add_graph(self.network, next(iter(train_dataloader)).to(DEVICE)[:,:-1,:])\n",
    "\n",
    "        self.best_val_loss = 1e20  # infinity\n",
    "        if os.path.exists(self.model_info_file):\n",
    "            with open(self.model_info_file, \"r\") as f:\n",
    "                model_info = json.load(f)\n",
    "                lr=model_info[\"lr\"]\n",
    "                logging.info(f\"Setting lr to {lr}\")\n",
    "                for g in self.optimizer.param_groups:\n",
    "                    g['lr'] = lr\n",
    "\n",
    "        if os.path.exists(self.model_info_best_file):\n",
    "            with open(self.model_info_best_file, \"r\") as f:\n",
    "                best_model_info = json.load(f)\n",
    "                self.best_val_loss = best_model_info[\"val_loss\"]\n",
    "\n",
    "\n",
    "        for epoch in range(self.start_epoch, self.nb_epochs):  # Training loop\n",
    "            self.network.train()\n",
    "            \"\"\"\"\n",
    "            0. Initialize loss and other metrics\n",
    "            \"\"\"\n",
    "            running_loss=Averager()\n",
    "            pbar = tqdm(train_dataloader, desc=f\"Epoch {epoch + 1}/{self.nb_epochs}\")\n",
    "            for _, batch in enumerate(pbar):\n",
    "                \"\"\"\n",
    "                Training lopp\n",
    "                \"\"\"\n",
    "                self.optimizer.zero_grad()\n",
    "                itr += self.batch_size\n",
    "                \"\"\"\n",
    "                1.Forward pass\n",
    "                \"\"\"\n",
    "                batch = batch.to(DEVICE)\n",
    "\n",
    "                y_pred = self.network(batch)\n",
    "                ## The output is the values of the density for each time step\n",
    "\n",
    "                \"\"\"\n",
    "                2.Loss computation and other metrics\n",
    "                \"\"\"\n",
    "                # The density is the last item of the batch\n",
    "                y_true = batch[:,:,-1].to(DEVICE)\n",
    "                loss = self.loss_fn(y_pred, y_true[:, -1:])\n",
    "\n",
    "                \"\"\"\n",
    "                3.Optimizing\n",
    "                \"\"\"\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "                running_loss.send(loss.cpu().item())\n",
    "                pbar.set_postfix(current_loss=loss.cpu().item(), current_mean_loss=running_loss.value)\n",
    "\n",
    "                \"\"\"\n",
    "                4.Writing logs and tensorboard data, loss and other metrics\n",
    "                \"\"\"\n",
    "                self.summary_writer.add_scalar(\"Train/loss\", loss.item(), itr)\n",
    "\n",
    "\n",
    "\n",
    "            #If step lr scheduler\n",
    "            if isinstance(self.scheduler,torch.optim.lr_scheduler.StepLR):\n",
    "                self.scheduler.step()\n",
    "            else:\n",
    "                self.scheduler.step(epoch_val_loss.value)\n",
    "            epoch_val_loss =self.eval(val_dataloader,epoch)\n",
    "\n",
    "            infos = {\n",
    "                \"epoch\": epoch,\n",
    "                \"train_loss\":running_loss.value,\n",
    "                \"val_loss\":epoch_val_loss.value,\n",
    "                \"lr\": self.optimizer.param_groups[0]['lr'],\n",
    "                \"input_dim\": self.network.input_dim,\n",
    "                \"emb_dim\": self.network.emb_dim,\n",
    "                \"dim_feedforward\": self.network.dim_feedforward,\n",
    "                \"n_head\": self.network.n_head,\n",
    "                \"n_layers\": self.network.n_layers,\n",
    "                \"seq_len\": train_dataloader.dataset.seq_len,\n",
    "                \"batch_size\": train_dataloader.batch_size,\n",
    "                \"stride\": train_dataloader.dataset.stride,\n",
    "                \"use_census\": self.network.use_census,\n",
    "                \"variante\": self.network.variante_num,\n",
    "\n",
    "            }\n",
    "\n",
    "            logging.info(\"Epoch {} - Train loss: {:.4f} - Val loss: {:.4f}\".format(epoch, running_loss.value, epoch_val_loss.value))\n",
    "\n",
    "            if epoch_val_loss.value < self.best_val_loss:\n",
    "                self.best_val_loss = epoch_val_loss.value\n",
    "                best = True\n",
    "            else:\n",
    "                best = False\n",
    "\n",
    "            self.network.save_state(best=best)\n",
    "            self.save_model_info(infos, best=best)\n",
    "\n",
    "\n",
    "            self.summary_writer.add_scalar(\"Epoch_train/loss\", running_loss.value, epoch)\n",
    "            self.summary_writer.add_scalar(\"Epoch_val/loss\", epoch_val_loss.value, epoch)\n",
    "\n",
    "\n",
    "\n",
    "    def eval(self, val_dataloader,epoch):\n",
    "        \"\"\"\n",
    "        Compute loss and metrics on a validation dataloader\n",
    "        @return:\n",
    "        \"\"\"\n",
    "        with torch.no_grad():\n",
    "            self.network.eval()\n",
    "            running_loss=Averager()\n",
    "            pbar = tqdm(val_dataloader, desc=f\"Validation Epoch {epoch + 1}/{self.nb_epochs}\")\n",
    "            for _, batch in enumerate(pbar):\n",
    "\n",
    "                \"\"\"\n",
    "                Training lopp\n",
    "                \"\"\"\n",
    "                \"\"\"\n",
    "                1.Forward pass\n",
    "                \"\"\"\n",
    "                batch=batch.to(DEVICE)\n",
    "                y_pred = self.network(batch)\n",
    "                \"\"\"\n",
    "                2.Loss computation and other metrics\n",
    "                \"\"\"\n",
    "                y_true = batch[:,:,-1]\n",
    "\n",
    "\n",
    "                loss = self.loss_fn(y_pred, y_true[:, -1:])\n",
    "\n",
    "                running_loss.send(loss.item())\n",
    "\n",
    "                pbar.set_postfix(current_loss=loss.item(), current_mean_loss=running_loss.value)\n",
    "\n",
    "\n",
    "        return running_loss\n",
    "\n",
    "\n",
    "\n",
    "    def run_test(self, test_dataloader):\n",
    "        \"\"\"\n",
    "        Compute loss and metrics on a validation dataloader\n",
    "        @return:\n",
    "        \"\"\"\n",
    "        assert test_dataloader.batch_size == 1, \"Batch size must be 1 for test\"\n",
    "        predictions = []\n",
    "        row_ids = []\n",
    "        with torch.no_grad():\n",
    "            self.network.eval()\n",
    "            for i, input in enumerate(tqdm(test_dataloader,\" Running tests for submission\")):\n",
    "                input = input.to(DEVICE)\n",
    "                y_pred = self.network(input.to(DEVICE)).cpu().squeeze().item()\n",
    "\n",
    "                # Denormalize. MEAN_MB, STD_MB (if noramlized)\n",
    "                # y_pred = y_pred * STD_MB + MEAN_MB\n",
    "                \"\"\"\n",
    "                2.Loss computation and other metrics\n",
    "                \"\"\"\n",
    "                predictions.append(y_pred)\n",
    "\n",
    "                ##Update all microbusiness_den isty column\n",
    "                row_id=test_dataloader.dataset.test_df.loc[i,\"row_id\"]\n",
    "                row_ids.append(row_id)\n",
    "\n",
    "                test_dataloader.dataset.main_df.loc[test_dataloader.dataset.main_df[\"row_id\"]==row_id,\"microbusiness_density\"]=y_pred\n",
    "\n",
    "\n",
    "        #Merge predictions\n",
    "        predictions=np.array(predictions)\n",
    "\n",
    "\n",
    "        #Update all microbusiness_denisty column\n",
    "\n",
    "        pred_test_df = pd.DataFrame(\n",
    "            {\n",
    "                \"row_id\":row_ids,\n",
    "                 \"microbusiness_density\":predictions}\n",
    "\n",
    "                                )\n",
    "        pred_test_df.to_csv(os.path.join(self.experiment_dir,\"submission.csv\"),index=False)\n",
    "\n",
    "        return pred_test_df\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "execution": {
     "iopub.status.busy": "2023-02-12T15:45:08.306778Z",
     "iopub.execute_input": "2023-02-12T15:45:08.307171Z",
     "iopub.status.idle": "2023-02-12T15:45:08.696308Z",
     "shell.execute_reply.started": "2023-02-12T15:45:08.307137Z",
     "shell.execute_reply": "2023-02-12T15:45:08.695339Z"
    },
    "trusted": true
   },
   "execution_count": 8,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Runner"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from dataclasses import dataclass\n",
    "import argparse\n",
    "import logging\n",
    "import os\n",
    "import pickle\n",
    "import torch.utils.data\n",
    "\n",
    "@dataclass\n",
    "class Arguments:\n",
    "    reset: bool = False\n",
    "    learning_rate: float = 0.001\n",
    "    nb_epochs: int = 1000\n",
    "    model_name: str = None\n",
    "    num_workers: int = 2\n",
    "    batch_size: int = 256\n",
    "    log_level: str = \"INFO\"\n",
    "    autorun_tb: bool = True\n",
    "    use_census: bool = True\n",
    "    use_derivative: bool = True\n",
    "    seq_len: int = 20\n",
    "    seq_stride: int = 1\n",
    "    emb_dim: int = 64\n",
    "    n_layers: int =8\n",
    "    n_head: int = 8\n",
    "    dim_feedforward: int = 256\n",
    "\n",
    "def cli():\n",
    "    return Arguments()\n",
    "\n",
    "def main(args):\n",
    "\n",
    "    #Format the model name\n",
    "    if args.model_name is None:\n",
    "        model_name=f\"trf_{'ae_' if args.use_census else ''}{'dv_' if args.use_derivative else ''}ed.{args.emb_dim}_nl.{args.n_layers}_nh.{args.n_head}_df.{args.dim_feedforward}_sl.{args.seq_len}_ss.{args.seq_stride}_lr.{args.learning_rate}_bs.{args.batch_size}\"\n",
    "    else :\n",
    "        model_name=args.model_name\n",
    "\n",
    "\n",
    "\n",
    "    experiment_dir = os.path.join(EXPERIMENTS_DIR, model_name)\n",
    "\n",
    "    network =TransformerPredictor(\n",
    "                                    experiment_dir=experiment_dir,\n",
    "                                    emb_dim=args.emb_dim,\n",
    "                                      n_layers=args.n_layers,\n",
    "                                      n_head=args.n_head,\n",
    "                                      dim_feedforward=args.dim_feedforward,\n",
    "                                      use_census=args.use_census,\n",
    "                                    max_seq_len=args.seq_len-1,\n",
    "                                    reset=args.reset\n",
    "                ).to(DEVICE)\n",
    "    \n",
    "    #Using multiple gpus\n",
    "    \n",
    "    #Adam optimizer\n",
    "    optimizer = torch.optim.Adam(network.parameters(), lr=args.learning_rate)\n",
    "\n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=150, gamma=0.5)#Reduce the learning rate by half every 150 epochs\n",
    "\n",
    "    criterion= SmapeCriterion().to(DEVICE)\n",
    "\n",
    "\n",
    "    logging.info(\"Training : \"+model_name)\n",
    "    trainer = TrainerTransformerPredictor(network,\n",
    "                      criterion,\n",
    "                      optimizer=optimizer,\n",
    "                      scheduler=scheduler,\n",
    "                      nb_epochs= args.nb_epochs,\n",
    "                      batch_size=args.batch_size,\n",
    "                      reset=args.reset,\n",
    "                      )\n",
    "\n",
    "    # Save  the dataset according to type, seq_len_stride and use_census: using pickle\n",
    "\n",
    "    if not os.path.exists(os.path.join(ROOT_DIR,\"dataset\",\"pickle\")):\n",
    "        os.makedirs(os.path.join(ROOT_DIR,\"dataset\",\"pickle\"))\n",
    "\n",
    "    datasets_pickle_path = os.path.join(ROOT_DIR,\"dataset\",\"pickle\",f\"all_dataset_{args.seq_len}_{args.seq_stride}_{args.use_census}.pickle\")\n",
    "\n",
    "\n",
    "    if not os.path.exists(datasets_pickle_path):\n",
    "        train_dataset = MicroDensityDataset(type=DatasetType.TRAIN, seq_len=args.seq_len, stride=args.seq_stride,\n",
    "                                            use_census=args.use_census)\n",
    "        val_dataset = MicroDensityDataset(type=DatasetType.VALID, seq_len=args.seq_len, stride=args.seq_stride,\n",
    "                                          use_census=args.use_census)\n",
    "\n",
    "        train_dataset.mix_with(val_dataset,size=0.8) #Mix train and val dataset to avoid disparity between the two in terms of dates distribution\n",
    "\n",
    "        test_dataset = MicroDensityDataset(type=DatasetType.TEST, seq_len=args.seq_len, stride=args.seq_stride,\n",
    "                                           use_census=args.use_census)\n",
    "\n",
    "        with open(datasets_pickle_path,\"wb\") as f:\n",
    "            logging.info(f\"Saving datasets to {datasets_pickle_path}\")\n",
    "            pickle.dump((train_dataset,val_dataset,test_dataset),f)\n",
    "    else:\n",
    "        with open(datasets_pickle_path,\"rb\") as f:\n",
    "            logging.info(f\"Loading datasets  from {datasets_pickle_path}\")\n",
    "            train_dataset,val_dataset,test_dataset = pickle.load(f)\n",
    "\n",
    "\n",
    "\n",
    "    logging.info(f\"Nb sequences : Train {len(train_dataset)} - Val {len(val_dataset)} - Test {len(test_dataset)}\")\n",
    "\n",
    "    train_dataloader=torch.utils.data.DataLoader(train_dataset,batch_size=args.batch_size,num_workers=args.num_workers,shuffle=True,drop_last=False,persistent_workers=True)\n",
    "    val_dataloader = torch.utils.data.DataLoader(val_dataset, batch_size=args.batch_size,num_workers=0,drop_last=False)\n",
    "    test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=1,num_workers=0,drop_last=False,shuffle=False)\n",
    "\n",
    "    ##Train\n",
    "    trainer.fit(train_dataloader,val_dataloader)\n",
    "\n",
    "    ##Load best model\n",
    "    trainer.network.load_state(best=True)\n",
    "    trainer.run_test(test_dataloader=test_dataloader)\n",
    "\n",
    "\n",
    "\n",
    "args = cli()\n",
    "setup_logger(args)\n",
    "main(args)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "execution": {
     "iopub.status.busy": "2023-02-12T15:45:08.697580Z",
     "iopub.execute_input": "2023-02-12T15:45:08.698140Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "text": "2023-02-12 15:45:12,272 - root - INFO - Training : trf_ae_dv_ed.64_nl.8_nh.8_df.256_sl.20_ss.1_lr.0.001_bs.256\n",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": "Preparing sequences of dataset of type train: 100%|ââââââââââ| 103435/103435 [00:36<00:00, 2819.21it/s]\nPreparing sequences of dataset of type eval: 100%|ââââââââââ| 18810/18810 [00:07<00:00, 2441.25it/s]",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "2023-02-12 15:45:58,403 - root - INFO - Combined dataset: 50159 sequences for train and 12540 sequences for test\n",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": "\nPreparing sequences of dataset of type test: 100%|ââââââââââ| 25080/25080 [00:09<00:00, 2636.44it/s]",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "2023-02-12 15:46:08,581 - root - INFO - Saving datasets to /kaggle/dataset/pickle/all_dataset_20_1_True.pickle\n",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": "\n",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "2023-02-12 15:46:09,391 - root - INFO - Nb sequences : Train 50159 - Val 12540 - Test 25080\n2023-02-12 15:46:09,392 - root - INFO - Launch training on cuda\n2023-02-12 15:46:09,393 - root - INFO - Using encoder census data\n",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": "Epoch 1/1000: 100%|ââââââââââ| 196/196 [01:08<00:00,  2.84it/s, current_loss=5.71, current_mean_loss=16.9]\nValidation Epoch 1/1000: 100%|ââââââââââ| 49/49 [00:19<00:00,  2.48it/s, current_loss=8.46, current_mean_loss=9.26]",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "2023-02-12 15:47:42,247 - root - INFO - Epoch 0 - Train loss: 16.8900 - Val loss: 9.2609\n2023-02-12 15:47:42,251 - root - INFO - Saving best model\n",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": "\nEpoch 2/1000: 100%|ââââââââââ| 196/196 [01:06<00:00,  2.96it/s, current_loss=4.25, current_mean_loss=6.41]\nValidation Epoch 2/1000: 100%|ââââââââââ| 49/49 [00:19<00:00,  2.51it/s, current_loss=4.78, current_mean_loss=5.41]",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "2023-02-12 15:49:08,007 - root - INFO - Epoch 1 - Train loss: 6.4054 - Val loss: 5.4110\n2023-02-12 15:49:08,008 - root - INFO - Saving best model\n",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": "\nEpoch 3/1000: 100%|ââââââââââ| 196/196 [01:06<00:00,  2.97it/s, current_loss=3.3, current_mean_loss=4.98] \nValidation Epoch 3/1000: 100%|ââââââââââ| 49/49 [00:19<00:00,  2.52it/s, current_loss=3.81, current_mean_loss=4.21]",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "2023-02-12 15:50:33,562 - root - INFO - Epoch 2 - Train loss: 4.9770 - Val loss: 4.2076\n2023-02-12 15:50:33,564 - root - INFO - Saving best model\n",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": "\nEpoch 4/1000: 100%|ââââââââââ| 196/196 [01:06<00:00,  2.95it/s, current_loss=7.36, current_mean_loss=4.04]\nValidation Epoch 4/1000: 100%|ââââââââââ| 49/49 [00:19<00:00,  2.50it/s, current_loss=4.32, current_mean_loss=4.23]",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "2023-02-12 15:51:59,690 - root - INFO - Epoch 3 - Train loss: 4.0414 - Val loss: 4.2336\n",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": "\nEpoch 5/1000: 100%|ââââââââââ| 196/196 [01:06<00:00,  2.97it/s, current_loss=4.59, current_mean_loss=3.97]\nValidation Epoch 5/1000: 100%|ââââââââââ| 49/49 [00:19<00:00,  2.50it/s, current_loss=3.57, current_mean_loss=3.81]",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "2023-02-12 15:53:25,435 - root - INFO - Epoch 4 - Train loss: 3.9659 - Val loss: 3.8146\n2023-02-12 15:53:25,436 - root - INFO - Saving best model\n",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": "\nEpoch 6/1000: 100%|ââââââââââ| 196/196 [01:06<00:00,  2.97it/s, current_loss=4.25, current_mean_loss=3.49]\nValidation Epoch 6/1000: 100%|ââââââââââ| 49/49 [00:19<00:00,  2.52it/s, current_loss=2.62, current_mean_loss=3]   ",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "2023-02-12 15:54:50,994 - root - INFO - Epoch 5 - Train loss: 3.4893 - Val loss: 2.9974\n2023-02-12 15:54:50,997 - root - INFO - Saving best model\n",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": "\nEpoch 7/1000: 100%|ââââââââââ| 196/196 [01:05<00:00,  2.99it/s, current_loss=2.49, current_mean_loss=3.42]\nValidation Epoch 7/1000: 100%|ââââââââââ| 49/49 [00:19<00:00,  2.54it/s, current_loss=3.89, current_mean_loss=3.87]",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "2023-02-12 15:56:16,031 - root - INFO - Epoch 6 - Train loss: 3.4230 - Val loss: 3.8698\n",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": "\nEpoch 8/1000: 100%|ââââââââââ| 196/196 [01:05<00:00,  2.98it/s, current_loss=3.47, current_mean_loss=3.57]\nValidation Epoch 8/1000: 100%|ââââââââââ| 49/49 [00:19<00:00,  2.48it/s, current_loss=3.02, current_mean_loss=3.44]",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "2023-02-12 15:57:41,669 - root - INFO - Epoch 7 - Train loss: 3.5735 - Val loss: 3.4387\n",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": "\nEpoch 9/1000: 100%|ââââââââââ| 196/196 [01:06<00:00,  2.96it/s, current_loss=4.61, current_mean_loss=2.99]\nValidation Epoch 9/1000: 100%|ââââââââââ| 49/49 [00:19<00:00,  2.52it/s, current_loss=3.23, current_mean_loss=4.36]",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "2023-02-12 15:59:07,311 - root - INFO - Epoch 8 - Train loss: 2.9936 - Val loss: 4.3565\n",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": "\nEpoch 10/1000: 100%|ââââââââââ| 196/196 [01:05<00:00,  2.98it/s, current_loss=3.12, current_mean_loss=3.72]\nValidation Epoch 10/1000: 100%|ââââââââââ| 49/49 [00:18<00:00,  2.60it/s, current_loss=1.82, current_mean_loss=1.83]",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "2023-02-12 16:00:32,089 - root - INFO - Epoch 9 - Train loss: 3.7212 - Val loss: 1.8338\n2023-02-12 16:00:32,090 - root - INFO - Saving best model\n",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": "\nEpoch 11/1000: 100%|ââââââââââ| 196/196 [01:06<00:00,  2.95it/s, current_loss=1.62, current_mean_loss=2.92]\nValidation Epoch 11/1000: 100%|ââââââââââ| 49/49 [00:19<00:00,  2.52it/s, current_loss=1.59, current_mean_loss=1.73]",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "2023-02-12 16:01:58,149 - root - INFO - Epoch 10 - Train loss: 2.9166 - Val loss: 1.7316\n2023-02-12 16:01:58,151 - root - INFO - Saving best model\n",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": "\nEpoch 12/1000: 100%|ââââââââââ| 196/196 [01:05<00:00,  2.97it/s, current_loss=2.69, current_mean_loss=2.93]\nValidation Epoch 12/1000: 100%|ââââââââââ| 49/49 [00:19<00:00,  2.50it/s, current_loss=3.31, current_mean_loss=3.74]",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "2023-02-12 16:03:23,837 - root - INFO - Epoch 11 - Train loss: 2.9264 - Val loss: 3.7361\n",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": "\nEpoch 13/1000: 100%|ââââââââââ| 196/196 [01:05<00:00,  2.99it/s, current_loss=2, current_mean_loss=2.73]   \nValidation Epoch 13/1000: 100%|ââââââââââ| 49/49 [00:19<00:00,  2.51it/s, current_loss=1.78, current_mean_loss=1.82]",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "2023-02-12 16:04:48,961 - root - INFO - Epoch 12 - Train loss: 2.7328 - Val loss: 1.8189\n",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": "\nEpoch 14/1000: 100%|ââââââââââ| 196/196 [01:05<00:00,  3.00it/s, current_loss=4.4, current_mean_loss=2.8]  \nValidation Epoch 14/1000: 100%|ââââââââââ| 49/49 [00:19<00:00,  2.51it/s, current_loss=4.64, current_mean_loss=4.55]",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "2023-02-12 16:06:13,903 - root - INFO - Epoch 13 - Train loss: 2.7973 - Val loss: 4.5478\n",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": "\nEpoch 15/1000: 100%|ââââââââââ| 196/196 [01:06<00:00,  2.96it/s, current_loss=2.36, current_mean_loss=3.08]\nValidation Epoch 15/1000: 100%|ââââââââââ| 49/49 [00:19<00:00,  2.52it/s, current_loss=4.13, current_mean_loss=4.26]",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "2023-02-12 16:07:39,719 - root - INFO - Epoch 14 - Train loss: 3.0818 - Val loss: 4.2640\n",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": "\nEpoch 16/1000: 100%|ââââââââââ| 196/196 [01:05<00:00,  2.99it/s, current_loss=3.33, current_mean_loss=2.53]\nValidation Epoch 16/1000: 100%|ââââââââââ| 49/49 [00:19<00:00,  2.51it/s, current_loss=5.11, current_mean_loss=4.88]",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "2023-02-12 16:09:04,781 - root - INFO - Epoch 15 - Train loss: 2.5343 - Val loss: 4.8847\n",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": "\nEpoch 17/1000: 100%|ââââââââââ| 196/196 [01:05<00:00,  2.98it/s, current_loss=2.08, current_mean_loss=2.75]\nValidation Epoch 17/1000: 100%|ââââââââââ| 49/49 [00:19<00:00,  2.49it/s, current_loss=2.41, current_mean_loss=2.56]",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "2023-02-12 16:10:30,290 - root - INFO - Epoch 16 - Train loss: 2.7470 - Val loss: 2.5624\n",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": "\nEpoch 18/1000: 100%|ââââââââââ| 196/196 [01:05<00:00,  2.99it/s, current_loss=1.88, current_mean_loss=2.52]\nValidation Epoch 18/1000: 100%|ââââââââââ| 49/49 [00:19<00:00,  2.50it/s, current_loss=1.91, current_mean_loss=2.02]",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "2023-02-12 16:11:55,551 - root - INFO - Epoch 17 - Train loss: 2.5242 - Val loss: 2.0226\n",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": "\nEpoch 19/1000: 100%|ââââââââââ| 196/196 [01:05<00:00,  2.99it/s, current_loss=1.71, current_mean_loss=2.21]\nValidation Epoch 19/1000: 100%|ââââââââââ| 49/49 [00:19<00:00,  2.56it/s, current_loss=2.06, current_mean_loss=2.08]",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "2023-02-12 16:13:20,259 - root - INFO - Epoch 18 - Train loss: 2.2087 - Val loss: 2.0816\n",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": "\nEpoch 20/1000: 100%|ââââââââââ| 196/196 [01:05<00:00,  3.01it/s, current_loss=2.14, current_mean_loss=2.38]\nValidation Epoch 20/1000: 100%|ââââââââââ| 49/49 [00:19<00:00,  2.55it/s, current_loss=2.24, current_mean_loss=2.37]",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "2023-02-12 16:14:44,550 - root - INFO - Epoch 19 - Train loss: 2.3808 - Val loss: 2.3725\n",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": "\nEpoch 21/1000: 100%|ââââââââââ| 196/196 [01:04<00:00,  3.03it/s, current_loss=2.21, current_mean_loss=2.57]\nValidation Epoch 21/1000: 100%|ââââââââââ| 49/49 [00:19<00:00,  2.51it/s, current_loss=1.84, current_mean_loss=1.99]",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "2023-02-12 16:16:08,777 - root - INFO - Epoch 20 - Train loss: 2.5687 - Val loss: 1.9851\n",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": "\nEpoch 22/1000: 100%|ââââââââââ| 196/196 [01:03<00:00,  3.06it/s, current_loss=1.55, current_mean_loss=2.07]\nValidation Epoch 22/1000: 100%|ââââââââââ| 49/49 [00:18<00:00,  2.62it/s, current_loss=1.82, current_mean_loss=1.93]",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "2023-02-12 16:17:31,501 - root - INFO - Epoch 21 - Train loss: 2.0726 - Val loss: 1.9321\n",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": "\nEpoch 23/1000: 100%|ââââââââââ| 196/196 [01:04<00:00,  3.05it/s, current_loss=5.45, current_mean_loss=2.25]\nValidation Epoch 23/1000: 100%|ââââââââââ| 49/49 [00:19<00:00,  2.54it/s, current_loss=2.96, current_mean_loss=3.01]",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "2023-02-12 16:18:55,074 - root - INFO - Epoch 22 - Train loss: 2.2474 - Val loss: 3.0097\n",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": "\nEpoch 24/1000: 100%|ââââââââââ| 196/196 [01:03<00:00,  3.09it/s, current_loss=2.07, current_mean_loss=2.28]\nValidation Epoch 24/1000: 100%|ââââââââââ| 49/49 [00:19<00:00,  2.54it/s, current_loss=2.06, current_mean_loss=2.18]",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "2023-02-12 16:20:17,945 - root - INFO - Epoch 23 - Train loss: 2.2799 - Val loss: 2.1846\n",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": "\nEpoch 25/1000: 100%|ââââââââââ| 196/196 [01:04<00:00,  3.04it/s, current_loss=1.63, current_mean_loss=2.24]\nValidation Epoch 25/1000: 100%|ââââââââââ| 49/49 [00:18<00:00,  2.61it/s, current_loss=1.79, current_mean_loss=1.87]",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "2023-02-12 16:21:41,199 - root - INFO - Epoch 24 - Train loss: 2.2447 - Val loss: 1.8703\n",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": "\nEpoch 26/1000: 100%|ââââââââââ| 196/196 [01:04<00:00,  3.06it/s, current_loss=2.26, current_mean_loss=2.09]\nValidation Epoch 26/1000: 100%|ââââââââââ| 49/49 [00:19<00:00,  2.52it/s, current_loss=2.19, current_mean_loss=2.22]",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "2023-02-12 16:23:04,725 - root - INFO - Epoch 25 - Train loss: 2.0871 - Val loss: 2.2194\n",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": "\nEpoch 27/1000: 100%|ââââââââââ| 196/196 [01:04<00:00,  3.04it/s, current_loss=1.99, current_mean_loss=2.37]\nValidation Epoch 27/1000: 100%|ââââââââââ| 49/49 [00:19<00:00,  2.56it/s, current_loss=1.84, current_mean_loss=2.05]",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "2023-02-12 16:24:28,357 - root - INFO - Epoch 26 - Train loss: 2.3735 - Val loss: 2.0483\n",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": "\nEpoch 28/1000: 100%|ââââââââââ| 196/196 [01:05<00:00,  3.00it/s, current_loss=2.16, current_mean_loss=2.13]\nValidation Epoch 28/1000: 100%|ââââââââââ| 49/49 [00:18<00:00,  2.60it/s, current_loss=1.91, current_mean_loss=1.86]",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "2023-02-12 16:25:52,577 - root - INFO - Epoch 27 - Train loss: 2.1327 - Val loss: 1.8642\n",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": "\nEpoch 29/1000: 100%|ââââââââââ| 196/196 [01:04<00:00,  3.04it/s, current_loss=2.87, current_mean_loss=2.09]\nValidation Epoch 29/1000: 100%|ââââââââââ| 49/49 [00:19<00:00,  2.55it/s, current_loss=2.27, current_mean_loss=2.41]",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "2023-02-12 16:27:16,437 - root - INFO - Epoch 28 - Train loss: 2.0888 - Val loss: 2.4128\n",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": "\nEpoch 30/1000: 100%|ââââââââââ| 196/196 [01:04<00:00,  3.04it/s, current_loss=2.62, current_mean_loss=2.15]\nValidation Epoch 30/1000: 100%|ââââââââââ| 49/49 [00:18<00:00,  2.62it/s, current_loss=3.45, current_mean_loss=3.28]",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "2023-02-12 16:28:39,664 - root - INFO - Epoch 29 - Train loss: 2.1488 - Val loss: 3.2847\n",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": "\nEpoch 31/1000: 100%|ââââââââââ| 196/196 [01:04<00:00,  3.05it/s, current_loss=1.79, current_mean_loss=2]   \nValidation Epoch 31/1000: 100%|ââââââââââ| 49/49 [00:19<00:00,  2.50it/s, current_loss=2.04, current_mean_loss=2.07]",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "2023-02-12 16:30:03,643 - root - INFO - Epoch 30 - Train loss: 1.9973 - Val loss: 2.0734\n",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": "\nEpoch 32/1000: 100%|ââââââââââ| 196/196 [01:04<00:00,  3.03it/s, current_loss=2.21, current_mean_loss=2.16]\nValidation Epoch 32/1000: 100%|ââââââââââ| 49/49 [00:19<00:00,  2.56it/s, current_loss=2.07, current_mean_loss=2.25]",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "2023-02-12 16:31:27,599 - root - INFO - Epoch 31 - Train loss: 2.1626 - Val loss: 2.2509\n",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": "\nEpoch 33/1000: 100%|ââââââââââ| 196/196 [01:04<00:00,  3.04it/s, current_loss=2.51, current_mean_loss=2.41]\nValidation Epoch 33/1000: 100%|ââââââââââ| 49/49 [00:18<00:00,  2.64it/s, current_loss=2.49, current_mean_loss=2.41]",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "2023-02-12 16:32:50,725 - root - INFO - Epoch 32 - Train loss: 2.4067 - Val loss: 2.4108\n",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": "\nEpoch 34/1000: 100%|ââââââââââ| 196/196 [01:04<00:00,  3.04it/s, current_loss=2.25, current_mean_loss=2.1] \nValidation Epoch 34/1000: 100%|ââââââââââ| 49/49 [00:19<00:00,  2.54it/s, current_loss=2.64, current_mean_loss=2.51]",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "2023-02-12 16:34:14,620 - root - INFO - Epoch 33 - Train loss: 2.0979 - Val loss: 2.5148\n",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": "\nEpoch 35/1000: 100%|ââââââââââ| 196/196 [01:05<00:00,  3.01it/s, current_loss=1.82, current_mean_loss=1.99]\nValidation Epoch 35/1000: 100%|ââââââââââ| 49/49 [00:19<00:00,  2.55it/s, current_loss=1.84, current_mean_loss=1.9] ",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "2023-02-12 16:35:38,938 - root - INFO - Epoch 34 - Train loss: 1.9867 - Val loss: 1.8962\n",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": "\nEpoch 36/1000: 100%|ââââââââââ| 196/196 [01:04<00:00,  3.02it/s, current_loss=1.74, current_mean_loss=2.11]\nValidation Epoch 36/1000: 100%|ââââââââââ| 49/49 [00:18<00:00,  2.61it/s, current_loss=1.65, current_mean_loss=1.73]",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "2023-02-12 16:37:02,731 - root - INFO - Epoch 35 - Train loss: 2.1106 - Val loss: 1.7284\n2023-02-12 16:37:02,732 - root - INFO - Saving best model\n",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": "\nEpoch 37/1000: 100%|ââââââââââ| 196/196 [01:04<00:00,  3.02it/s, current_loss=1.84, current_mean_loss=1.98]\nValidation Epoch 37/1000: 100%|ââââââââââ| 49/49 [00:19<00:00,  2.56it/s, current_loss=1.96, current_mean_loss=1.98]",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "2023-02-12 16:38:26,912 - root - INFO - Epoch 36 - Train loss: 1.9753 - Val loss: 1.9794\n",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": "\nEpoch 38/1000: 100%|ââââââââââ| 196/196 [01:05<00:00,  2.99it/s, current_loss=1.67, current_mean_loss=2.22]\nValidation Epoch 38/1000: 100%|ââââââââââ| 49/49 [00:19<00:00,  2.49it/s, current_loss=2.1, current_mean_loss=2.26] ",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "2023-02-12 16:39:52,299 - root - INFO - Epoch 37 - Train loss: 2.2171 - Val loss: 2.2589\n",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": "\nEpoch 39/1000: 100%|ââââââââââ| 196/196 [01:05<00:00,  3.01it/s, current_loss=1.56, current_mean_loss=2.36]\nValidation Epoch 39/1000: 100%|ââââââââââ| 49/49 [00:18<00:00,  2.59it/s, current_loss=1.63, current_mean_loss=1.69]",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "2023-02-12 16:41:16,351 - root - INFO - Epoch 38 - Train loss: 2.3648 - Val loss: 1.6864\n2023-02-12 16:41:16,352 - root - INFO - Saving best model\n",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": "\nEpoch 40/1000: 100%|ââââââââââ| 196/196 [01:04<00:00,  3.04it/s, current_loss=1.74, current_mean_loss=1.87]\nValidation Epoch 40/1000: 100%|ââââââââââ| 49/49 [00:19<00:00,  2.53it/s, current_loss=2.31, current_mean_loss=2.32]",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "2023-02-12 16:42:40,357 - root - INFO - Epoch 39 - Train loss: 1.8742 - Val loss: 2.3229\n",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": "\nEpoch 41/1000: 100%|ââââââââââ| 196/196 [01:04<00:00,  3.03it/s, current_loss=1.77, current_mean_loss=2.04]\nValidation Epoch 41/1000: 100%|ââââââââââ| 49/49 [00:19<00:00,  2.48it/s, current_loss=1.78, current_mean_loss=1.92]",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "2023-02-12 16:44:04,809 - root - INFO - Epoch 40 - Train loss: 2.0400 - Val loss: 1.9194\n",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": "\nEpoch 42/1000: 100%|ââââââââââ| 196/196 [01:04<00:00,  3.02it/s, current_loss=2.31, current_mean_loss=1.96]\nValidation Epoch 42/1000: 100%|ââââââââââ| 49/49 [00:18<00:00,  2.63it/s, current_loss=1.97, current_mean_loss=2.13]",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "2023-02-12 16:45:28,515 - root - INFO - Epoch 41 - Train loss: 1.9591 - Val loss: 2.1341\n",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": "\nEpoch 43/1000: 100%|ââââââââââ| 196/196 [01:04<00:00,  3.03it/s, current_loss=1.6, current_mean_loss=2.01] \nValidation Epoch 43/1000: 100%|ââââââââââ| 49/49 [00:19<00:00,  2.51it/s, current_loss=2.21, current_mean_loss=2.2] ",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "2023-02-12 16:46:52,761 - root - INFO - Epoch 42 - Train loss: 2.0104 - Val loss: 2.2005\n",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": "\nEpoch 44/1000: 100%|ââââââââââ| 196/196 [01:04<00:00,  3.03it/s, current_loss=1.83, current_mean_loss=1.97]\nValidation Epoch 44/1000: 100%|ââââââââââ| 49/49 [00:18<00:00,  2.58it/s, current_loss=2.62, current_mean_loss=2.66]",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "2023-02-12 16:48:16,388 - root - INFO - Epoch 43 - Train loss: 1.9664 - Val loss: 2.6590\n",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": "\nEpoch 45/1000: 100%|ââââââââââ| 196/196 [01:04<00:00,  3.02it/s, current_loss=1.88, current_mean_loss=1.96]\nValidation Epoch 45/1000: 100%|ââââââââââ| 49/49 [00:19<00:00,  2.52it/s, current_loss=1.97, current_mean_loss=1.97]",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "2023-02-12 16:49:40,752 - root - INFO - Epoch 44 - Train loss: 1.9571 - Val loss: 1.9682\n",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": "\nEpoch 46/1000: 100%|ââââââââââ| 196/196 [01:04<00:00,  3.05it/s, current_loss=1.38, current_mean_loss=1.99]\nValidation Epoch 46/1000: 100%|ââââââââââ| 49/49 [00:19<00:00,  2.54it/s, current_loss=2.2, current_mean_loss=2.22] ",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "2023-02-12 16:51:04,324 - root - INFO - Epoch 45 - Train loss: 1.9917 - Val loss: 2.2180\n",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": "\nEpoch 47/1000: 100%|ââââââââââ| 196/196 [01:04<00:00,  3.03it/s, current_loss=2.87, current_mean_loss=1.9] \nValidation Epoch 47/1000: 100%|ââââââââââ| 49/49 [00:19<00:00,  2.55it/s, current_loss=2.15, current_mean_loss=2.19]",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "2023-02-12 16:52:28,320 - root - INFO - Epoch 46 - Train loss: 1.9045 - Val loss: 2.1945\n",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": "\nEpoch 48/1000: 100%|ââââââââââ| 196/196 [01:08<00:00,  2.87it/s, current_loss=1.45, current_mean_loss=1.89]\nValidation Epoch 48/1000: 100%|ââââââââââ| 49/49 [00:18<00:00,  2.61it/s, current_loss=1.65, current_mean_loss=1.68]",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "2023-02-12 16:53:55,394 - root - INFO - Epoch 47 - Train loss: 1.8899 - Val loss: 1.6791\n2023-02-12 16:53:55,396 - root - INFO - Saving best model\n",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": "\nEpoch 49/1000: 100%|ââââââââââ| 196/196 [01:05<00:00,  2.99it/s, current_loss=1.45, current_mean_loss=1.88]\nValidation Epoch 49/1000: 100%|ââââââââââ| 49/49 [00:19<00:00,  2.50it/s, current_loss=2.09, current_mean_loss=2.24]",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "2023-02-12 16:55:20,532 - root - INFO - Epoch 48 - Train loss: 1.8825 - Val loss: 2.2421\n",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": "\nEpoch 50/1000: 100%|ââââââââââ| 196/196 [01:04<00:00,  3.02it/s, current_loss=1.48, current_mean_loss=1.79]\nValidation Epoch 50/1000: 100%|ââââââââââ| 49/49 [00:18<00:00,  2.63it/s, current_loss=1.58, current_mean_loss=1.65]",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "2023-02-12 16:56:44,165 - root - INFO - Epoch 49 - Train loss: 1.7937 - Val loss: 1.6508\n2023-02-12 16:56:44,166 - root - INFO - Saving best model\n",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": "\nEpoch 51/1000: 100%|ââââââââââ| 196/196 [01:04<00:00,  3.04it/s, current_loss=2.64, current_mean_loss=1.86]\nValidation Epoch 51/1000: 100%|ââââââââââ| 49/49 [00:19<00:00,  2.58it/s, current_loss=2.66, current_mean_loss=2.69]",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "2023-02-12 16:58:07,772 - root - INFO - Epoch 50 - Train loss: 1.8560 - Val loss: 2.6933\n",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": "\nEpoch 52/1000: 100%|ââââââââââ| 196/196 [01:04<00:00,  3.04it/s, current_loss=1.89, current_mean_loss=2.13]\nValidation Epoch 52/1000: 100%|ââââââââââ| 49/49 [00:19<00:00,  2.56it/s, current_loss=1.74, current_mean_loss=1.97]",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "2023-02-12 16:59:31,370 - root - INFO - Epoch 51 - Train loss: 2.1336 - Val loss: 1.9689\n",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": "\nEpoch 53/1000: 100%|ââââââââââ| 196/196 [01:05<00:00,  3.01it/s, current_loss=1.58, current_mean_loss=1.88]\nValidation Epoch 53/1000: 100%|ââââââââââ| 49/49 [00:19<00:00,  2.57it/s, current_loss=1.8, current_mean_loss=1.94] ",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "2023-02-12 17:00:55,706 - root - INFO - Epoch 52 - Train loss: 1.8794 - Val loss: 1.9415\n",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": "\nEpoch 54/1000: 100%|ââââââââââ| 196/196 [01:04<00:00,  3.02it/s, current_loss=1.34, current_mean_loss=1.93]\nValidation Epoch 54/1000: 100%|ââââââââââ| 49/49 [00:19<00:00,  2.53it/s, current_loss=1.92, current_mean_loss=1.96]",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "2023-02-12 17:02:19,949 - root - INFO - Epoch 53 - Train loss: 1.9252 - Val loss: 1.9598\n",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": "\nEpoch 55/1000: 100%|ââââââââââ| 196/196 [01:05<00:00,  3.00it/s, current_loss=1.77, current_mean_loss=1.75]\nValidation Epoch 55/1000: 100%|ââââââââââ| 49/49 [00:19<00:00,  2.54it/s, current_loss=1.81, current_mean_loss=1.86]",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "2023-02-12 17:03:44,748 - root - INFO - Epoch 54 - Train loss: 1.7461 - Val loss: 1.8555\n",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": "\nEpoch 56/1000: 100%|ââââââââââ| 196/196 [01:04<00:00,  3.02it/s, current_loss=1.71, current_mean_loss=1.85]\nValidation Epoch 56/1000: 100%|ââââââââââ| 49/49 [00:18<00:00,  2.59it/s, current_loss=1.87, current_mean_loss=2.03]",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "2023-02-12 17:05:08,654 - root - INFO - Epoch 55 - Train loss: 1.8454 - Val loss: 2.0319\n",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": "\nEpoch 57/1000: 100%|ââââââââââ| 196/196 [01:04<00:00,  3.02it/s, current_loss=2.21, current_mean_loss=1.83]\nValidation Epoch 57/1000: 100%|ââââââââââ| 49/49 [00:19<00:00,  2.53it/s, current_loss=2.04, current_mean_loss=2.07]",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "2023-02-12 17:06:33,072 - root - INFO - Epoch 56 - Train loss: 1.8254 - Val loss: 2.0697\n",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": "\nEpoch 58/1000: 100%|ââââââââââ| 196/196 [01:05<00:00,  3.01it/s, current_loss=1.82, current_mean_loss=1.9] \nValidation Epoch 58/1000: 100%|ââââââââââ| 49/49 [00:19<00:00,  2.51it/s, current_loss=2.05, current_mean_loss=2.13]",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "2023-02-12 17:07:57,858 - root - INFO - Epoch 57 - Train loss: 1.8986 - Val loss: 2.1340\n",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": "\nEpoch 59/1000: 100%|ââââââââââ| 196/196 [01:05<00:00,  3.01it/s, current_loss=1.76, current_mean_loss=1.87]\nValidation Epoch 59/1000: 100%|ââââââââââ| 49/49 [00:19<00:00,  2.54it/s, current_loss=1.65, current_mean_loss=1.76]",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "2023-02-12 17:09:22,341 - root - INFO - Epoch 58 - Train loss: 1.8695 - Val loss: 1.7607\n",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": "\nEpoch 60/1000: 100%|ââââââââââ| 196/196 [01:05<00:00,  3.00it/s, current_loss=1.83, current_mean_loss=1.81]\nValidation Epoch 60/1000: 100%|ââââââââââ| 49/49 [00:19<00:00,  2.54it/s, current_loss=1.61, current_mean_loss=1.67]",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "2023-02-12 17:10:47,035 - root - INFO - Epoch 59 - Train loss: 1.8079 - Val loss: 1.6732\n",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": "\nEpoch 61/1000: 100%|ââââââââââ| 196/196 [01:05<00:00,  3.00it/s, current_loss=1.54, current_mean_loss=1.91]\nValidation Epoch 61/1000: 100%|ââââââââââ| 49/49 [00:19<00:00,  2.53it/s, current_loss=1.65, current_mean_loss=1.74]",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "2023-02-12 17:12:11,877 - root - INFO - Epoch 60 - Train loss: 1.9093 - Val loss: 1.7412\n",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": "\nEpoch 62/1000: 100%|ââââââââââ| 196/196 [01:04<00:00,  3.02it/s, current_loss=1.53, current_mean_loss=1.78]\nValidation Epoch 62/1000: 100%|ââââââââââ| 49/49 [00:18<00:00,  2.60it/s, current_loss=1.58, current_mean_loss=1.62]",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "2023-02-12 17:13:35,742 - root - INFO - Epoch 61 - Train loss: 1.7841 - Val loss: 1.6202\n2023-02-12 17:13:35,743 - root - INFO - Saving best model\n",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": "\nEpoch 63/1000: 100%|ââââââââââ| 196/196 [01:05<00:00,  3.00it/s, current_loss=2.73, current_mean_loss=1.92]\nValidation Epoch 63/1000: 100%|ââââââââââ| 49/49 [00:19<00:00,  2.51it/s, current_loss=2.36, current_mean_loss=2.36]",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "2023-02-12 17:15:00,627 - root - INFO - Epoch 62 - Train loss: 1.9195 - Val loss: 2.3573\n",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": "\nEpoch 64/1000: 100%|ââââââââââ| 196/196 [01:05<00:00,  2.98it/s, current_loss=1.68, current_mean_loss=1.94]\nValidation Epoch 64/1000: 100%|ââââââââââ| 49/49 [00:19<00:00,  2.49it/s, current_loss=1.91, current_mean_loss=1.98]",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "2023-02-12 17:16:26,221 - root - INFO - Epoch 63 - Train loss: 1.9406 - Val loss: 1.9822\n",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": "\nEpoch 65/1000: 100%|ââââââââââ| 196/196 [01:04<00:00,  3.03it/s, current_loss=2.39, current_mean_loss=2]   \nValidation Epoch 65/1000: 100%|ââââââââââ| 49/49 [00:18<00:00,  2.59it/s, current_loss=1.59, current_mean_loss=1.72]",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "2023-02-12 17:17:49,858 - root - INFO - Epoch 64 - Train loss: 2.0028 - Val loss: 1.7216\n",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": "\nEpoch 66/1000: 100%|ââââââââââ| 196/196 [01:05<00:00,  3.01it/s, current_loss=1.87, current_mean_loss=1.85]\nValidation Epoch 66/1000: 100%|ââââââââââ| 49/49 [00:19<00:00,  2.51it/s, current_loss=1.59, current_mean_loss=1.69]",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "2023-02-12 17:19:14,611 - root - INFO - Epoch 65 - Train loss: 1.8457 - Val loss: 1.6851\n",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": "\nEpoch 67/1000:  69%|âââââââ   | 136/196 [00:45<00:16,  3.68it/s, current_loss=1.94, current_mean_loss=1.91]",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "Epoch 13071: reducing learning rate of group 0 to 5.0000e-04.\n",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": "Epoch 67/1000: 100%|ââââââââââ| 196/196 [01:05<00:00,  3.00it/s, current_loss=1.66, current_mean_loss=1.81]\nValidation Epoch 67/1000: 100%|ââââââââââ| 49/49 [00:19<00:00,  2.51it/s, current_loss=1.56, current_mean_loss=1.64]",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "2023-02-12 17:20:39,484 - root - INFO - Epoch 66 - Train loss: 1.8107 - Val loss: 1.6436\n",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": "\nEpoch 68/1000: 100%|ââââââââââ| 196/196 [01:05<00:00,  2.98it/s, current_loss=1.42, current_mean_loss=1.55]\nValidation Epoch 68/1000: 100%|ââââââââââ| 49/49 [00:19<00:00,  2.55it/s, current_loss=1.66, current_mean_loss=1.69]",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "2023-02-12 17:22:04,568 - root - INFO - Epoch 67 - Train loss: 1.5488 - Val loss: 1.6941\n",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": "\nEpoch 69/1000: 100%|ââââââââââ| 196/196 [01:05<00:00,  3.01it/s, current_loss=1.43, current_mean_loss=1.54]\nValidation Epoch 69/1000: 100%|ââââââââââ| 49/49 [00:19<00:00,  2.50it/s, current_loss=1.67, current_mean_loss=1.68]",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "2023-02-12 17:23:29,411 - root - INFO - Epoch 68 - Train loss: 1.5399 - Val loss: 1.6806\n",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": "\nEpoch 70/1000: 100%|ââââââââââ| 196/196 [01:05<00:00,  3.01it/s, current_loss=1.34, current_mean_loss=1.55]\nValidation Epoch 70/1000: 100%|ââââââââââ| 49/49 [00:20<00:00,  2.44it/s, current_loss=1.53, current_mean_loss=1.61]",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "2023-02-12 17:24:54,792 - root - INFO - Epoch 69 - Train loss: 1.5508 - Val loss: 1.6094\n2023-02-12 17:24:54,793 - root - INFO - Saving best model\n",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": "\nEpoch 71/1000: 100%|ââââââââââ| 196/196 [01:06<00:00,  2.97it/s, current_loss=1.88, current_mean_loss=1.6] \nValidation Epoch 71/1000: 100%|ââââââââââ| 49/49 [00:19<00:00,  2.58it/s, current_loss=1.59, current_mean_loss=1.66]",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "2023-02-12 17:26:19,953 - root - INFO - Epoch 70 - Train loss: 1.5954 - Val loss: 1.6621\n",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": "\nEpoch 72/1000: 100%|ââââââââââ| 196/196 [01:06<00:00,  2.96it/s, current_loss=1.38, current_mean_loss=1.53]\nValidation Epoch 72/1000: 100%|ââââââââââ| 49/49 [00:20<00:00,  2.44it/s, current_loss=1.67, current_mean_loss=1.67]",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "2023-02-12 17:27:46,282 - root - INFO - Epoch 71 - Train loss: 1.5264 - Val loss: 1.6668\n",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": "\nEpoch 73/1000: 100%|ââââââââââ| 196/196 [01:04<00:00,  3.05it/s, current_loss=1.59, current_mean_loss=1.55]\nValidation Epoch 73/1000: 100%|ââââââââââ| 49/49 [00:19<00:00,  2.52it/s, current_loss=1.67, current_mean_loss=1.69]",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "2023-02-12 17:29:10,062 - root - INFO - Epoch 72 - Train loss: 1.5483 - Val loss: 1.6855\n",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": "\nEpoch 74/1000: 100%|ââââââââââ| 196/196 [01:05<00:00,  3.01it/s, current_loss=1.86, current_mean_loss=1.6] \nValidation Epoch 74/1000: 100%|ââââââââââ| 49/49 [00:18<00:00,  2.60it/s, current_loss=1.66, current_mean_loss=1.71]",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "2023-02-12 17:30:34,152 - root - INFO - Epoch 73 - Train loss: 1.6028 - Val loss: 1.7129\n",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": "\nEpoch 75/1000: 100%|ââââââââââ| 196/196 [01:04<00:00,  3.03it/s, current_loss=1.48, current_mean_loss=1.54]\nValidation Epoch 75/1000: 100%|ââââââââââ| 49/49 [00:18<00:00,  2.59it/s, current_loss=1.56, current_mean_loss=1.57]",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "2023-02-12 17:31:57,874 - root - INFO - Epoch 74 - Train loss: 1.5422 - Val loss: 1.5713\n2023-02-12 17:31:57,875 - root - INFO - Saving best model\n",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": "\nEpoch 76/1000: 100%|ââââââââââ| 196/196 [01:04<00:00,  3.06it/s, current_loss=1.72, current_mean_loss=1.58]\nValidation Epoch 76/1000: 100%|ââââââââââ| 49/49 [00:19<00:00,  2.56it/s, current_loss=1.56, current_mean_loss=1.61]",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "2023-02-12 17:33:21,125 - root - INFO - Epoch 75 - Train loss: 1.5762 - Val loss: 1.6091\n",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": "\nEpoch 77/1000: 100%|ââââââââââ| 196/196 [01:04<00:00,  3.03it/s, current_loss=1.4, current_mean_loss=1.55] \nValidation Epoch 77/1000: 100%|ââââââââââ| 49/49 [00:18<00:00,  2.59it/s, current_loss=1.65, current_mean_loss=1.77]",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "2023-02-12 17:34:44,687 - root - INFO - Epoch 76 - Train loss: 1.5491 - Val loss: 1.7686\n",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": "\nEpoch 78/1000: 100%|ââââââââââ| 196/196 [01:04<00:00,  3.03it/s, current_loss=1.31, current_mean_loss=1.58]\nValidation Epoch 78/1000: 100%|ââââââââââ| 49/49 [00:19<00:00,  2.55it/s, current_loss=1.57, current_mean_loss=1.67]",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "2023-02-12 17:36:08,702 - root - INFO - Epoch 77 - Train loss: 1.5844 - Val loss: 1.6732\n",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": "\nEpoch 79/1000: 100%|ââââââââââ| 196/196 [01:04<00:00,  3.04it/s, current_loss=1.46, current_mean_loss=1.6] \nValidation Epoch 79/1000: 100%|ââââââââââ| 49/49 [00:19<00:00,  2.55it/s, current_loss=1.51, current_mean_loss=1.6] ",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "2023-02-12 17:37:32,564 - root - INFO - Epoch 78 - Train loss: 1.5997 - Val loss: 1.6035\n",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": "\nEpoch 80/1000: 100%|ââââââââââ| 196/196 [01:03<00:00,  3.07it/s, current_loss=1.61, current_mean_loss=1.57]\nValidation Epoch 80/1000: 100%|ââââââââââ| 49/49 [00:18<00:00,  2.62it/s, current_loss=1.67, current_mean_loss=1.73]",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "2023-02-12 17:38:55,289 - root - INFO - Epoch 79 - Train loss: 1.5709 - Val loss: 1.7289\n",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": "\nEpoch 81/1000: 100%|ââââââââââ| 196/196 [01:05<00:00,  2.98it/s, current_loss=1.89, current_mean_loss=1.59]\nValidation Epoch 81/1000: 100%|ââââââââââ| 49/49 [00:19<00:00,  2.54it/s, current_loss=1.68, current_mean_loss=1.73]",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "2023-02-12 17:40:20,395 - root - INFO - Epoch 80 - Train loss: 1.5875 - Val loss: 1.7323\n",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": "\nEpoch 82/1000: 100%|ââââââââââ| 196/196 [01:04<00:00,  3.04it/s, current_loss=1.82, current_mean_loss=1.57] \nValidation Epoch 82/1000: 100%|ââââââââââ| 49/49 [00:19<00:00,  2.55it/s, current_loss=1.48, current_mean_loss=1.57]",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "2023-02-12 17:41:44,167 - root - INFO - Epoch 81 - Train loss: 1.5700 - Val loss: 1.5657\n2023-02-12 17:41:44,170 - root - INFO - Saving best model\n",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": "\nEpoch 83/1000: 100%|ââââââââââ| 196/196 [01:03<00:00,  3.06it/s, current_loss=1.52, current_mean_loss=1.6] \nValidation Epoch 83/1000: 100%|ââââââââââ| 49/49 [00:18<00:00,  2.63it/s, current_loss=1.66, current_mean_loss=1.65]",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "2023-02-12 17:43:06,914 - root - INFO - Epoch 82 - Train loss: 1.6000 - Val loss: 1.6542\n",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": "\nEpoch 84/1000: 100%|ââââââââââ| 196/196 [01:04<00:00,  3.06it/s, current_loss=1.29, current_mean_loss=1.57]\nValidation Epoch 84/1000: 100%|ââââââââââ| 49/49 [00:18<00:00,  2.58it/s, current_loss=1.56, current_mean_loss=1.6] ",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "2023-02-12 17:44:30,086 - root - INFO - Epoch 83 - Train loss: 1.5705 - Val loss: 1.6041\n",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": "\nEpoch 85/1000: 100%|ââââââââââ| 196/196 [01:04<00:00,  3.05it/s, current_loss=1.65, current_mean_loss=1.57]\nValidation Epoch 85/1000: 100%|ââââââââââ| 49/49 [00:18<00:00,  2.62it/s, current_loss=1.62, current_mean_loss=1.69]",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "2023-02-12 17:45:53,188 - root - INFO - Epoch 84 - Train loss: 1.5700 - Val loss: 1.6855\n",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": "\nEpoch 86/1000: 100%|ââââââââââ| 196/196 [01:04<00:00,  3.05it/s, current_loss=1.72, current_mean_loss=1.57]\nValidation Epoch 86/1000: 100%|ââââââââââ| 49/49 [00:19<00:00,  2.53it/s, current_loss=1.54, current_mean_loss=1.66]",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "2023-02-12 17:47:16,904 - root - INFO - Epoch 85 - Train loss: 1.5735 - Val loss: 1.6584\n",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": "\nEpoch 87/1000: 100%|ââââââââââ| 196/196 [01:05<00:00,  2.99it/s, current_loss=1.28, current_mean_loss=1.57]\nValidation Epoch 87/1000: 100%|ââââââââââ| 49/49 [00:19<00:00,  2.50it/s, current_loss=1.55, current_mean_loss=1.59]",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "2023-02-12 17:48:42,014 - root - INFO - Epoch 86 - Train loss: 1.5651 - Val loss: 1.5887\n",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": "\nEpoch 88/1000: 100%|ââââââââââ| 196/196 [01:05<00:00,  3.00it/s, current_loss=1.79, current_mean_loss=1.55]\nValidation Epoch 88/1000: 100%|ââââââââââ| 49/49 [00:18<00:00,  2.59it/s, current_loss=1.63, current_mean_loss=1.67]",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "2023-02-12 17:50:06,278 - root - INFO - Epoch 87 - Train loss: 1.5508 - Val loss: 1.6718\n",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": "\nEpoch 89/1000: 100%|ââââââââââ| 196/196 [01:04<00:00,  3.05it/s, current_loss=1.78, current_mean_loss=1.54]\nValidation Epoch 89/1000: 100%|ââââââââââ| 49/49 [00:19<00:00,  2.54it/s, current_loss=1.49, current_mean_loss=1.56]",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "2023-02-12 17:51:29,927 - root - INFO - Epoch 88 - Train loss: 1.5401 - Val loss: 1.5623\n2023-02-12 17:51:29,928 - root - INFO - Saving best model\n",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": "\nEpoch 90/1000: 100%|ââââââââââ| 196/196 [01:04<00:00,  3.03it/s, current_loss=1.16, current_mean_loss=1.6] \nValidation Epoch 90/1000: 100%|ââââââââââ| 49/49 [00:19<00:00,  2.52it/s, current_loss=1.7, current_mean_loss=1.73] ",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "2023-02-12 17:52:54,193 - root - INFO - Epoch 89 - Train loss: 1.5972 - Val loss: 1.7279\n",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": "\nEpoch 91/1000: 100%|ââââââââââ| 196/196 [01:05<00:00,  2.99it/s, current_loss=1.99, current_mean_loss=1.54]\nValidation Epoch 91/1000: 100%|ââââââââââ| 49/49 [00:19<00:00,  2.56it/s, current_loss=1.71, current_mean_loss=1.74]",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "2023-02-12 17:54:18,930 - root - INFO - Epoch 90 - Train loss: 1.5397 - Val loss: 1.7431\n",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": "\nEpoch 92/1000: 100%|ââââââââââ| 196/196 [01:04<00:00,  3.05it/s, current_loss=1.31, current_mean_loss=1.58]\nValidation Epoch 92/1000: 100%|ââââââââââ| 49/49 [00:19<00:00,  2.55it/s, current_loss=1.6, current_mean_loss=1.62] ",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "2023-02-12 17:55:42,503 - root - INFO - Epoch 91 - Train loss: 1.5758 - Val loss: 1.6197\n",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": "\nEpoch 93/1000: 100%|ââââââââââ| 196/196 [01:04<00:00,  3.05it/s, current_loss=1.49, current_mean_loss=1.54]\nValidation Epoch 93/1000: 100%|ââââââââââ| 49/49 [00:19<00:00,  2.53it/s, current_loss=1.56, current_mean_loss=1.56]",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "2023-02-12 17:57:06,072 - root - INFO - Epoch 92 - Train loss: 1.5391 - Val loss: 1.5586\n2023-02-12 17:57:06,073 - root - INFO - Saving best model\n",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": "\nEpoch 94/1000: 100%|ââââââââââ| 196/196 [01:04<00:00,  3.05it/s, current_loss=1.41, current_mean_loss=1.54]\nValidation Epoch 94/1000: 100%|ââââââââââ| 49/49 [00:18<00:00,  2.59it/s, current_loss=1.54, current_mean_loss=1.58]",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "2023-02-12 17:58:29,374 - root - INFO - Epoch 93 - Train loss: 1.5364 - Val loss: 1.5804\n",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": "\nEpoch 95/1000: 100%|ââââââââââ| 196/196 [01:03<00:00,  3.07it/s, current_loss=1.32, current_mean_loss=1.53]\nValidation Epoch 95/1000: 100%|ââââââââââ| 49/49 [00:19<00:00,  2.53it/s, current_loss=1.64, current_mean_loss=1.72]",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "2023-02-12 17:59:52,667 - root - INFO - Epoch 94 - Train loss: 1.5340 - Val loss: 1.7178\n",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": "\nEpoch 96/1000: 100%|ââââââââââ| 196/196 [01:04<00:00,  3.03it/s, current_loss=1.57, current_mean_loss=1.55]\nValidation Epoch 96/1000: 100%|ââââââââââ| 49/49 [00:18<00:00,  2.62it/s, current_loss=1.56, current_mean_loss=1.6] ",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "2023-02-12 18:01:16,148 - root - INFO - Epoch 95 - Train loss: 1.5541 - Val loss: 1.5999\n",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": "\nEpoch 97/1000: 100%|ââââââââââ| 196/196 [01:04<00:00,  3.04it/s, current_loss=1.41, current_mean_loss=1.59]\nValidation Epoch 97/1000: 100%|ââââââââââ| 49/49 [00:19<00:00,  2.51it/s, current_loss=1.62, current_mean_loss=1.71]",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "2023-02-12 18:02:40,262 - root - INFO - Epoch 96 - Train loss: 1.5916 - Val loss: 1.7149\n",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": "\nEpoch 98/1000:   5%|â         | 9/196 [00:03<00:58,  3.22it/s, current_loss=1.4, current_mean_loss=1.45] ",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "Epoch 19021: reducing learning rate of group 0 to 2.5000e-04.\n",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": "Epoch 98/1000: 100%|ââââââââââ| 196/196 [01:07<00:00,  2.89it/s, current_loss=1.38, current_mean_loss=1.45]\nValidation Epoch 98/1000: 100%|ââââââââââ| 49/49 [00:19<00:00,  2.49it/s, current_loss=1.53, current_mean_loss=1.54]",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "2023-02-12 18:04:07,754 - root - INFO - Epoch 97 - Train loss: 1.4464 - Val loss: 1.5394\n2023-02-12 18:04:07,755 - root - INFO - Saving best model\n",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": "\nEpoch 99/1000: 100%|ââââââââââ| 196/196 [01:04<00:00,  3.02it/s, current_loss=1.44, current_mean_loss=1.45]\nValidation Epoch 99/1000: 100%|ââââââââââ| 49/49 [00:18<00:00,  2.62it/s, current_loss=1.5, current_mean_loss=1.51] ",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "2023-02-12 18:05:31,536 - root - INFO - Epoch 98 - Train loss: 1.4503 - Val loss: 1.5137\n2023-02-12 18:05:31,537 - root - INFO - Saving best model\n",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": "\nEpoch 100/1000: 100%|ââââââââââ| 196/196 [01:04<00:00,  3.05it/s, current_loss=1.1, current_mean_loss=1.43] \nValidation Epoch 100/1000: 100%|ââââââââââ| 49/49 [00:19<00:00,  2.56it/s, current_loss=1.48, current_mean_loss=1.52]",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "2023-02-12 18:06:55,096 - root - INFO - Epoch 99 - Train loss: 1.4305 - Val loss: 1.5243\n",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": "\nEpoch 101/1000: 100%|ââââââââââ| 196/196 [01:05<00:00,  3.01it/s, current_loss=1.55, current_mean_loss=1.44] \nValidation Epoch 101/1000: 100%|ââââââââââ| 49/49 [00:19<00:00,  2.57it/s, current_loss=1.46, current_mean_loss=1.51]",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "2023-02-12 18:08:19,291 - root - INFO - Epoch 100 - Train loss: 1.4428 - Val loss: 1.5060\n2023-02-12 18:08:19,293 - root - INFO - Saving best model\n",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": "\nEpoch 102/1000: 100%|ââââââââââ| 196/196 [01:03<00:00,  3.07it/s, current_loss=1.37, current_mean_loss=1.44]\nValidation Epoch 102/1000: 100%|ââââââââââ| 49/49 [00:18<00:00,  2.60it/s, current_loss=1.49, current_mean_loss=1.51]",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "2023-02-12 18:09:42,045 - root - INFO - Epoch 101 - Train loss: 1.4391 - Val loss: 1.5079\n",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": "\nEpoch 103/1000: 100%|ââââââââââ| 196/196 [01:04<00:00,  3.05it/s, current_loss=2.1, current_mean_loss=1.45] \nValidation Epoch 103/1000: 100%|ââââââââââ| 49/49 [00:19<00:00,  2.55it/s, current_loss=1.54, current_mean_loss=1.57]",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "2023-02-12 18:11:05,624 - root - INFO - Epoch 102 - Train loss: 1.4524 - Val loss: 1.5667\n",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": "\nEpoch 104/1000: 100%|ââââââââââ| 196/196 [01:03<00:00,  3.07it/s, current_loss=1.23, current_mean_loss=1.45]\nValidation Epoch 104/1000: 100%|ââââââââââ| 49/49 [00:18<00:00,  2.58it/s, current_loss=1.66, current_mean_loss=1.61]",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "2023-02-12 18:12:28,513 - root - INFO - Epoch 103 - Train loss: 1.4473 - Val loss: 1.6109\n",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": "\nEpoch 105/1000: 100%|ââââââââââ| 196/196 [01:04<00:00,  3.06it/s, current_loss=1.19, current_mean_loss=1.44]\nValidation Epoch 105/1000: 100%|ââââââââââ| 49/49 [00:18<00:00,  2.60it/s, current_loss=1.49, current_mean_loss=1.53]",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "2023-02-12 18:13:51,443 - root - INFO - Epoch 104 - Train loss: 1.4380 - Val loss: 1.5252\n",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": "\nEpoch 106/1000: 100%|ââââââââââ| 196/196 [01:04<00:00,  3.06it/s, current_loss=1.32, current_mean_loss=1.44]\nValidation Epoch 106/1000: 100%|ââââââââââ| 49/49 [00:19<00:00,  2.56it/s, current_loss=1.49, current_mean_loss=1.51]",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "2023-02-12 18:15:14,746 - root - INFO - Epoch 105 - Train loss: 1.4409 - Val loss: 1.5067\n",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": "\nEpoch 107/1000: 100%|ââââââââââ| 196/196 [01:04<00:00,  3.06it/s, current_loss=1.32, current_mean_loss=1.46]\nValidation Epoch 107/1000: 100%|ââââââââââ| 49/49 [00:18<00:00,  2.61it/s, current_loss=1.51, current_mean_loss=1.52]",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "2023-02-12 18:16:37,731 - root - INFO - Epoch 106 - Train loss: 1.4616 - Val loss: 1.5240\n",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": "\nEpoch 108/1000: 100%|ââââââââââ| 196/196 [01:04<00:00,  3.03it/s, current_loss=1.16, current_mean_loss=1.43] \nValidation Epoch 108/1000: 100%|ââââââââââ| 49/49 [00:19<00:00,  2.53it/s, current_loss=1.49, current_mean_loss=1.57]",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "2023-02-12 18:18:01,811 - root - INFO - Epoch 107 - Train loss: 1.4348 - Val loss: 1.5692\n",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": "\nEpoch 109/1000: 100%|ââââââââââ| 196/196 [01:04<00:00,  3.05it/s, current_loss=1.24, current_mean_loss=1.45]\nValidation Epoch 109/1000: 100%|ââââââââââ| 49/49 [00:18<00:00,  2.59it/s, current_loss=1.55, current_mean_loss=1.56]",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "2023-02-12 18:19:25,061 - root - INFO - Epoch 108 - Train loss: 1.4456 - Val loss: 1.5604\n",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": "\nEpoch 110/1000: 100%|ââââââââââ| 196/196 [01:04<00:00,  3.03it/s, current_loss=1.32, current_mean_loss=1.44]\nValidation Epoch 110/1000: 100%|ââââââââââ| 49/49 [00:18<00:00,  2.59it/s, current_loss=1.52, current_mean_loss=1.57]",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "2023-02-12 18:20:48,664 - root - INFO - Epoch 109 - Train loss: 1.4442 - Val loss: 1.5661\n",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": "\nEpoch 111/1000: 100%|ââââââââââ| 196/196 [01:03<00:00,  3.07it/s, current_loss=1.29, current_mean_loss=1.46] \nValidation Epoch 111/1000: 100%|ââââââââââ| 49/49 [00:18<00:00,  2.60it/s, current_loss=1.57, current_mean_loss=1.63]",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "2023-02-12 18:22:11,464 - root - INFO - Epoch 110 - Train loss: 1.4559 - Val loss: 1.6262\n",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": "\nEpoch 112/1000: 100%|ââââââââââ| 196/196 [01:04<00:00,  3.05it/s, current_loss=1.4, current_mean_loss=1.46] \nValidation Epoch 112/1000: 100%|ââââââââââ| 49/49 [00:19<00:00,  2.56it/s, current_loss=1.55, current_mean_loss=1.56]",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "2023-02-12 18:23:34,837 - root - INFO - Epoch 111 - Train loss: 1.4585 - Val loss: 1.5578\n",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": "\nEpoch 113/1000: 100%|ââââââââââ| 196/196 [01:03<00:00,  3.10it/s, current_loss=1.09, current_mean_loss=1.47]\nValidation Epoch 113/1000: 100%|ââââââââââ| 49/49 [00:18<00:00,  2.59it/s, current_loss=1.48, current_mean_loss=1.52]",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "2023-02-12 18:24:57,075 - root - INFO - Epoch 112 - Train loss: 1.4697 - Val loss: 1.5200\n",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": "\nEpoch 114/1000: 100%|ââââââââââ| 196/196 [01:05<00:00,  3.01it/s, current_loss=1.41, current_mean_loss=1.44]\nValidation Epoch 114/1000: 100%|ââââââââââ| 49/49 [00:18<00:00,  2.58it/s, current_loss=1.48, current_mean_loss=1.53]",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "2023-02-12 18:26:21,140 - root - INFO - Epoch 113 - Train loss: 1.4427 - Val loss: 1.5320\n",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": "\nEpoch 115/1000: 100%|ââââââââââ| 196/196 [01:04<00:00,  3.03it/s, current_loss=1.32, current_mean_loss=1.43]\nValidation Epoch 115/1000: 100%|ââââââââââ| 49/49 [00:18<00:00,  2.58it/s, current_loss=1.47, current_mean_loss=1.53]",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "2023-02-12 18:27:44,817 - root - INFO - Epoch 114 - Train loss: 1.4262 - Val loss: 1.5271\n",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": "\nEpoch 116/1000: 100%|ââââââââââ| 196/196 [01:05<00:00,  2.99it/s, current_loss=1.57, current_mean_loss=1.44]\nValidation Epoch 116/1000: 100%|ââââââââââ| 49/49 [00:18<00:00,  2.59it/s, current_loss=1.48, current_mean_loss=1.55]",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "2023-02-12 18:29:09,404 - root - INFO - Epoch 115 - Train loss: 1.4371 - Val loss: 1.5513\n",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": "\nEpoch 117/1000: 100%|ââââââââââ| 196/196 [01:05<00:00,  3.00it/s, current_loss=1.62, current_mean_loss=1.43]\nValidation Epoch 117/1000: 100%|ââââââââââ| 49/49 [00:19<00:00,  2.53it/s, current_loss=1.58, current_mean_loss=1.59]",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "2023-02-12 18:30:34,204 - root - INFO - Epoch 116 - Train loss: 1.4267 - Val loss: 1.5941\n",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": "\nEpoch 118/1000: 100%|ââââââââââ| 196/196 [01:04<00:00,  3.02it/s, current_loss=1.31, current_mean_loss=1.43]\nValidation Epoch 118/1000: 100%|ââââââââââ| 49/49 [00:18<00:00,  2.63it/s, current_loss=1.52, current_mean_loss=1.52]",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "2023-02-12 18:31:57,844 - root - INFO - Epoch 117 - Train loss: 1.4307 - Val loss: 1.5194\n",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": "\nEpoch 119/1000: 100%|ââââââââââ| 196/196 [01:04<00:00,  3.06it/s, current_loss=1.2, current_mean_loss=1.44] \nValidation Epoch 119/1000: 100%|ââââââââââ| 49/49 [00:19<00:00,  2.52it/s, current_loss=1.49, current_mean_loss=1.52]",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "2023-02-12 18:33:21,437 - root - INFO - Epoch 118 - Train loss: 1.4384 - Val loss: 1.5154\n",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": "\nEpoch 120/1000: 100%|ââââââââââ| 196/196 [01:03<00:00,  3.07it/s, current_loss=1.62, current_mean_loss=1.44] \nValidation Epoch 120/1000: 100%|ââââââââââ| 49/49 [00:19<00:00,  2.54it/s, current_loss=1.51, current_mean_loss=1.56]",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "2023-02-12 18:34:44,545 - root - INFO - Epoch 119 - Train loss: 1.4355 - Val loss: 1.5563\n",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": "\nEpoch 121/1000: 100%|ââââââââââ| 196/196 [01:03<00:00,  3.07it/s, current_loss=1.36, current_mean_loss=1.44]\nValidation Epoch 121/1000: 100%|ââââââââââ| 49/49 [00:18<00:00,  2.61it/s, current_loss=1.55, current_mean_loss=1.56]",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "2023-02-12 18:36:07,231 - root - INFO - Epoch 120 - Train loss: 1.4402 - Val loss: 1.5622\n",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": "\nEpoch 122/1000: 100%|ââââââââââ| 196/196 [01:04<00:00,  3.05it/s, current_loss=1.32, current_mean_loss=1.43]\nValidation Epoch 122/1000: 100%|ââââââââââ| 49/49 [00:19<00:00,  2.52it/s, current_loss=1.53, current_mean_loss=1.52]",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "2023-02-12 18:37:31,042 - root - INFO - Epoch 121 - Train loss: 1.4255 - Val loss: 1.5228\n",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": "\nEpoch 123/1000: 100%|ââââââââââ| 196/196 [01:04<00:00,  3.04it/s, current_loss=1.52, current_mean_loss=1.43]\nValidation Epoch 123/1000: 100%|ââââââââââ| 49/49 [00:19<00:00,  2.54it/s, current_loss=1.52, current_mean_loss=1.55]",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "2023-02-12 18:38:54,809 - root - INFO - Epoch 122 - Train loss: 1.4321 - Val loss: 1.5478\n",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": "\nEpoch 124/1000: 100%|ââââââââââ| 196/196 [01:04<00:00,  3.04it/s, current_loss=1.43, current_mean_loss=1.42]\nValidation Epoch 124/1000: 100%|ââââââââââ| 49/49 [00:19<00:00,  2.57it/s, current_loss=1.52, current_mean_loss=1.55]",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "2023-02-12 18:40:18,513 - root - INFO - Epoch 123 - Train loss: 1.4222 - Val loss: 1.5549\n",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": "\nEpoch 125/1000: 100%|ââââââââââ| 196/196 [01:04<00:00,  3.02it/s, current_loss=1.13, current_mean_loss=1.43]\nValidation Epoch 125/1000: 100%|ââââââââââ| 49/49 [00:19<00:00,  2.47it/s, current_loss=1.6, current_mean_loss=1.61] ",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "2023-02-12 18:41:43,282 - root - INFO - Epoch 124 - Train loss: 1.4254 - Val loss: 1.6125\n",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": "\nEpoch 126/1000: 100%|ââââââââââ| 196/196 [01:05<00:00,  2.98it/s, current_loss=1.92, current_mean_loss=1.43]\nValidation Epoch 126/1000: 100%|ââââââââââ| 49/49 [00:19<00:00,  2.50it/s, current_loss=1.48, current_mean_loss=1.55]",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "2023-02-12 18:43:08,606 - root - INFO - Epoch 125 - Train loss: 1.4332 - Val loss: 1.5517\n",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": "\nEpoch 127/1000: 100%|ââââââââââ| 196/196 [01:05<00:00,  2.99it/s, current_loss=1.2, current_mean_loss=1.43] \nValidation Epoch 127/1000: 100%|ââââââââââ| 49/49 [00:18<00:00,  2.59it/s, current_loss=1.53, current_mean_loss=1.58]",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "2023-02-12 18:44:32,995 - root - INFO - Epoch 126 - Train loss: 1.4284 - Val loss: 1.5841\n",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": "\nEpoch 128/1000: 100%|ââââââââââ| 196/196 [01:04<00:00,  3.05it/s, current_loss=1.22, current_mean_loss=1.42]\nValidation Epoch 128/1000: 100%|ââââââââââ| 49/49 [00:19<00:00,  2.51it/s, current_loss=1.53, current_mean_loss=1.57]",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "2023-02-12 18:45:56,894 - root - INFO - Epoch 127 - Train loss: 1.4155 - Val loss: 1.5673\n",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": "\nEpoch 129/1000: 100%|ââââââââââ| 196/196 [01:03<00:00,  3.08it/s, current_loss=1.27, current_mean_loss=1.45]\nValidation Epoch 129/1000: 100%|ââââââââââ| 49/49 [00:18<00:00,  2.62it/s, current_loss=1.56, current_mean_loss=1.62]",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "2023-02-12 18:47:19,420 - root - INFO - Epoch 128 - Train loss: 1.4478 - Val loss: 1.6241\n",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": "\nEpoch 130/1000: 100%|ââââââââââ| 196/196 [01:05<00:00,  2.98it/s, current_loss=1.48, current_mean_loss=1.43]\nValidation Epoch 130/1000:  41%|ââââ      | 20/49 [00:08<00:10,  2.66it/s, current_loss=1.35, current_mean_loss=1.65]",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  }
 ]
}