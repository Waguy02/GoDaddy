{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Setup\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name '__file__' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[1;32m~\\AppData\\Local\\Temp\\ipykernel_20632\\2038509542.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m      5\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mtorch\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      6\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m----> 7\u001B[1;33m \u001B[0mROOT_DIR\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mos\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mpath\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mdirname\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mos\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mpath\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mrealpath\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0m__file__\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m      8\u001B[0m \u001B[0mDATA_DIR\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mos\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mpath\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mjoin\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mROOT_DIR\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;34m\"data\"\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;34m\"godaddy-microbusiness-density-forecasting\"\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;31m##Directory of dataset\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      9\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mNameError\u001B[0m: name '__file__' is not defined"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "ROOT_DIR=os.path.dirname(os.path.realpath(__file__))\n",
    "DATA_DIR=os.path.join(ROOT_DIR,\"data\",\"godaddy-microbusiness-density-forecasting\") ##Directory of dataset\n",
    "\n",
    "EXPERIMENTS_DIR=os.path.join(ROOT_DIR, \"logs/experiments\")\n",
    "use_cuda = torch .cuda.is_available()\n",
    "DEVICE = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "\n",
    "\n",
    "N_CENSUS_FEATURES= 5 #pct_bb,pct_college,pct_foreign_born,pct_it_workers,median_hh_inc\n",
    "#cfips is not considered as a feature we use a one-hot encoding for it\n",
    "\n",
    "\n",
    "USE_CENSUS= False #Without census features\n",
    "\n",
    "AE_LATENT_DIM= 32\n",
    "\n",
    "LSTM_HIDDEN_DIM = 8\n",
    "\n",
    "SEQ_LEN=6\n",
    "SEQ_STRIDE= 1\n",
    "\n",
    "N_COUNTY=3142\n",
    "N_DIMS_COUNTY_ENCODING=  math.ceil(math.log(N_COUNTY,2))\n",
    "\n",
    "FEATURES_AE_CENSUS_DIR=os.path.join(EXPERIMENTS_DIR, \"features_ae_2_dims\")\n",
    "FEATURES_AE_LATENT_DIM= 2\n",
    "\n",
    "TRAIN_FILE= os.path.join(DATA_DIR, \"train.csv\")\n",
    "TEST_FILE= os.path.join(DATA_DIR, \"test.csv\")\n",
    "\n",
    "CENSUS_FILE =os.path.join(DATA_DIR, \"census_interpolated.csv\")\n",
    "\n",
    "NB_FUTURES= 10 #Number of days to predict\n",
    "\n",
    "\n",
    "#Scaling factors for microbusiness density\n",
    "MEAN_MB= 3.817671\n",
    "STD_MB= 4.991087\n",
    "\n",
    "MAX_MB= 300\n",
    "MIN_MB= 0.0"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Utils"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from enum import Enum\n",
    "from itertools import islice\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import logging\n",
    "import os\n",
    "import sys\n",
    "from time import strftime\n",
    "def setup_logger(args):\n",
    "    formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "    a_logger = logging.getLogger()\n",
    "    a_logger.setLevel(args.log_level)\n",
    "    log_dir=os.path.join(ROOT_DIR,\"logs\",\"output_logs\")\n",
    "    if not os.path.exists(log_dir):\n",
    "        os.makedirs(log_dir)\n",
    "    output_file_handler = logging.FileHandler(os.path.join(log_dir,strftime(\"log_%d_%m_%Y_%H_%M.log\")))\n",
    "    stdout_handler = logging.StreamHandler(sys.stdout)\n",
    "    stdout_handler.setFormatter(formatter)\n",
    "    a_logger.propagate=False\n",
    "    a_logger.addHandler(output_file_handler)\n",
    "    a_logger.addHandler(stdout_handler)\n",
    "\n",
    "def read_json(path_json):\n",
    "    with open(path_json, encoding='utf8') as json_file:\n",
    "        return json.load(json_file)\n",
    "def softmax(x):\n",
    "    return np.exp(x) / np.sum(np.exp(x))\n",
    "def chunks(data, SIZE):\n",
    "    \"\"\"Split a dictionnary into parts of max_size =SIZE\"\"\"\n",
    "    it = iter(data)\n",
    "    for _ in range(0, len(data), SIZE):\n",
    "        yield {k: data[k] for k in islice(it, SIZE)}\n",
    "\n",
    "def sorted_dict(x, ascending=True):\n",
    "    \"\"\"\n",
    "    Sort dict according to value.\n",
    "    x must be a primitive type: int,float, str...\n",
    "    @param x:\n",
    "    @return:\n",
    "    \"\"\"\n",
    "    return dict(sorted(x.items(), key=lambda item: (1 if ascending else -1) * item[1]))\n",
    "def reverse_dict(input_dict):\n",
    "    \"\"\"\n",
    "    Reverse a dictonary\n",
    "    Args:\n",
    "        input_dict:\n",
    "\n",
    "    Returns:\n",
    "\n",
    "    \"\"\"\n",
    "    inv_dict = {}\n",
    "    for k, v in input_dict.items():\n",
    "        inv_dict[v] = inv_dict.get(v, []) + [k]\n",
    "\n",
    "    return inv_dict\n",
    "\n",
    "def save_matrix(matrix,filename):\n",
    "    with open(filename,'wb') as output:\n",
    "        np.save(output,matrix)\n",
    "def load_matrix(filename,auto_delete=False):\n",
    "    with open(filename,'rb') as input:\n",
    "        matrix=np.load(input)\n",
    "\n",
    "    if auto_delete:\n",
    "        os.remove(filename)\n",
    "    return matrix\n",
    "\n",
    "\n",
    "\n",
    "class Averager:\n",
    "    def __init__(self):\n",
    "        self.current_total = 0.0\n",
    "        self.iterations = 0.0\n",
    "\n",
    "    def send(self, value):\n",
    "        self.current_total += value\n",
    "        self.iterations += 1\n",
    "\n",
    "    @property\n",
    "    def value(self):\n",
    "        if self.iterations == 0:\n",
    "            return 0\n",
    "        else:\n",
    "            return 1.0 * self.current_total / self.iterations\n",
    "\n",
    "    def reset(self):\n",
    "        self.current_total = 0.0\n",
    "        self.iterations = 0.0\n",
    "\n",
    "\n",
    "\n",
    "from enum import Enum\n",
    "class DatasetType(Enum):\n",
    "    TRAIN=\"train\"\n",
    "    VALID=\"valid\"\n",
    "    TEST=\"test\"\n",
    "\n",
    "\n",
    "\n",
    "def extract_census_features(row,cfips_index,single_row=True):\n",
    "    \"\"\"\n",
    "\n",
    "    @param row: Row of the dataframe\n",
    "    @param cfips_index: index of the cfips for one-hot encoding\n",
    "    @return:\n",
    "    \"\"\"\n",
    "    ##If series :\n",
    "\n",
    "\n",
    "    if single_row:\n",
    "        features_tensor = torch.tensor( [row['pct_bb'],\n",
    "                                        row['pct_college'],\n",
    "                                        row['pct_foreign_born'],\n",
    "                                        row['pct_it_workers'],\n",
    "                                        row['median_hh_inc']\n",
    "                                        ], dtype=torch.float32)\n",
    "        cfips_one_hot = get_cfips_encoding(row['cfips'], cfips_index)\n",
    "        # Min-max normalization\n",
    "        features_tensor[ 0] = (features_tensor[ 0] - 24.5) / (97.6 - 24.5)\n",
    "        features_tensor[ 1] = (features_tensor[ 1] / 48)\n",
    "        features_tensor[ 2] = (features_tensor[ 2] / 54)\n",
    "        features_tensor[ 3] = (features_tensor[ 3] / 17.4)\n",
    "        features_tensor[ 4] = (features_tensor[ 4] - 17109) / (1586821 - 17109)\n",
    "\n",
    "    else :\n",
    "        features_tensor= torch.from_numpy(row[['pct_bb', 'pct_college', 'pct_foreign_born', 'pct_it_workers', 'median_hh_inc']].values)\n",
    "        row_one_hots= [get_cfips_encoding(cfips,cfips_index) for cfips in row['cfips']]\n",
    "        cfips_one_hot = torch.stack(row_one_hots)\n",
    "        #Min-max normalization\n",
    "        features_tensor[:,0] = (features_tensor[:,0]- 24.5)/ (97.6-24.5)\n",
    "        features_tensor[:,1] = (features_tensor[:,1] /48)\n",
    "        features_tensor[:,2] = (features_tensor[:,2]/ 54)\n",
    "        features_tensor[:,3] = (features_tensor[:,3] / 17.4)\n",
    "        features_tensor[:,4] = (features_tensor[:,4]- 17109)/(1586821-17109)\n",
    "\n",
    "\n",
    "    ##Add one-hot encoding of cfips\n",
    "    if single_row:\n",
    "        features_tensor = torch.cat((cfips_one_hot, features_tensor))\n",
    "    else:\n",
    "        features_tensor = torch.cat((cfips_one_hot,features_tensor), 1)\n",
    "\n",
    "    return features_tensor.float()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def get_cfips_index():\n",
    "    \"\"\"\n",
    "    Return a dictionary with key=cfips and value=index for using a one-hot encoding\n",
    "    \"\"\"\n",
    "    df= pd.read_csv(os.path.join(DATA_DIR, \"census_ae.csv\"))\n",
    "    cfips = df['cfips'].unique()\n",
    "    cfips.sort()\n",
    "    #Sort cfips\n",
    "    return {cfips[i]: i for i in range(len(cfips))}\n",
    "\n",
    "\n",
    "def get_cfips_encoding(cfips,cfips_index):\n",
    "    \"\"\"\n",
    "     return the base 2 encoding of cfips\n",
    "    \"\"\"\n",
    "\n",
    "    #n_dims is the number of bits needed to represent the cfips\n",
    "\n",
    "    bin_index=np.binary_repr(cfips_index[cfips],width=N_DIMS_COUNTY_ENCODING)\n",
    "    enc = torch.tensor([int(x) for x in bin_index],dtype=torch.float32)\n",
    "    return enc\n",
    "\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Network"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import json\n",
    "import logging\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=5000):\n",
    "        #Sin/Cos positional encoding\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.max_len = max_len\n",
    "        self.pe = torch.zeros(self.max_len, self.d_model)\n",
    "        position = torch.arange(0, self.max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, self.d_model, 2).float() * (-np.log(10000.0) / self.d_model))\n",
    "        self.pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        self.pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        self.pe = self.pe.unsqueeze(0).transpose(0, 1).squeeze(1)\n",
    "        #COnvert to nn.Parameter\n",
    "        self.pe = nn.Parameter(self.pe, requires_grad=False)\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        #Add positional encoding to the input (Pay attention to the dimensions (the pe does not have the batch dimension))\n",
    "        x = x + self.pe[:x.size(0), :]\n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "class TransformerPredictor(nn.Module):\n",
    "\n",
    "    def __init__(self,\n",
    "                 emb_dim=32,\n",
    "                 n_layers=3,\n",
    "                 n_head=8,\n",
    "                 max_seq_len=100,\n",
    "                 dim_feedforward=128,\n",
    "                 use_derivative=True,\n",
    "                 use_census=USE_CENSUS,\n",
    "                 n_dims_census_emb=2,\n",
    "                 experiment_dir=\"my_model\", reset=False, load_best=True):\n",
    "        \"\"\"\n",
    "        @param features_encoder :\n",
    "        @param input_dim:\n",
    "        @param hidden_dim:\n",
    "        @param ues_encoder:Â²\n",
    "        @param experiment_dir:\n",
    "        @param reset:\n",
    "        @param load_best:\n",
    "        \"\"\"\n",
    "\n",
    "        super(TransformerPredictor, self).__init__()\n",
    "        self.variante_num=4\n",
    "        self.emb_dim = emb_dim\n",
    "        self.n_layers = n_layers\n",
    "        self.n_head = n_head\n",
    "        self.dim_feedforward = dim_feedforward\n",
    "        self.use_census = use_census\n",
    "        self.max_seq_len = max_seq_len\n",
    "        self.census_features_encoder = None\n",
    "        self.n_dims_census_emb = n_dims_census_emb\n",
    "        self.input_dim =1\n",
    "        self.use_derivative = use_derivative\n",
    "        if self.use_derivative:\n",
    "            self.input_dim += 2 # 2 for derivative\n",
    "\n",
    "        if self.use_census:\n",
    "            self.input_dim = self.input_dim  + self.n_dims_census_emb\n",
    "\n",
    "\n",
    "\n",
    "        self.experiment_dir = experiment_dir\n",
    "        self.model_name = os.path.basename(self.experiment_dir)\n",
    "        self.reset = reset\n",
    "        self.load_best = load_best\n",
    "        self.setup_dirs()\n",
    "        self.setup_network()\n",
    "\n",
    "\n",
    "        if not reset: self.load_state()\n",
    "\n",
    "    ##1. Defining network architecture\n",
    "    def setup_network(self):\n",
    "        \"\"\"\n",
    "        Initialize the network  architecture here\n",
    "        @return:\n",
    "        \"\"\"\n",
    "        #Input encoder from self.input_dim to self.emb_dim along with positional encoding\n",
    "        if self.use_census:\n",
    "            self.query_encoder = nn.Sequential(nn.Linear(N_DIMS_COUNTY_ENCODING + N_CENSUS_FEATURES, self.emb_dim))\n",
    "            self.census_features_encoder= nn.Sequential(\n",
    "                nn.Linear(N_CENSUS_FEATURES,self.n_dims_census_emb),\n",
    "            )\n",
    "\n",
    "        self.input_embedding = nn.Sequential(\n",
    "            nn.Linear(self.input_dim, self.emb_dim),\n",
    "        )\n",
    "\n",
    "        ##Positional encoding\n",
    "        self.positional_encoding = PositionalEncoding(self.emb_dim, max_len=self.max_seq_len)\n",
    "        self.dropout = nn.Dropout(p=0.1)\n",
    "        self.transformer_encoder  = nn.TransformerEncoder(\n",
    "            nn.TransformerEncoderLayer(d_model=self.emb_dim, nhead=self.n_head, dim_feedforward=self.dim_feedforward,\n",
    "                                       dropout=0.01,\n",
    "                                       batch_first=True),\n",
    "            num_layers=self.n_layers\n",
    "        )\n",
    "        self.transformer_decoder = nn.TransformerDecoder(\n",
    "            nn.TransformerDecoderLayer(d_model=self.emb_dim, nhead=self.n_head, dim_feedforward=self.dim_feedforward,\n",
    "                                       dropout=0.01,\n",
    "                                       batch_first=True),\n",
    "            num_layers=self.n_layers\n",
    "        )\n",
    "\n",
    "        if self.use_census:\n",
    "            self.regressor = nn.Sequential(\n",
    "                nn.Linear(2*self.emb_dim, 1)\n",
    "            )\n",
    "\n",
    "        else:\n",
    "            self.regressor = nn.Sequential(\n",
    "                nn.Linear(self.emb_dim, 1)\n",
    "            )\n",
    "\n",
    "\n",
    "    ##2. Model Saving/Loading\n",
    "    def load_state(self, best=False):\n",
    "        \"\"\"\n",
    "        Load model\n",
    "        :param self:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        if best and os.path.exists(self.save_best_file):\n",
    "            logging.info(f\"Loading best model state : {self.save_file}\")\n",
    "            self.load_state_dict(torch.load(self.save_file, map_location=DEVICE))\n",
    "            return\n",
    "\n",
    "        if os.path.exists(self.save_file):\n",
    "            logging.info(f\"Loading model state : {self.save_file}\")\n",
    "            self.load_state_dict(torch.load(self.save_file, map_location=DEVICE))\n",
    "\n",
    "    def save_state(self, best=False):\n",
    "        if best:\n",
    "            logging.info(\"Saving best model\")\n",
    "            torch.save(self.state_dict(), self.save_best_file)\n",
    "        torch.save(self.state_dict(), self.save_file)\n",
    "\n",
    "    ##3. Setupping directories for weights /logs ... etc\n",
    "    def setup_dirs(self):\n",
    "        \"\"\"\n",
    "        Checking and creating directories for weights storage\n",
    "        @return:\n",
    "        \"\"\"\n",
    "        self.save_file = os.path.join(self.experiment_dir, f\"{self.model_name}.pt\")\n",
    "        self.save_best_file = os.path.join(self.experiment_dir, f\"{self.model_name}_best.pt\")\n",
    "        if not os.path.exists(self.experiment_dir):\n",
    "            os.makedirs(self.experiment_dir)\n",
    "\n",
    "    #4. Forward call\n",
    "    def forward(self, X_input):\n",
    "        \"\"\"\n",
    "        +Forward call here.\n",
    "        It a time series, so we need the full sequence output (strided by 1)\n",
    "        @param X:\n",
    "        @return:\n",
    "        \"\"\"\n",
    "        #0. Preparing the input (Removing the target from the input)\n",
    "        X = X_input[:, :-1, :] # Removing the target from the input (Only required when using census features)\n",
    "\n",
    "\n",
    "        if self.use_derivative:\n",
    "            d_left= torch.zeros((X.shape[0], X.shape[1], 1), device=DEVICE)\n",
    "            d_left[:,1:, -1] = X[:, 1:, -1] - X[:, :-1, -1]\n",
    "\n",
    "            d_right= torch.zeros((X.shape[0], X.shape[1], 1), device=DEVICE)\n",
    "            d_right[:,:-1, -1] = X[:, 1:, -1] - X[:, :-1, -1]\n",
    "\n",
    "            X = torch.cat((d_left, X, d_right), dim=-1) ## Adding the derivative to the input as a new feature\n",
    "\n",
    "\n",
    "        if self.use_census:\n",
    "            target = X[:, -1, :]  # Last element of the sequence is the target .\n",
    "            query = self.query_encoder(target[:, :N_DIMS_COUNTY_ENCODING+N_CENSUS_FEATURES])\n",
    "\n",
    "            enc_census = self.census_features_encoder(X[:, :, N_DIMS_COUNTY_ENCODING:N_CENSUS_FEATURES + N_DIMS_COUNTY_ENCODING])\n",
    "            X = torch.cat((X[:, :, N_CENSUS_FEATURES + N_DIMS_COUNTY_ENCODING:], enc_census), dim=-1)\n",
    "\n",
    "\n",
    "        #2. Apply the input encoder to the input\n",
    "        X = self.input_embedding(X)\n",
    "\n",
    "        #3. Add the positional encoding\n",
    "        X = self.positional_encoding(X)\n",
    "\n",
    "\n",
    "        #4. Add a query token to the input. Encoding of the cfips. (It is the same for all the sequence)\n",
    "        if self.use_census:\n",
    "            X = torch.cat((query.unsqueeze(1), X), dim=1)\n",
    "\n",
    "\n",
    "        #4. Apply the transformer encoder to get the memory\n",
    "        X = self.transformer_encoder(X)\n",
    "\n",
    "        if self.use_census:\n",
    "            query_enc= X[:, 0, :]\n",
    "            X = X[:, 1:, :]#Removing the query token\n",
    "\n",
    "\n",
    "\n",
    "        #.5 Apply the transformer decoder to get the next item in the sequence\n",
    "        tgt_sequence = torch.zeros(X.shape[0], 1, X.shape[-1]).to(DEVICE)\n",
    "        tgt_mask = torch.ones(1,1).to(DEVICE)\n",
    "\n",
    "        #6. Then apply the transformer to get the next item in the sequence\n",
    "        output = self.transformer_decoder(tgt_sequence, memory=X, tgt_mask= tgt_mask)#We want to predict the next item in the sequence\n",
    "\n",
    "        #7.We only want the last output of the sequence\n",
    "        output= output[:, -1, :]\n",
    "        if self.use_census:\n",
    "            output= torch.cat((output, query_enc), dim=-1)\n",
    "\n",
    "        #3. Finally apply the regressor to get the predictions.\n",
    "        output = self.regressor(output)\n",
    "\n",
    "        return output\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Dataset"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "class LstmDataset(Dataset):\n",
    "    def __init__(self, type, seq_len, stride=1):\n",
    "        self.type = type\n",
    "        self.seq_len = seq_len\n",
    "        self.stride = stride\n",
    "\n",
    "        self.file = os.path.join(DATA_DIR, f\"train_with_census_{'train' if type==DatasetType.TRAIN else 'val' if type==DatasetType.VALID  else 'test'}.csv\")\n",
    "        self.load_data()\n",
    "\n",
    "    def init_transforms(self):\n",
    "        \"\"\"\n",
    "        Initialize transforms.Might be different for each dataset type\n",
    "        \"\"\"\n",
    "\n",
    "    def load_data(self):\n",
    "        \"\"\"\n",
    "        Load data from the data items if necessary\n",
    "        \"\"\"\n",
    "        self.data = pd.read_csv(self.file)\n",
    "        self.data['first_day_of_month'] = pd.to_datetime(self.data['first_day_of_month'])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data) // self.stride\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        \"\"\"\n",
    "        Retrieving seq_len data\n",
    "        1. The county (CFIPS) should be the same\n",
    "        2. And the difference between the date(first_day_of_month) should be at most 3 months\n",
    "        \"\"\"\n",
    "        i = item * self.stride\n",
    "        county = self.data.iloc[i]['cfips']\n",
    "\n",
    "        rows_data=self.data.iloc[i:i+self.seq_len]\n",
    "\n",
    "        #Check if the county is the same\n",
    "        is_valid = len(rows_data)==self.seq_len and (rows_data['cfips'].unique()[0]==county) and (rows_data['first_day_of_month'].diff().max()<pd.Timedelta(days=90))\n",
    "\n",
    "        if not is_valid:\n",
    "            ##Find a random item that is valid\n",
    "            return self.__getitem__(torch.randint(0, len(self), (1,)).item())\n",
    "\n",
    "        #Taking seq_len rows and considering the following features\n",
    "        #pct_bb,pct_college,pct_foreign_born,pct_it_workers,median_hh_inc, active,microbusiness_density\n",
    "        features_tensor = torch.tensor(\n",
    "            rows_data[['pct_bb', 'pct_college', 'pct_foreign_born', 'pct_it_workers', 'median_hh_inc','year', 'active',\n",
    "                        'microbusiness_density']].values, dtype=torch.float32)\n",
    "\n",
    "        #return the iterator\n",
    "        return features_tensor\n",
    "\n",
    "import json\n",
    "import os\n",
    "from unicodedata import category\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from PIL import Image\n",
    "from matplotlib import pyplot as plt\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision.transforms import transforms\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "\n",
    "from enum import Enum\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class CensusDataset(Dataset):\n",
    "    def __init__(self, type):\n",
    "        self.type=type\n",
    "        self.load_data()\n",
    "        pass\n",
    "\n",
    "    def load_data(self):\n",
    "        \"\"\"\n",
    "        Load data from the data items if necessary\n",
    "        Returns:\n",
    "\n",
    "        \"\"\"\n",
    "        self.data_file=os.path.join(DATA_DIR,f\"train_with_census_ae_{'train' if self.type == DatasetType.TRAIN else 'test'}.csv\")\n",
    "        self.data = pd.read_csv(self.data_file)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        pct_bb,pct_college,pct_foreign_born,pct_it_workers,median_hh_inc,year .\n",
    "        Retrieve the following features from the dataset and return the corresponding tensor\n",
    "\n",
    "        Returns:\n",
    "        \"\"\"\n",
    "        row=self.data.iloc[idx]\n",
    "        features_tensor=torch.tensor([row['pct_bb'],row['pct_college'],row['pct_foreign_born'],\\\n",
    "                                      row['pct_it_workers'],row['median_hh_inc'],row['year']],dtype=torch.float32)\n",
    "        return features_tensor\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Loss and metrics"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "\n",
    "class SmapeCriterion(nn.Module):\n",
    "    \"\"\"\n",
    "    Class to compute the SMAPE loss.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super(SmapeCriterion, self).__init__()\n",
    "\n",
    "    def forward(self, y_pred, y_true):\n",
    "        \"\"\"\n",
    "        @param y_pred: Predicted values\n",
    "        @param y_true: True values\n",
    "        @return: SMAPE loss\n",
    "        \"\"\"\n",
    "        eps = 1e-8\n",
    "        return 100*torch.mean(2 * torch.abs(y_pred - y_true) / (torch.abs(y_pred) + torch.abs(y_true) + eps))\n",
    "\n",
    "    def __str__(self):\n",
    "        return \"SMAPE\"\n",
    "\n",
    "    def __repr__(self):\n",
    "        return str(self)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Dataset"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "import random\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from tqdm import tqdm\n",
    "\n",
    "EVAL_START_DATE = \"2022-05-01\"\n",
    "TEST_START_DATE =  \"2022-11-01\"\n",
    "\n",
    "SEED=42\n",
    "class MicroDensityDataset(Dataset):\n",
    "    def __init__(self, type, seq_len, stride=1,use_census=USE_CENSUS):\n",
    "        self.type = type\n",
    "        self.seq_len = seq_len\n",
    "        self.stride = stride if type == DatasetType.TRAIN else 1\n",
    "        self.use_census = use_census\n",
    "        self.load_data()\n",
    "        self.prepare_sequences()\n",
    "\n",
    "\n",
    "    def init_transforms(self):\n",
    "        \"\"\"\n",
    "        Initialize transforms.Might be different for each dataset type\n",
    "        \"\"\"\n",
    "\n",
    "    def load_data(self):\n",
    "        \"\"\"\n",
    "        Load data from the data items if necessary\n",
    "        \"\"\"\n",
    "\n",
    "        self.main_file = os.path.join(DATA_DIR, \"train.csv\")\n",
    "        self.main_df = pd.read_csv(self.main_file)\n",
    "\n",
    "        if self.type == DatasetType.TEST:\n",
    "            self.test_df = pd.read_csv(TEST_FILE)\n",
    "            self.test_df[\"microbusiness_density\"] = [0 for _ in range(len(self.test_df))]\n",
    "            self.test_df[\"county\"] =[\"NAN\" for _ in range(len(self.test_df))]\n",
    "            self.test_df[\"state\"] =[\"NAN\" for _ in range(len(self.test_df))]\n",
    "\n",
    "            self.main_df = pd.concat([self.main_df, self.test_df], ignore_index=True)\n",
    "\n",
    "            self.test_df =self.test_df.sort_values(by=[\"cfips\",\"first_day_of_month\"])\n",
    "            self.test_df = self.test_df.reset_index(drop=True)\n",
    "\n",
    "        if self.use_census:\n",
    "            #Merge the census features\n",
    "            self.cfips_index=get_cfips_index()\n",
    "            self.census_df = pd.read_csv(CENSUS_FILE)\n",
    "\n",
    "            self.main_df=pd.merge(self.main_df,self.census_df,on=[\"cfips\",\"first_day_of_month\"],how=\"left\")\n",
    "\n",
    "\n",
    "        ##Group by cfips and sort by date\n",
    "        self.main_df=self.main_df.sort_values(by=[\"cfips\",\"first_day_of_month\"])\n",
    "        self.main_df[\"id\"] =list(range(len(self.main_df)))\n",
    "\n",
    "\n",
    "\n",
    "    def prepare_sequences(self):\n",
    "        \"\"\"\n",
    "        Prepare the sequences for the LSTM:\n",
    "        Build a list of (id(offset), id(seq_len+offset)) tuples\n",
    "        \"\"\"\n",
    "        self.sequences=[]\n",
    "\n",
    "        if self.type == DatasetType.TRAIN:\n",
    "            ##Train data are dates before EVAL_START_DATE\n",
    "            df=self.main_df[self.main_df['first_day_of_month']<EVAL_START_DATE]\n",
    "\n",
    "            for i in tqdm(range(0, len(df)-self.seq_len, self.stride), desc=\"Preparing sequences of dataset of type train\"):\n",
    "\n",
    "                ##The cfips should be the same for the whole sequence(just check the first and last rows)\n",
    "                if df.iloc[i][\"cfips\"] != df.iloc[i + self.seq_len - 1][\"cfips\"]:\n",
    "                    continue\n",
    "\n",
    "                if i + self.seq_len > len(df) :\n",
    "                    break\n",
    "\n",
    "                #Get the corresponding ids\n",
    "                self.sequences.append((df.iloc[i][\"id\"], df.iloc[i][\"id\"]+ self.seq_len))\n",
    "\n",
    "\n",
    "\n",
    "        else :\n",
    "\n",
    "\n",
    "            if self.type == DatasetType.VALID:\n",
    "                df = self.main_df[self.main_df['first_day_of_month'] >= EVAL_START_DATE]\n",
    "\n",
    "            else:\n",
    "                df = self.main_df[self.main_df['first_day_of_month'] >= TEST_START_DATE]\n",
    "\n",
    "\n",
    "            for i in tqdm(range(0, len(df),self.stride), desc=\"Preparing sequences of dataset of type {}\".format(\"eval\" if self.type == DatasetType.VALID else \"test\")):\n",
    "                ## In eval and test sequences, the step to predict should always be the last one of the sequence\n",
    "\n",
    "                ##Find the offest of the start in the main df\n",
    "\n",
    "\n",
    "                offset=df.iloc[i][\"id\"]\n",
    "\n",
    "                offset = offset - self.seq_len  # The step to predict is the last one of the sequence\n",
    "\n",
    "\n",
    "                ##check if the cfips is the same\n",
    "                if self.main_df.iloc[offset][\"cfips\"] != self.main_df.iloc[offset + self.seq_len - 1][\"cfips\"]:\n",
    "                    #Warning\n",
    "                    print(\"Warning: cfips is not the same for the whole sequence . Offsets :\",offset,offset + self.seq_len - 1)\n",
    "\n",
    "                self.sequences.append((offset, offset + self.seq_len))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sequences)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        \"\"\"\n",
    "        Retrieving seq_len data\n",
    "        1. The county (CFIPS) should be the same\n",
    "        2. And the difference between the date(first_day_of_month) should be at most 3 months\n",
    "        \"\"\"\n",
    "        start,end=self.sequences[item]\n",
    "        rows_data=self.main_df.iloc[start:end]\n",
    "\n",
    "\n",
    "        #ensure unique cfips\n",
    "        # assert len(rows_data[\"cfips\"].unique())==1\n",
    "\n",
    "        tensor = torch.tensor(rows_data[['microbusiness_density']].values,\n",
    "                                       dtype=torch.float32)  # Not considering the census features\n",
    "\n",
    "        #FEatures scaling\n",
    "\n",
    "\n",
    "        if self.use_census:\n",
    "            censur_features_tensor = extract_census_features(rows_data, cfips_index=self.cfips_index,single_row=False)\n",
    "            tensor = torch.cat((censur_features_tensor,tensor), dim=1)\n",
    "\n",
    "\n",
    "\n",
    "        return tensor\n",
    "\n",
    "\n",
    "\n",
    "    def mix_with(self, other_dataset, size=0.8):\n",
    "        \"\"\"\n",
    "        Combine two datasets exemple a train dataset and test dataset\n",
    "        @param other_dataset:\n",
    "        @param size:\n",
    "        @return:\n",
    "        \"\"\"\n",
    "\n",
    "        all_sequences= self.sequences + other_dataset.sequences\n",
    "        random.shuffle(all_sequences)\n",
    "        self.sequences=all_sequences[:int(len(all_sequences)*size)]\n",
    "        other_dataset.sequences=all_sequences[int(len(all_sequences)*size):]\n",
    "        logging.info(\"Combined dataset: {} sequences for train and {} sequences for test\".format(len(self.sequences),len(other_dataset.sequences)))\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Trainer"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import csv\n",
    "import json\n",
    "import logging\n",
    "import os\n",
    "import shutil\n",
    "import warnings\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from tqdm import tqdm\n",
    "class TrainerTransformerPredictor:\n",
    "    \"\"\"\n",
    "    Class to manage the full training pipeline\n",
    "    \"\"\"\n",
    "    def __init__(self, network: TransformerPredictor,\n",
    "                 criterion,\n",
    "                 optimizer,\n",
    "                 scheduler=None,\n",
    "                 nb_epochs=10, batch_size=128, reset=False):\n",
    "        \"\"\"\n",
    "        @param network:\n",
    "        @param dataset_name:\n",
    "        @param images_dirs:\n",
    "        @param loss:\n",
    "        @param optimizer:\n",
    "        @param nb_epochs:\n",
    "        @param nb_workers: Number of worker for the dataloader\n",
    "        \"\"\"\n",
    "        self.network = network\n",
    "        self.batch_size = batch_size\n",
    "        self.loss_fn=criterion\n",
    "\n",
    "        self.optimizer = optimizer\n",
    "        self.scheduler =scheduler if scheduler else\\\n",
    "            torch.optim.lr_scheduler.ReduceLROnPlateau(self.optimizer, mode='min', factor=0.2, patience=10,min_lr=1e-5)\n",
    "\n",
    "        self.nb_epochs = nb_epochs\n",
    "        self.experiment_dir = self.network.experiment_dir\n",
    "        self.model_info_file = os.path.join(self.experiment_dir, \"model.json\")\n",
    "        self.model_info_best_file = os.path.join(self.experiment_dir, \"model_best.json\")\n",
    "\n",
    "        if reset:\n",
    "            if os.path.exists(self.experiment_dir):\n",
    "                shutil.rmtree(self.experiment_dir)\n",
    "        if not os.path.exists(self.experiment_dir):\n",
    "            os.makedirs(self.experiment_dir)\n",
    "\n",
    "        self.start_epoch = 0\n",
    "        if not reset and os.path.exists(self.model_info_file):\n",
    "            with open(self.model_info_file, \"r\") as f:\n",
    "                self.start_epoch = json.load(f)[\"epoch\"] + 1\n",
    "                self.nb_epochs += self.start_epoch\n",
    "                logging.info(\"Resuming from epoch {}\".format(self.start_epoch))\n",
    "\n",
    "\n",
    "    def save_model_info(self, infos, best=False):\n",
    "        json.dump(infos, open(self.model_info_file, 'w'),indent=4)\n",
    "        if best: json.dump(infos, open(self.model_info_best_file, 'w'),indent=4)\n",
    "\n",
    "    def fit(self,train_dataloader,val_dataloader):\n",
    "        logging.info(\"Launch training on {}\".format(DEVICE))\n",
    "        if self.network.use_census:\n",
    "            logging.info(\"Using encoder census data\")\n",
    "\n",
    "        self.summary_writer = SummaryWriter(log_dir=self.experiment_dir)\n",
    "        itr = self.start_epoch * len(train_dataloader) * self.batch_size  ##Global counter for steps\n",
    "\n",
    "        #Save model graph\n",
    "        # self.summary_writer.add_graph(self.network, next(iter(train_dataloader)).to(DEVICE)[:,:-1,:])\n",
    "\n",
    "        self.best_val_loss = 1e20  # infinity\n",
    "        if os.path.exists(self.model_info_file):\n",
    "            with open(self.model_info_file, \"r\") as f:\n",
    "                model_info = json.load(f)\n",
    "                lr=model_info[\"lr\"]\n",
    "                logging.info(f\"Setting lr to {lr}\")\n",
    "                for g in self.optimizer.param_groups:\n",
    "                    g['lr'] = lr\n",
    "\n",
    "        if os.path.exists(self.model_info_best_file):\n",
    "            with open(self.model_info_best_file, \"r\") as f:\n",
    "                best_model_info = json.load(f)\n",
    "                self.best_val_loss = best_model_info[\"val_loss\"]\n",
    "\n",
    "\n",
    "        for epoch in range(self.start_epoch, self.nb_epochs):  # Training loop\n",
    "            self.network.train()\n",
    "            \"\"\"\"\n",
    "            0. Initialize loss and other metrics\n",
    "            \"\"\"\n",
    "            running_loss=Averager()\n",
    "            pbar = tqdm(train_dataloader, desc=f\"Epoch {epoch + 1}/{self.nb_epochs}\")\n",
    "            for _, batch in enumerate(pbar):\n",
    "                \"\"\"\n",
    "                Training lopp\n",
    "                \"\"\"\n",
    "                self.optimizer.zero_grad()\n",
    "                itr += self.batch_size\n",
    "                \"\"\"\n",
    "                1.Forward pass\n",
    "                \"\"\"\n",
    "                batch = batch.to(DEVICE)\n",
    "\n",
    "                y_pred = self.network(batch)\n",
    "                ## The output is the values of the density for each time step\n",
    "\n",
    "                \"\"\"\n",
    "                2.Loss computation and other metrics\n",
    "                \"\"\"\n",
    "                # The density is the last item of the batch\n",
    "                y_true = batch[:,:,-1].to(DEVICE)\n",
    "                loss = self.loss_fn(y_pred, y_true[:, -1:])\n",
    "\n",
    "                \"\"\"\n",
    "                3.Optimizing\n",
    "                \"\"\"\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "                running_loss.send(loss.cpu().item())\n",
    "                pbar.set_postfix(current_loss=loss.cpu().item(), current_mean_loss=running_loss.value)\n",
    "\n",
    "                \"\"\"\n",
    "                4.Writing logs and tensorboard data, loss and other metrics\n",
    "                \"\"\"\n",
    "                self.summary_writer.add_scalar(\"Train/loss\", loss.item(), itr)\n",
    "                self.scheduler.step(loss.item())\n",
    "\n",
    "\n",
    "\n",
    "            epoch_val_loss =self.eval(val_dataloader,epoch)\n",
    "\n",
    "            infos = {\n",
    "                \"epoch\": epoch,\n",
    "                \"train_loss\":running_loss.value,\n",
    "                \"val_loss\":epoch_val_loss.value,\n",
    "                \"lr\": self.optimizer.param_groups[0]['lr'],\n",
    "                \"input_dim\": self.network.input_dim,\n",
    "                \"emb_dim\": self.network.emb_dim,\n",
    "                \"dim_feedforward\": self.network.dim_feedforward,\n",
    "                \"n_head\": self.network.n_head,\n",
    "                \"n_layers\": self.network.n_layers,\n",
    "                \"seq_len\": train_dataloader.dataset.seq_len,\n",
    "                \"batch_size\": train_dataloader.batch_size,\n",
    "                \"stride\": train_dataloader.dataset.stride,\n",
    "                \"use_census\": self.network.use_census,\n",
    "                \"variante\": self.network.variante_num,\n",
    "\n",
    "            }\n",
    "\n",
    "            logging.info(\"Epoch {} - Train loss: {:.4f} - Val loss: {:.4f}\".format(epoch, running_loss.value, epoch_val_loss.value))\n",
    "\n",
    "            if epoch_val_loss.value < self.best_val_loss:\n",
    "                self.best_val_loss = epoch_val_loss.value\n",
    "                best = True\n",
    "            else:\n",
    "                best = False\n",
    "\n",
    "            self.network.save_state(best=best)\n",
    "            self.save_model_info(infos, best=best)\n",
    "\n",
    "\n",
    "            self.summary_writer.add_scalar(\"Epoch_train/loss\", running_loss.value, epoch)\n",
    "            self.summary_writer.add_scalar(\"Epoch_val/loss\", epoch_val_loss.value, epoch)\n",
    "\n",
    "\n",
    "\n",
    "    def eval(self, val_dataloader,epoch):\n",
    "        \"\"\"\n",
    "        Compute loss and metrics on a validation dataloader\n",
    "        @return:\n",
    "        \"\"\"\n",
    "        with torch.no_grad():\n",
    "            self.network.eval()\n",
    "            running_loss=Averager()\n",
    "            pbar = tqdm(val_dataloader, desc=f\"Validation Epoch {epoch + 1}/{self.nb_epochs}\")\n",
    "            for _, batch in enumerate(pbar):\n",
    "\n",
    "                \"\"\"\n",
    "                Training lopp\n",
    "                \"\"\"\n",
    "                \"\"\"\n",
    "                1.Forward pass\n",
    "                \"\"\"\n",
    "                batch=batch.to(DEVICE)\n",
    "                y_pred = self.network(batch)\n",
    "                \"\"\"\n",
    "                2.Loss computation and other metrics\n",
    "                \"\"\"\n",
    "                y_true = batch[:,:,-1]\n",
    "\n",
    "\n",
    "                loss = self.loss_fn(y_pred, y_true[:, -1:])\n",
    "\n",
    "                running_loss.send(loss.item())\n",
    "\n",
    "                pbar.set_postfix(current_loss=loss.item(), current_mean_loss=running_loss.value)\n",
    "\n",
    "\n",
    "        return running_loss\n",
    "\n",
    "\n",
    "\n",
    "    def run_test(self, test_dataloader):\n",
    "        \"\"\"\n",
    "        Compute loss and metrics on a validation dataloader\n",
    "        @return:\n",
    "        \"\"\"\n",
    "        assert test_dataloader.batch_size == 1, \"Batch size must be 1 for test\"\n",
    "        predictions = []\n",
    "        row_ids = []\n",
    "        with torch.no_grad():\n",
    "            self.network.eval()\n",
    "            for i, input in enumerate(tqdm(test_dataloader,\" Running tests for submission\")):\n",
    "                input = input.to(DEVICE)\n",
    "                y_pred = self.network(input.to(DEVICE)).cpu().squeeze().item()\n",
    "\n",
    "                # Denormalize. MEAN_MB, STD_MB (if noramlized)\n",
    "                # y_pred = y_pred * STD_MB + MEAN_MB\n",
    "                \"\"\"\n",
    "                2.Loss computation and other metrics\n",
    "                \"\"\"\n",
    "                predictions.append(y_pred)\n",
    "\n",
    "                ##Update all microbusiness_den isty column\n",
    "                row_id=test_dataloader.dataset.test_df.loc[i,\"row_id\"]\n",
    "                row_ids.append(row_id)\n",
    "\n",
    "                test_dataloader.dataset.main_df.loc[test_dataloader.dataset.main_df[\"row_id\"]==row_id,\"microbusiness_density\"]=y_pred\n",
    "\n",
    "\n",
    "        #Merge predictions\n",
    "        predictions=np.array(predictions)\n",
    "\n",
    "\n",
    "        #Update all microbusiness_denisty column\n",
    "\n",
    "        pred_test_df = pd.DataFrame(\n",
    "            {\n",
    "                \"row_id\":row_ids,\n",
    "                 \"microbusiness_density\":predictions}\n",
    "\n",
    "                                )\n",
    "        pred_test_df.to_csv(os.path.join(self.experiment_dir,\"submission.csv\"),index=False)\n",
    "\n",
    "        return pred_test_df\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Runner"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "import argparse\n",
    "import logging\n",
    "import os\n",
    "import pickle\n",
    "import torch.utils.data\n",
    "\n",
    "@dataclass\n",
    "class Arguments:\n",
    "    reset: bool = False\n",
    "    learning_rate: float = 0.001\n",
    "    nb_epochs: int = 1000\n",
    "    model_name: str = None\n",
    "    num_workers: int = 0\n",
    "    batch_size: int = 1024\n",
    "    log_level: str = \"INFO\"\n",
    "    autorun_tb: bool = True\n",
    "    use_census: bool = True\n",
    "    use_derivative: bool = False\n",
    "    seq_len: int = 12\n",
    "    seq_stride: int = 1\n",
    "    emb_dim: int = 8\n",
    "    n_layers: int = 2\n",
    "    n_head: int = 2\n",
    "    dim_feedforward: int = 32\n",
    "\n",
    "def cli():\n",
    "    return Arguments()\n",
    "\n",
    "def main(args):\n",
    "\n",
    "    #Format the model name\n",
    "    if args.model_name is None:\n",
    "        model_name=f\"trf_{'ae_' if args.use_census else ''}{'dv_' if args.use_derivative else ''}ed.{args.emb_dim}_nl.{args.n_layers}_nh.{args.n_head}_df.{args.dim_feedforward}_sl.{args.seq_len}_ss.{args.seq_stride}_lr.{args.learning_rate}_bs.{args.batch_size}\"\n",
    "    else :\n",
    "        model_name=args.model_name\n",
    "\n",
    "\n",
    "\n",
    "    experiment_dir = os.path.join(EXPERIMENTS_DIR, model_name)\n",
    "\n",
    "\n",
    "    # Setup logger\n",
    "\n",
    "\n",
    "    network =TransformerPredictor(\n",
    "                                    experiment_dir=experiment_dir,\n",
    "                                    emb_dim=args.emb_dim,\n",
    "                                      n_layers=args.n_layers,\n",
    "                                      n_head=args.n_head,\n",
    "                                      dim_feedforward=args.dim_feedforward,\n",
    "                                      use_census=args.use_census,\n",
    "                                    max_seq_len=args.seq_len-1,\n",
    "                                    reset=args.reset\n",
    "                ).to(DEVICE)\n",
    "\n",
    "\n",
    "    #Adam optimizer\n",
    "    optimizer = torch.optim.Adam(network.parameters(), lr=args.learning_rate)\n",
    "\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=int(300*512/args.batch_size), factor=0.5, verbose=True ,min_lr=1e-5)\n",
    "\n",
    "    criterion= SmapeCriterion().to(DEVICE)\n",
    "\n",
    "\n",
    "    logging.info(\"Training : \"+model_name)\n",
    "    trainer = TrainerTransformerPredictor(network,\n",
    "                      criterion,\n",
    "                      optimizer=optimizer,\n",
    "                      scheduler=scheduler,\n",
    "                      nb_epochs= args.nb_epochs,\n",
    "                      batch_size=args.batch_size,\n",
    "                      reset=args.reset,\n",
    "                      )\n",
    "\n",
    "    # Save  the dataset according to type, seq_len_stride and use_census: using pickle\n",
    "\n",
    "    if not os.path.exists(os.path.join(ROOT_DIR,\"dataset\",\"pickle\")):\n",
    "        os.makedirs(os.path.join(ROOT_DIR,\"dataset\",\"pickle\"))\n",
    "\n",
    "    datasets_pickle_path = os.path.join(ROOT_DIR,\"dataset\",\"pickle\",f\"all_dataset_{args.seq_len}_{args.seq_stride}_{args.use_census}.pickle\")\n",
    "\n",
    "\n",
    "    if not os.path.exists(datasets_pickle_path):\n",
    "        train_dataset = MicroDensityDataset(type=DatasetType.TRAIN, seq_len=args.seq_len, stride=args.seq_stride,\n",
    "                                            use_census=args.use_census)\n",
    "        val_dataset = MicroDensityDataset(type=DatasetType.VALID, seq_len=args.seq_len, stride=args.seq_stride,\n",
    "                                          use_census=args.use_census)\n",
    "\n",
    "        train_dataset.mix_with(val_dataset,size=0.8) #Mix train and val dataset to avoid disparity between the two in terms of dates distribution\n",
    "\n",
    "        test_dataset = MicroDensityDataset(type=DatasetType.TEST, seq_len=args.seq_len, stride=args.seq_stride,\n",
    "                                           use_census=args.use_census)\n",
    "\n",
    "        with open(datasets_pickle_path,\"wb\") as f:\n",
    "            logging.info(f\"Saving datasets to {datasets_pickle_path}\")\n",
    "            pickle.dump((train_dataset,val_dataset,test_dataset),f)\n",
    "    else:\n",
    "        with open(datasets_pickle_path,\"rb\") as f:\n",
    "            logging.info(f\"Loading datasets  from {datasets_pickle_path}\")\n",
    "            train_dataset,val_dataset,test_dataset = pickle.load(f)\n",
    "\n",
    "\n",
    "\n",
    "    logging.info(f\"Nb sequences : Train {len(train_dataset)} - Val {len(val_dataset)} - Test {len(test_dataset)}\")\n",
    "\n",
    "    train_dataloader=torch.utils.data.DataLoader(train_dataset,batch_size=args.batch_size,num_workers=args.num_workers,shuffle=True,drop_last=False)\n",
    "    val_dataloader = torch.utils.data.DataLoader(val_dataset, batch_size=args.batch_size,num_workers=0,drop_last=False)\n",
    "    test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=1,num_workers=0,drop_last=False,shuffle=False)\n",
    "\n",
    "    ##Train\n",
    "    trainer.fit(train_dataloader,val_dataloader)\n",
    "\n",
    "    ##Load best model\n",
    "    trainer.network.load_state(best=True)\n",
    "    trainer.run_test(test_dataloader=test_dataloader)\n",
    "\n",
    "\n",
    "\n",
    "args = cli()\n",
    "setup_logger(args)\n",
    "main(args)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}