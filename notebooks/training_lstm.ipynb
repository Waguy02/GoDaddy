{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Setup\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name '__file__' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[1;32m~\\AppData\\Local\\Temp\\ipykernel_11376\\2038509542.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m      5\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mtorch\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      6\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m----> 7\u001B[1;33m \u001B[0mROOT_DIR\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mos\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mpath\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mdirname\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mos\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mpath\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mrealpath\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0m__file__\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m      8\u001B[0m \u001B[0mDATA_DIR\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mos\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mpath\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mjoin\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mROOT_DIR\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;34m\"data\"\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;34m\"godaddy-microbusiness-density-forecasting\"\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;31m##Directory of dataset\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      9\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mNameError\u001B[0m: name '__file__' is not defined"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "ROOT_DIR=os.path.dirname(os.path.realpath(__file__))\n",
    "DATA_DIR=os.path.join(ROOT_DIR,\"data\",\"godaddy-microbusiness-density-forecasting\") ##Directory of dataset\n",
    "\n",
    "EXPERIMENTS_DIR=os.path.join(ROOT_DIR, \"logs/experiments\")\n",
    "use_cuda = torch .cuda.is_available()\n",
    "DEVICE = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "\n",
    "\n",
    "N_CENSUS_FEATURES= 5 #pct_bb,pct_college,pct_foreign_born,pct_it_workers,median_hh_inc\n",
    "#cfips is not considered as a feature we use a one-hot encoding for it\n",
    "\n",
    "\n",
    "USE_CENSUS= False #Without census features\n",
    "\n",
    "AE_LATENT_DIM= 32\n",
    "\n",
    "LSTM_HIDDEN_DIM = 8\n",
    "\n",
    "SEQ_LEN=6\n",
    "SEQ_STRIDE= 1\n",
    "\n",
    "N_COUNTY=3142\n",
    "N_DIMS_COUNTY_ENCODING=  math.ceil(math.log(N_COUNTY,2))\n",
    "\n",
    "FEATURES_AE_CENSUS_DIR=os.path.join(EXPERIMENTS_DIR, \"features_ae_2_dims\")\n",
    "FEATURES_AE_LATENT_DIM= 2\n",
    "\n",
    "TRAIN_FILE= os.path.join(DATA_DIR, \"train.csv\")\n",
    "TEST_FILE= os.path.join(DATA_DIR, \"test.csv\")\n",
    "\n",
    "CENSUS_FILE =os.path.join(DATA_DIR, \"census_interpolated.csv\")\n",
    "\n",
    "NB_FUTURES= 10 #Number of days to predict\n",
    "\n",
    "\n",
    "#Scaling factors for microbusiness density\n",
    "MEAN_MB= 3.817671\n",
    "STD_MB= 4.991087\n",
    "\n",
    "MAX_MB= 300\n",
    "MIN_MB= 0.0"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Utils"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from enum import Enum\n",
    "from itertools import islice\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import logging\n",
    "import os\n",
    "import sys\n",
    "from time import strftime\n",
    "def setup_logger(args):\n",
    "    formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "    a_logger = logging.getLogger()\n",
    "    a_logger.setLevel(args.log_level)\n",
    "    log_dir=os.path.join(ROOT_DIR,\"logs\",\"output_logs\")\n",
    "    if not os.path.exists(log_dir):\n",
    "        os.makedirs(log_dir)\n",
    "    output_file_handler = logging.FileHandler(os.path.join(log_dir,strftime(\"log_%d_%m_%Y_%H_%M.log\")))\n",
    "    stdout_handler = logging.StreamHandler(sys.stdout)\n",
    "    stdout_handler.setFormatter(formatter)\n",
    "    a_logger.propagate=False\n",
    "    a_logger.addHandler(output_file_handler)\n",
    "    a_logger.addHandler(stdout_handler)\n",
    "\n",
    "def read_json(path_json):\n",
    "    with open(path_json, encoding='utf8') as json_file:\n",
    "        return json.load(json_file)\n",
    "def softmax(x):\n",
    "    return np.exp(x) / np.sum(np.exp(x))\n",
    "def chunks(data, SIZE):\n",
    "    \"\"\"Split a dictionnary into parts of max_size =SIZE\"\"\"\n",
    "    it = iter(data)\n",
    "    for _ in range(0, len(data), SIZE):\n",
    "        yield {k: data[k] for k in islice(it, SIZE)}\n",
    "\n",
    "def sorted_dict(x, ascending=True):\n",
    "    \"\"\"\n",
    "    Sort dict according to value.\n",
    "    x must be a primitive type: int,float, str...\n",
    "    @param x:\n",
    "    @return:\n",
    "    \"\"\"\n",
    "    return dict(sorted(x.items(), key=lambda item: (1 if ascending else -1) * item[1]))\n",
    "def reverse_dict(input_dict):\n",
    "    \"\"\"\n",
    "    Reverse a dictonary\n",
    "    Args:\n",
    "        input_dict:\n",
    "\n",
    "    Returns:\n",
    "\n",
    "    \"\"\"\n",
    "    inv_dict = {}\n",
    "    for k, v in input_dict.items():\n",
    "        inv_dict[v] = inv_dict.get(v, []) + [k]\n",
    "\n",
    "    return inv_dict\n",
    "\n",
    "def save_matrix(matrix,filename):\n",
    "    with open(filename,'wb') as output:\n",
    "        np.save(output,matrix)\n",
    "def load_matrix(filename,auto_delete=False):\n",
    "    with open(filename,'rb') as input:\n",
    "        matrix=np.load(input)\n",
    "\n",
    "    if auto_delete:\n",
    "        os.remove(filename)\n",
    "    return matrix\n",
    "\n",
    "\n",
    "\n",
    "class Averager:\n",
    "    def __init__(self):\n",
    "        self.current_total = 0.0\n",
    "        self.iterations = 0.0\n",
    "\n",
    "    def send(self, value):\n",
    "        self.current_total += value\n",
    "        self.iterations += 1\n",
    "\n",
    "    @property\n",
    "    def value(self):\n",
    "        if self.iterations == 0:\n",
    "            return 0\n",
    "        else:\n",
    "            return 1.0 * self.current_total / self.iterations\n",
    "\n",
    "    def reset(self):\n",
    "        self.current_total = 0.0\n",
    "        self.iterations = 0.0\n",
    "\n",
    "\n",
    "\n",
    "from enum import Enum\n",
    "class DatasetType(Enum):\n",
    "    TRAIN=\"train\"\n",
    "    VALID=\"valid\"\n",
    "    TEST=\"test\"\n",
    "\n",
    "\n",
    "\n",
    "def extract_census_features(row,cfips_index,single_row=True):\n",
    "    \"\"\"\n",
    "\n",
    "    @param row: Row of the dataframe\n",
    "    @param cfips_index: index of the cfips for one-hot encoding\n",
    "    @return:\n",
    "    \"\"\"\n",
    "    ##If series :\n",
    "\n",
    "\n",
    "    if single_row:\n",
    "        features_tensor = torch.tensor( [row['pct_bb'],\n",
    "                                        row['pct_college'],\n",
    "                                        row['pct_foreign_born'],\n",
    "                                        row['pct_it_workers'],\n",
    "                                        row['median_hh_inc']\n",
    "                                        ], dtype=torch.float32)\n",
    "        cfips_one_hot = get_cfips_encoding(row['cfips'], cfips_index)\n",
    "        # Min-max normalization\n",
    "        features_tensor[ 0] = (features_tensor[ 0] - 24.5) / (97.6 - 24.5)\n",
    "        features_tensor[ 1] = (features_tensor[ 1] / 48)\n",
    "        features_tensor[ 2] = (features_tensor[ 2] / 54)\n",
    "        features_tensor[ 3] = (features_tensor[ 3] / 17.4)\n",
    "        features_tensor[ 4] = (features_tensor[ 4] - 17109) / (1586821 - 17109)\n",
    "\n",
    "    else :\n",
    "        features_tensor= torch.from_numpy(row[['pct_bb', 'pct_college', 'pct_foreign_born', 'pct_it_workers', 'median_hh_inc']].values)\n",
    "        row_one_hots= [get_cfips_encoding(cfips,cfips_index) for cfips in row['cfips']]\n",
    "        cfips_one_hot = torch.stack(row_one_hots)\n",
    "        #Min-max normalization\n",
    "        features_tensor[:,0] = (features_tensor[:,0]- 24.5)/ (97.6-24.5)\n",
    "        features_tensor[:,1] = (features_tensor[:,1] /48)\n",
    "        features_tensor[:,2] = (features_tensor[:,2]/ 54)\n",
    "        features_tensor[:,3] = (features_tensor[:,3] / 17.4)\n",
    "        features_tensor[:,4] = (features_tensor[:,4]- 17109)/(1586821-17109)\n",
    "\n",
    "\n",
    "    ##Add one-hot encoding of cfips\n",
    "    if single_row:\n",
    "        features_tensor = torch.cat((cfips_one_hot, features_tensor))\n",
    "    else:\n",
    "        features_tensor = torch.cat((cfips_one_hot,features_tensor), 1)\n",
    "\n",
    "    return features_tensor.float()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def get_cfips_index():\n",
    "    \"\"\"\n",
    "    Return a dictionary with key=cfips and value=index for using a one-hot encoding\n",
    "    \"\"\"\n",
    "    df= pd.read_csv(os.path.join(DATA_DIR, \"census_ae.csv\"))\n",
    "    cfips = df['cfips'].unique()\n",
    "    cfips.sort()\n",
    "    #Sort cfips\n",
    "    return {cfips[i]: i for i in range(len(cfips))}\n",
    "\n",
    "\n",
    "def get_cfips_encoding(cfips,cfips_index):\n",
    "    \"\"\"\n",
    "     return the base 2 encoding of cfips\n",
    "    \"\"\"\n",
    "\n",
    "    #n_dims is the number of bits needed to represent the cfips\n",
    "\n",
    "    bin_index=np.binary_repr(cfips_index[cfips],width=N_DIMS_COUNTY_ENCODING)\n",
    "    enc = torch.tensor([int(x) for x in bin_index],dtype=torch.float32)\n",
    "    return enc\n",
    "\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Network"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import json\n",
    "import logging\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "class FeaturesAENetwork(nn.Module):\n",
    "    \"\"\"\n",
    "    The features are\n",
    "    pct_bb,pct_college,pct_foreign_born,pct_it_workers,median_hh_income and the date\n",
    "    Autoencoder network for features representation of the input data\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, experiment_dir=\"my_model\", reset=False, load_best=False, input_dim=N_CENSUS_FEATURES + N_DIMS_COUNTY_ENCODING,\n",
    "                 hidden_dim=AE_LATENT_DIM):\n",
    "        super(FeaturesAENetwork, self).__init__()\n",
    "        self.experiment_dir = experiment_dir\n",
    "        self.model_name = os.path.basename(self.experiment_dir)\n",
    "        self.reset = reset\n",
    "        self.load_best = load_best\n",
    "        self.setup_dirs()\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.setup_network()\n",
    "        if not reset: self.load_state()\n",
    "\n",
    "    ##1. Defining network architecture\n",
    "    def setup_network(self):\n",
    "        \"\"\"\n",
    "        Initialize the network  architecture here\n",
    "        @return:\n",
    "        \"\"\"\n",
    "\n",
    "        # 1. Encoder\n",
    "\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(self.input_dim,12),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(12, 12),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(12, self.hidden_dim),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "\n",
    "        # 2. Decoder\n",
    "        self.decoder_bone = nn.Sequential(\n",
    "            nn.Linear(self.hidden_dim, 12),\n",
    "            nn.Sigmoid(),\n",
    "            nn.Linear(12, 12),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "\n",
    "        # Decoder for census one-hot encoded features\n",
    "        self.decoder_cfips = nn.Sequential(\n",
    "            nn.Linear(12, N_DIMS_COUNTY_ENCODING),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "        ## Decoder for census continuous features\n",
    "        self.decoder_census = nn.Sequential(\n",
    "            nn.Linear(12, N_CENSUS_FEATURES),\n",
    "            )\n",
    "\n",
    "    ##2. Model Saving/Loading\n",
    "    def load_state(self, best=False):\n",
    "        \"\"\"\n",
    "        Load model\n",
    "        :param self:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        if best and os.path.exists(self.save_best_file):\n",
    "            logging.info(f\"Loading features encoder : {self.save_best_file}\")\n",
    "            self.load_state_dict(torch.load(self.save_file, map_location=DEVICE))\n",
    "            return\n",
    "\n",
    "        if os.path.exists(self.save_file):\n",
    "            logging.info(f\"Loading features encoder : {self.save_file}\")\n",
    "            self.load_state_dict(torch.load(self.save_file, map_location=DEVICE))\n",
    "\n",
    "    def save_state(self, best=False):\n",
    "        if best:\n",
    "            logging.info(\"Saving best model\")\n",
    "            torch.save(self.state_dict(), self.save_best_file)\n",
    "        torch.save(self.state_dict(), self.save_file)\n",
    "\n",
    "    ##3. Setupping directories for weights /logs ... etc\n",
    "    def setup_dirs(self):\n",
    "        \"\"\"\n",
    "        Checking and creating directories for weights storage\n",
    "        @return:\n",
    "        \"\"\"\n",
    "        self.save_file = os.path.join(self.experiment_dir, f\"{self.model_name}.pt\")\n",
    "        self.save_best_file = os.path.join(self.experiment_dir, f\"{self.model_name}_best.pt\")\n",
    "        if not os.path.exists(self.experiment_dir):\n",
    "            os.makedirs(self.experiment_dir)\n",
    "\n",
    "    # 4. Forward call\n",
    "    def forward(self, input):\n",
    "        \"\"\"\n",
    "        Forward call here during training.\n",
    "        Return the reconstructed input\n",
    "        \"\"\"\n",
    "        hidden_state = self.encoder(input)\n",
    "        output = self.decode(hidden_state)\n",
    "        return hidden_state, output\n",
    "\n",
    "\n",
    "    def decode(self,hidden_state):\n",
    "        output = self.decoder_bone(hidden_state)\n",
    "        output_census = self.decoder_census(output)\n",
    "        output_cfips = self.decoder_cfips(output)\n",
    "        output= torch.cat((output_cfips,output_census),1)\n",
    "        return output\n",
    "\n",
    "\n",
    "    # 5. Inference call (Just encoding)\n",
    "    def encode(self, input):\n",
    "        \"\"\"\n",
    "        Forward call here during inference.\n",
    "        Return the hidden state\n",
    "        \"\"\"\n",
    "        return self.encoder(input)\n",
    "\n",
    "import json\n",
    "import logging\n",
    "import os\n",
    "import torch\n",
    "import torchvision.models\n",
    "from torch import nn\n",
    "from constants import ROOT_DIR, DEVICE, LSTM_HIDDEN_DIM, N_CENSUS_FEATURES, USE_CENSUS, EXPERIMENTS_DIR, \\\n",
    "    FEATURES_AE_CENSUS_DIR, FEATURES_AE_LATENT_DIM\n",
    "from networks.features_autoencoder import FeaturesAENetwork\n",
    "\n",
    "\n",
    "class LstmPredictor(nn.Module):\n",
    "\n",
    "    def __init__(self, hidden_dim=LSTM_HIDDEN_DIM,\n",
    "                 n_hidden_layers=1,\n",
    "                 use_census=USE_CENSUS,\n",
    "                 experiment_dir=\"my_model\", reset=False, load_best=True):\n",
    "        \"\"\"\n",
    "        @param features_encoder :\n",
    "        @param input_dim:\n",
    "        @param hidden_dim:\n",
    "        @param ues_encoder:²\n",
    "        @param experiment_dir:\n",
    "        @param reset:\n",
    "        @param load_best:\n",
    "        \"\"\"\n",
    "\n",
    "        super(LstmPredictor, self).__init__()\n",
    "\n",
    "        self.variante_num=0\n",
    "\n",
    "        self.use_census  = use_census\n",
    "        if self.use_census:\n",
    "            #Get the hidden dimension of the encoder\n",
    "            config_encoder=os.path.join(FEATURES_AE_CENSUS_DIR,\"model.json\")\n",
    "            with open(config_encoder) as f:\n",
    "                config = json.load(f)\n",
    "                ae_hidden_dim = config[\"hidden_dim\"]\n",
    "            self.features_encoder = FeaturesAENetwork(experiment_dir=FEATURES_AE_CENSUS_DIR,hidden_dim=ae_hidden_dim).to(DEVICE)\n",
    "\n",
    "            # self.features_encoder = FeaturesAENetwork(hidden_dim=FEATURES_AE_LATENT_DIM).to(DEVICE)\n",
    "            self.input_dim = self.features_encoder.hidden_dim + 1\n",
    "\n",
    "            # # Freeze the encoder weights\n",
    "            # for param in self.features_encoder.parameters():\n",
    "            #     param.requires_grad = False\n",
    "\n",
    "        else :\n",
    "            self.features_encoder = None\n",
    "            self.input_dim =1\n",
    "\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.n_hidden_layers = n_hidden_layers\n",
    "        self.experiment_dir = experiment_dir\n",
    "        self.model_name = os.path.basename(self.experiment_dir)\n",
    "        self.reset = reset\n",
    "        self.load_best = load_best\n",
    "        self.setup_dirs()\n",
    "        self.setup_network()\n",
    "\n",
    "\n",
    "        if not reset: self.load_state()\n",
    "\n",
    "    ##1. Defining network architecture\n",
    "    def setup_network(self):\n",
    "        \"\"\"\n",
    "        Initialize the network  architecture here\n",
    "        @return:\n",
    "        \"\"\"\n",
    "        self.lstm=nn.LSTM(input_size=self.input_dim,hidden_size=self.hidden_dim,num_layers=self.n_hidden_layers,batch_first=True)\n",
    "\n",
    "        self.regressor=nn.Sequential(\n",
    "            nn.Linear(self.hidden_dim,8),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(8, 1)\n",
    "            )\n",
    "\n",
    "        if self.use_census:\n",
    "            # Freeze the encoder weights/\n",
    "            for param in self.features_encoder.parameters():\n",
    "                param.requires_grad = False\n",
    "\n",
    "\n",
    "    ##2. Model Saving/Loading\n",
    "    def load_state(self, best=False):\n",
    "        \"\"\"\n",
    "        Load model\n",
    "        :param self:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        if best and os.path.exists(self.save_best_file):\n",
    "            logging.info(f\"Loading best model state : {self.save_file}\")\n",
    "            self.load_state_dict(torch.load(self.save_file, map_location=DEVICE))\n",
    "            return\n",
    "\n",
    "        if os.path.exists(self.save_file):\n",
    "            logging.info(f\"Loading model state : {self.save_file}\")\n",
    "            self.load_state_dict(torch.load(self.save_file, map_location=DEVICE))\n",
    "\n",
    "    def save_state(self, best=False):\n",
    "        if best:\n",
    "            logging.info(\"Saving best model\")\n",
    "            torch.save(self.state_dict(), self.save_best_file)\n",
    "        torch.save(self.state_dict(), self.save_file)\n",
    "\n",
    "    ##3. Setupping directories for weights /logs ... etc\n",
    "    def setup_dirs(self):\n",
    "        \"\"\"\n",
    "        Checking and creating directories for weights storage\n",
    "        @return:\n",
    "        \"\"\"\n",
    "        self.save_file = os.path.join(self.experiment_dir, f\"{self.model_name}.pt\")\n",
    "        self.save_best_file = os.path.join(self.experiment_dir, f\"{self.model_name}_best.pt\")\n",
    "        if not os.path.exists(self.experiment_dir):\n",
    "            os.makedirs(self.experiment_dir)\n",
    "\n",
    "    #4. Forward call\n",
    "    def forward(self, input):\n",
    "        \"\"\"\n",
    "        Forward call here.\n",
    "        It a time series, so we need the full sequence output (strided by 1)\n",
    "        @param input:\n",
    "        @return:\n",
    "        \"\"\"\n",
    "        #1. First apply the encoder to the first N_CENSUS8FEAUTRES features of each element in the sequence\n",
    "        if self.use_census:\n",
    "            encoded_features = self.features_encoder.encode(input[:, :, :self.features_encoder.input_dim])\n",
    "            input = torch.cat((encoded_features, input[:, :, self.features_encoder.input_dim:]), dim=-1)\n",
    "\n",
    "        #2. Then apply the LSTM\n",
    "        output, _ = self.lstm(input)\n",
    "\n",
    "        #3. Finally apply the regressor to get the predictions.\n",
    "        output = self.regressor(output)\n",
    "\n",
    "        return output\n",
    "class LstmPredictor2(LstmPredictor):\n",
    "    \"\"\"\n",
    "    Lstm Predictor that do not directly incorporate census but only on the regression part\n",
    "    \"\"\"\n",
    "    def __init__(self, hidden_dim=LSTM_HIDDEN_DIM,\n",
    "                 n_hidden_layers=1,\n",
    "                 use_census=USE_CENSUS,\n",
    "                 experiment_dir=\"my_model\", reset=False, load_best=True):\n",
    "        \"\"\"\n",
    "        @param features_encoder :\n",
    "        @param input_dim:\n",
    "        @param hidden_dim:\n",
    "        @param ues_encoder:²\n",
    "        @param experiment_dir:\n",
    "        @param reset:\n",
    "        @param load_best:\n",
    "        \"\"\"\n",
    "\n",
    "        super(LstmPredictor2, self).__init__(hidden_dim=hidden_dim,\n",
    "                                             n_hidden_layers=n_hidden_layers,\n",
    "                                             use_census=use_census,\n",
    "                                             experiment_dir=experiment_dir,\n",
    "                                             reset=reset,\n",
    "                                             load_best=load_best)\n",
    "\n",
    "        self.variante_num=1\n",
    "\n",
    "    ##1. Defining network architecture\n",
    "    def setup_network(self):\n",
    "        \"\"\"\n",
    "        Initialize the network  architecture here\n",
    "        @return:\n",
    "        \"\"\"\n",
    "        self.input_dim = 1\n",
    "\n",
    "        if self.use_census:\n",
    "            # Freeze the encoder weights\n",
    "            # for param in self.features_encoder.parameters():\n",
    "            #     param.requires_grad = False\n",
    "            pass\n",
    "\n",
    "\n",
    "        self.regressor_dim = self.hidden_dim + (0 if not self.use_census else self.features_encoder.hidden_dim)\n",
    "\n",
    "        self.lstm=nn.LSTM(input_size=self.input_dim,hidden_size=self.hidden_dim,num_layers=self.n_hidden_layers,batch_first=True)\n",
    "\n",
    "        self.regressor=nn.Sequential(\n",
    "            nn.Linear(self.regressor_dim, 8),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(8, 1))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    #4. Forward call\n",
    "    def forward(self, input):\n",
    "        \"\"\"\n",
    "        Forward call here.\n",
    "        It a time series, so we need the full sequence output (strided by 1)\n",
    "        @param input:\n",
    "        @return:\n",
    "        \"\"\"\n",
    "\n",
    "\n",
    "        #1. First apply the encoder to the first N_CENSUS8FEAUTRES features of each element in the sequence\n",
    "        if self.use_census:\n",
    "            encoded_features = self.features_encoder.encode(input[:, :, :self.features_encoder.input_dim])\n",
    "            ##The encoded features do not go through the LSTM, so we need to concatenate them with the rest of the input\n",
    "            lstm_input= input[:, :, self.features_encoder.input_dim:]\n",
    "        else:\n",
    "            lstm_input = input\n",
    "\n",
    "        #2. Then apply the LSTM\n",
    "        lstm_output, _ = self.lstm(lstm_input)\n",
    "\n",
    "        #3. Concatenate the encoded features with the LSTM output\n",
    "        if self.use_census:\n",
    "            output = torch.cat((encoded_features, lstm_output), dim=2)\n",
    "        else :\n",
    "            output = lstm_output\n",
    "\n",
    "        #3. Finally apply the regressor to get the predictions.\n",
    "        output = self.regressor(output)\n",
    "\n",
    "        return output\n",
    "class LstmPredictorWithAttention(LstmPredictor2):\n",
    "    def __init__(self, hidden_dim=LSTM_HIDDEN_DIM,\n",
    "                 n_hidden_layers=1,\n",
    "                 use_census=USE_CENSUS,\n",
    "                 use_derivative=False,\n",
    "                 experiment_dir=\"my_model\", reset=False, load_best=True):\n",
    "        self.use_derivative=use_derivative\n",
    "        super(LstmPredictorWithAttention, self).__init__(hidden_dim=hidden_dim,\n",
    "                                                         n_hidden_layers=n_hidden_layers,\n",
    "                                                         use_census=use_census,\n",
    "                                                         experiment_dir=experiment_dir,\n",
    "                                                         reset=reset,\n",
    "                                                         load_best=load_best)\n",
    "        self. variante_num=2\n",
    "\n",
    "\n",
    "    def setup_network(self):\n",
    "        \"\"\"\n",
    "                Initialize the network  architecture here\n",
    "                @return:\n",
    "                \"\"\"\n",
    "        self.input_dim = 1\n",
    "\n",
    "        if self.use_derivative:\n",
    "            self.input_dim += 2# 2 for the derivative at left and right\n",
    "\n",
    "        if self.use_census:\n",
    "            # Freeze the encoder weights\n",
    "            self.input_dim+=self.features_encoder.hidden_dim\n",
    "            for param in self.features_encoder.parameters():\n",
    "                param.requires_grad = False\n",
    "\n",
    "\n",
    "        self.regressor_dim = self.hidden_dim + (0 if not self.use_census else self.features_encoder.hidden_dim)\n",
    "\n",
    "        self.lstm = nn.LSTM(input_size=self.input_dim, hidden_size=self.hidden_dim, num_layers=self.n_hidden_layers,\n",
    "                            batch_first=True)\n",
    "\n",
    "        self.regressor = nn.Sequential(\n",
    "            nn.Linear(self.regressor_dim, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 1))\n",
    "        self.attention=nn.Linear(self.hidden_dim,1)\n",
    "\n",
    "    def forward(self, input):\n",
    "\n",
    "        #Remove the target (last token)\n",
    "        query = input[:, -1, :]\n",
    "        input = input[:, :-1, :]\n",
    "\n",
    "        if self.use_census:\n",
    "            encoded_features = self.features_encoder.encode(input[:,:,:self.features_encoder.input_dim])\n",
    "            encoded_query = self.features_encoder.encode(query[:,  :self.features_encoder.input_dim])\n",
    "            input = torch.cat((encoded_features, input[:, :, self.features_encoder.input_dim:]), dim=2)\n",
    "\n",
    "\n",
    "\n",
    "        if self.use_derivative:\n",
    "            d_left= torch.zeros((input.shape[0],input.shape[1],1), device=DEVICE)\n",
    "            d_left[:,1:, -1] = input[:, 1:, -1] - input[:, :-1, -1]\n",
    "\n",
    "            d_right= torch.zeros((input.shape[0],input.shape[1],1), device=DEVICE)\n",
    "            d_right[:,:-1, -1] = input[:, 1:, -1] - input[:, :-1, -1]\n",
    "\n",
    "            input = torch.cat((d_left, input, d_right), dim=-1) ## Adding the derivative to the input as a new feature\n",
    "\n",
    "\n",
    "        lstm_output, _ = self.lstm(input[:, :, -self.input_dim:])\n",
    "\n",
    "        # Apply attention to the LSTM output\n",
    "        attention_weights = self.attention(lstm_output)\n",
    "        attention_weights = torch.softmax(attention_weights, dim=1)\n",
    "        weighted_output = attention_weights * lstm_output\n",
    "\n",
    "\n",
    "\n",
    "        context_vector = weighted_output\n",
    "\n",
    "        context_vector=context_vector.sum(dim=1)\n",
    "        if self.use_census:\n",
    "            context_vector = torch.cat((context_vector, encoded_query), dim=1)\n",
    "\n",
    "        # Apply the regressor to get the predictions\n",
    "        output = self.regressor(context_vector)\n",
    "\n",
    "\n",
    "        return output\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Dataset"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "class LstmDataset(Dataset):\n",
    "    def __init__(self, type, seq_len, stride=1):\n",
    "        self.type = type\n",
    "        self.seq_len = seq_len\n",
    "        self.stride = stride\n",
    "\n",
    "        self.file = os.path.join(DATA_DIR, f\"train_with_census_{'train' if type==DatasetType.TRAIN else 'val' if type==DatasetType.VALID  else 'test'}.csv\")\n",
    "        self.load_data()\n",
    "\n",
    "    def init_transforms(self):\n",
    "        \"\"\"\n",
    "        Initialize transforms.Might be different for each dataset type\n",
    "        \"\"\"\n",
    "\n",
    "    def load_data(self):\n",
    "        \"\"\"\n",
    "        Load data from the data items if necessary\n",
    "        \"\"\"\n",
    "        self.data = pd.read_csv(self.file)\n",
    "        self.data['first_day_of_month'] = pd.to_datetime(self.data['first_day_of_month'])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data) // self.stride\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        \"\"\"\n",
    "        Retrieving seq_len data\n",
    "        1. The county (CFIPS) should be the same\n",
    "        2. And the difference between the date(first_day_of_month) should be at most 3 months\n",
    "        \"\"\"\n",
    "        i = item * self.stride\n",
    "        county = self.data.iloc[i]['cfips']\n",
    "\n",
    "        rows_data=self.data.iloc[i:i+self.seq_len]\n",
    "\n",
    "        #Check if the county is the same\n",
    "        is_valid = len(rows_data)==self.seq_len and (rows_data['cfips'].unique()[0]==county) and (rows_data['first_day_of_month'].diff().max()<pd.Timedelta(days=90))\n",
    "\n",
    "        if not is_valid:\n",
    "            ##Find a random item that is valid\n",
    "            return self.__getitem__(torch.randint(0, len(self), (1,)).item())\n",
    "\n",
    "        #Taking seq_len rows and considering the following features\n",
    "        #pct_bb,pct_college,pct_foreign_born,pct_it_workers,median_hh_inc, active,microbusiness_density\n",
    "        features_tensor = torch.tensor(\n",
    "            rows_data[['pct_bb', 'pct_college', 'pct_foreign_born', 'pct_it_workers', 'median_hh_inc','year', 'active',\n",
    "                        'microbusiness_density']].values, dtype=torch.float32)\n",
    "\n",
    "        #return the iterator\n",
    "        return features_tensor\n",
    "\n",
    "import json\n",
    "import os\n",
    "from unicodedata import category\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from PIL import Image\n",
    "from matplotlib import pyplot as plt\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision.transforms import transforms\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "\n",
    "from enum import Enum\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class CensusDataset(Dataset):\n",
    "    def __init__(self, type):\n",
    "        self.type=type\n",
    "        self.load_data()\n",
    "        pass\n",
    "\n",
    "    def load_data(self):\n",
    "        \"\"\"\n",
    "        Load data from the data items if necessary\n",
    "        Returns:\n",
    "\n",
    "        \"\"\"\n",
    "        self.data_file=os.path.join(DATA_DIR,f\"train_with_census_ae_{'train' if self.type == DatasetType.TRAIN else 'test'}.csv\")\n",
    "        self.data = pd.read_csv(self.data_file)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        pct_bb,pct_college,pct_foreign_born,pct_it_workers,median_hh_inc,year .\n",
    "        Retrieve the following features from the dataset and return the corresponding tensor\n",
    "\n",
    "        Returns:\n",
    "        \"\"\"\n",
    "        row=self.data.iloc[idx]\n",
    "        features_tensor=torch.tensor([row['pct_bb'],row['pct_college'],row['pct_foreign_born'],\\\n",
    "                                      row['pct_it_workers'],row['median_hh_inc'],row['year']],dtype=torch.float32)\n",
    "        return features_tensor\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Loss and metrics"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "\n",
    "class SmapeCriterion(nn.Module):\n",
    "    \"\"\"\n",
    "    Class to compute the SMAPE loss.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super(SmapeCriterion, self).__init__()\n",
    "\n",
    "    def forward(self, y_pred, y_true):\n",
    "        \"\"\"\n",
    "        @param y_pred: Predicted values\n",
    "        @param y_true: True values\n",
    "        @return: SMAPE loss\n",
    "        \"\"\"\n",
    "        eps = 1e-8\n",
    "        return 100*torch.mean(2 * torch.abs(y_pred - y_true) / (torch.abs(y_pred) + torch.abs(y_true) + eps))\n",
    "\n",
    "    def __str__(self):\n",
    "        return \"SMAPE\"\n",
    "\n",
    "    def __repr__(self):\n",
    "        return str(self)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Dataset"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "import random\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from tqdm import tqdm\n",
    "\n",
    "EVAL_START_DATE = \"2022-05-01\"\n",
    "TEST_START_DATE =  \"2022-11-01\"\n",
    "\n",
    "SEED=42\n",
    "class MicroDensityDataset(Dataset):\n",
    "    def __init__(self, type, seq_len, stride=1,use_census=USE_CENSUS):\n",
    "        self.type = type\n",
    "        self.seq_len = seq_len\n",
    "        self.stride = stride if type == DatasetType.TRAIN else 1\n",
    "        self.use_census = use_census\n",
    "        self.load_data()\n",
    "        self.prepare_sequences()\n",
    "\n",
    "\n",
    "    def init_transforms(self):\n",
    "        \"\"\"\n",
    "        Initialize transforms.Might be different for each dataset type\n",
    "        \"\"\"\n",
    "\n",
    "    def load_data(self):\n",
    "        \"\"\"\n",
    "        Load data from the data items if necessary\n",
    "        \"\"\"\n",
    "\n",
    "        self.main_file = os.path.join(DATA_DIR, \"train.csv\")\n",
    "        self.main_df = pd.read_csv(self.main_file)\n",
    "\n",
    "        if self.type == DatasetType.TEST:\n",
    "            self.test_df = pd.read_csv(TEST_FILE)\n",
    "            self.test_df[\"microbusiness_density\"] = [0 for _ in range(len(self.test_df))]\n",
    "            self.test_df[\"county\"] =[\"NAN\" for _ in range(len(self.test_df))]\n",
    "            self.test_df[\"state\"] =[\"NAN\" for _ in range(len(self.test_df))]\n",
    "\n",
    "            self.main_df = pd.concat([self.main_df, self.test_df], ignore_index=True)\n",
    "\n",
    "            self.test_df =self.test_df.sort_values(by=[\"cfips\",\"first_day_of_month\"])\n",
    "            self.test_df = self.test_df.reset_index(drop=True)\n",
    "\n",
    "        if self.use_census:\n",
    "            #Merge the census features\n",
    "            self.cfips_index=get_cfips_index()\n",
    "            self.census_df = pd.read_csv(CENSUS_FILE)\n",
    "\n",
    "            self.main_df=pd.merge(self.main_df,self.census_df,on=[\"cfips\",\"first_day_of_month\"],how=\"left\")\n",
    "\n",
    "\n",
    "        ##Group by cfips and sort by date\n",
    "        self.main_df=self.main_df.sort_values(by=[\"cfips\",\"first_day_of_month\"])\n",
    "        self.main_df[\"id\"] =list(range(len(self.main_df)))\n",
    "\n",
    "\n",
    "\n",
    "    def prepare_sequences(self):\n",
    "        \"\"\"\n",
    "        Prepare the sequences for the LSTM:\n",
    "        Build a list of (id(offset), id(seq_len+offset)) tuples\n",
    "        \"\"\"\n",
    "        self.sequences=[]\n",
    "\n",
    "        if self.type == DatasetType.TRAIN:\n",
    "            ##Train data are dates before EVAL_START_DATE\n",
    "            df=self.main_df[self.main_df['first_day_of_month']<EVAL_START_DATE]\n",
    "\n",
    "            for i in tqdm(range(0, len(df)-self.seq_len, self.stride), desc=\"Preparing sequences of dataset of type train\"):\n",
    "\n",
    "                ##The cfips should be the same for the whole sequence(just check the first and last rows)\n",
    "                if df.iloc[i][\"cfips\"] != df.iloc[i + self.seq_len - 1][\"cfips\"]:\n",
    "                    continue\n",
    "\n",
    "                if i + self.seq_len > len(df) :\n",
    "                    break\n",
    "\n",
    "                #Get the corresponding ids\n",
    "                self.sequences.append((df.iloc[i][\"id\"], df.iloc[i][\"id\"]+ self.seq_len))\n",
    "\n",
    "\n",
    "\n",
    "        else :\n",
    "\n",
    "\n",
    "            if self.type == DatasetType.VALID:\n",
    "                df = self.main_df[self.main_df['first_day_of_month'] >= EVAL_START_DATE]\n",
    "\n",
    "            else:\n",
    "                df = self.main_df[self.main_df['first_day_of_month'] >= TEST_START_DATE]\n",
    "\n",
    "\n",
    "            for i in tqdm(range(0, len(df),self.stride), desc=\"Preparing sequences of dataset of type {}\".format(\"eval\" if self.type == DatasetType.VALID else \"test\")):\n",
    "                ## In eval and test sequences, the step to predict should always be the last one of the sequence\n",
    "\n",
    "                ##Find the offest of the start in the main df\n",
    "\n",
    "\n",
    "                offset=df.iloc[i][\"id\"]\n",
    "\n",
    "                offset = offset - self.seq_len  # The step to predict is the last one of the sequence\n",
    "\n",
    "\n",
    "                ##check if the cfips is the same\n",
    "                if self.main_df.iloc[offset][\"cfips\"] != self.main_df.iloc[offset + self.seq_len - 1][\"cfips\"]:\n",
    "                    #Warning\n",
    "                    print(\"Warning: cfips is not the same for the whole sequence . Offsets :\",offset,offset + self.seq_len - 1)\n",
    "\n",
    "                self.sequences.append((offset, offset + self.seq_len))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sequences)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        \"\"\"\n",
    "        Retrieving seq_len data\n",
    "        1. The county (CFIPS) should be the same\n",
    "        2. And the difference between the date(first_day_of_month) should be at most 3 months\n",
    "        \"\"\"\n",
    "        start,end=self.sequences[item]\n",
    "        rows_data=self.main_df.iloc[start:end]\n",
    "\n",
    "\n",
    "        #ensure unique cfips\n",
    "        # assert len(rows_data[\"cfips\"].unique())==1\n",
    "\n",
    "        tensor = torch.tensor(rows_data[['microbusiness_density']].values,\n",
    "                                       dtype=torch.float32)  # Not considering the census features\n",
    "\n",
    "        #FEatures scaling\n",
    "\n",
    "\n",
    "        if self.use_census:\n",
    "            censur_features_tensor = extract_census_features(rows_data, cfips_index=self.cfips_index,single_row=False)\n",
    "            tensor = torch.cat((censur_features_tensor,tensor), dim=1)\n",
    "\n",
    "\n",
    "\n",
    "        return tensor\n",
    "\n",
    "\n",
    "\n",
    "    def mix_with(self, other_dataset, size=0.8):\n",
    "        \"\"\"\n",
    "        Combine two datasets exemple a train dataset and test dataset\n",
    "        @param other_dataset:\n",
    "        @param size:\n",
    "        @return:\n",
    "        \"\"\"\n",
    "\n",
    "        all_sequences= self.sequences + other_dataset.sequences\n",
    "        random.shuffle(all_sequences)\n",
    "        self.sequences=all_sequences[:int(len(all_sequences)*size)]\n",
    "        other_dataset.sequences=all_sequences[int(len(all_sequences)*size):]\n",
    "        logging.info(\"Combined dataset: {} sequences for train and {} sequences for test\".format(len(self.sequences),len(other_dataset.sequences)))\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Trainer"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import csv\n",
    "import json\n",
    "import logging\n",
    "import os\n",
    "import shutil\n",
    "import warnings\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from tqdm import tqdm\n",
    "class TrainerLstmPredictor:\n",
    "    \"\"\"\n",
    "    Class to manage the full training pipeline\n",
    "    \"\"\"\n",
    "    def __init__(self, network,\n",
    "                 criterion,\n",
    "                 optimizer,\n",
    "                 scheduler=None,\n",
    "                 nb_epochs=10, batch_size=128, reset=False):\n",
    "        \"\"\"\n",
    "        @param network:\n",
    "        @param dataset_name:\n",
    "        @param images_dirs:\n",
    "        @param loss:\n",
    "        @param optimizer:\n",
    "        @param nb_epochs:\n",
    "        @param nb_workers: Number of worker for the dataloader\n",
    "        \"\"\"\n",
    "        self.network = network\n",
    "        self.batch_size = batch_size\n",
    "        self.loss_fn=criterion\n",
    "\n",
    "        self.optimizer = optimizer\n",
    "        self.scheduler =scheduler if scheduler else\\\n",
    "            torch.optim.lr_scheduler.ReduceLROnPlateau(self.optimizer, mode='min', factor=0.2, patience=10,min_lr=1e-5)\n",
    "\n",
    "        self.nb_epochs = nb_epochs\n",
    "        self.experiment_dir = self.network.experiment_dir\n",
    "        self.model_info_file = os.path.join(self.experiment_dir, \"model.json\")\n",
    "        self.model_info_best_file = os.path.join(self.experiment_dir, \"model_best.json\")\n",
    "\n",
    "        if reset:\n",
    "            if os.path.exists(self.experiment_dir):\n",
    "                shutil.rmtree(self.experiment_dir)\n",
    "        if not os.path.exists(self.experiment_dir):\n",
    "            os.makedirs(self.experiment_dir)\n",
    "\n",
    "        self.start_epoch = 0\n",
    "        if not reset and os.path.exists(self.model_info_file):\n",
    "            with open(self.model_info_file, \"r\") as f:\n",
    "                self.start_epoch = json.load(f)[\"epoch\"] + 1\n",
    "                self.nb_epochs += self.start_epoch\n",
    "                logging.info(\"Resuming from epoch {}\".format(self.start_epoch))\n",
    "\n",
    "\n",
    "    def save_model_info(self, infos, best=False):\n",
    "        json.dump(infos, open(self.model_info_file, 'w'),indent=4)\n",
    "        if best: json.dump(infos, open(self.model_info_best_file, 'w'),indent=4)\n",
    "\n",
    "    def fit(self,train_dataloader,val_dataloader):\n",
    "        logging.info(\"Launch training on {}\".format(DEVICE))\n",
    "        if self.network.use_census:\n",
    "            logging.info(\"Using encoder census data\")\n",
    "\n",
    "        self.summary_writer = SummaryWriter(log_dir=self.experiment_dir)\n",
    "        itr = self.start_epoch * len(train_dataloader) * self.batch_size  ##Global counter for steps\n",
    "\n",
    "        #Save model graph\n",
    "        self.summary_writer.add_graph(self.network, next(iter(train_dataloader)).to(DEVICE))\n",
    "\n",
    "        self.best_val_loss = 1e20  # infinity\n",
    "        if os.path.exists(self.model_info_file):\n",
    "            with open(self.model_info_file, \"r\") as f:\n",
    "                model_info = json.load(f)\n",
    "                lr=model_info[\"lr\"]\n",
    "                logging.info(f\"Setting lr to {lr}\")\n",
    "                for g in self.optimizer.param_groups:\n",
    "                    g['lr'] = lr\n",
    "\n",
    "        if os.path.exists(self.model_info_best_file):\n",
    "            with open(self.model_info_best_file, \"r\") as f:\n",
    "                best_model_info = json.load(f)\n",
    "                self.best_val_loss = best_model_info[\"val_loss\"]\n",
    "\n",
    "\n",
    "        for epoch in range(self.start_epoch, self.nb_epochs):  # Training loop\n",
    "            self.network.train()\n",
    "            \"\"\"\"\n",
    "            0. Initialize loss and other metrics\n",
    "            \"\"\"\n",
    "            running_loss=Averager()\n",
    "            pbar = tqdm(train_dataloader, desc=f\"Epoch {epoch + 1}/{self.nb_epochs}\")\n",
    "            for _, batch in enumerate(pbar):\n",
    "                \"\"\"\n",
    "                Training lopp\n",
    "                \"\"\"\n",
    "                self.optimizer.zero_grad()\n",
    "                itr += self.batch_size\n",
    "                \"\"\"\n",
    "                1.Forward pass\n",
    "                \"\"\"\n",
    "                batch=batch.to(DEVICE)\n",
    "                y_pred = self.network(batch[:,:-1,:])  # [batch_size, seq_len, 1\n",
    "                ## The output is the values of the density for each time step\n",
    "\n",
    "                \"\"\"\n",
    "                2.Loss computation and other metrics\n",
    "                \"\"\"\n",
    "                # The density is the last item of the batch\n",
    "                y_true = batch[:,:,-1]\n",
    "\n",
    "                nb_futures = min(train_dataloader.dataset.seq_len - 1, NB_FUTURES)\n",
    "                if self.network.variante_num ==2:#Attention model: (single output)\n",
    "                    loss = self.loss_fn(y_pred, y_true[:, -nb_futures:])\n",
    "                else :\n",
    "                    y_pred=y_pred.squeeze()\n",
    "                    loss = self.loss_fn(y_pred[:, -1 - nb_futures:-1], y_true[:, -nb_futures:])\n",
    "\n",
    "\n",
    "\n",
    "                \"\"\"\n",
    "                3.Optimizing\n",
    "                \"\"\"\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "\n",
    "\n",
    "                running_loss.send(loss.cpu().item())\n",
    "                pbar.set_postfix(current_loss=loss.cpu().item(), current_mean_loss=running_loss.value)\n",
    "\n",
    "                \"\"\"\n",
    "                4.Writing logs and tensorboard data, loss and other metrics\n",
    "                \"\"\"\n",
    "                self.summary_writer.add_scalar(\"Train/loss\", loss.item(), itr)\n",
    "\n",
    "                self.scheduler.step(loss.item())\n",
    "\n",
    "\n",
    "            epoch_val_loss =self.eval(val_dataloader,epoch)\n",
    "\n",
    "            infos = {\n",
    "                \"epoch\": epoch,\n",
    "                \"train_loss\":running_loss.value,\n",
    "                \"val_loss\":epoch_val_loss.value,\n",
    "                \"lr\": self.optimizer.param_groups[0]['lr'],\n",
    "                \"input_dim\": self.network.input_dim,\n",
    "                \"hidden_dim\": self.network.hidden_dim,\n",
    "                \"n_hidden_lstm_layers\": self.network.n_hidden_layers,\n",
    "                \"seq_len\": train_dataloader.dataset.seq_len,\n",
    "                \"batch_size\": train_dataloader.batch_size,\n",
    "                \"stride\": train_dataloader.dataset.stride,\n",
    "                \"use_census\": self.network.use_census,\n",
    "                \"variante\": self.network.variante_num,\n",
    "                \"census_dim\": -1 if not self.network.use_census else self.network.features_encoder.hidden_dim,\n",
    "            }\n",
    "\n",
    "            logging.info(\"Epoch {} - Train loss: {:.4f} - Val loss: {:.4f}\".format(epoch, running_loss.value, epoch_val_loss.value))\n",
    "\n",
    "            if epoch_val_loss.value < self.best_val_loss:\n",
    "                self.best_val_loss = epoch_val_loss.value\n",
    "                best = True\n",
    "            else:\n",
    "                best = False\n",
    "\n",
    "            self.network.save_state(best=best)\n",
    "            self.save_model_info(infos, best=best)\n",
    "\n",
    "             # if scheduler is StepLR\n",
    "            # if isinstance(self.scheduler, torch.optim.lr_scheduler.StepLR):\n",
    "            #     self.scheduler.step()\n",
    "            # else:\n",
    "            #     self.scheduler.step(epoch_val_loss.value)\n",
    "\n",
    "            self.summary_writer.add_scalar(\"Epoch_train/loss\", running_loss.value, epoch)\n",
    "            self.summary_writer.add_scalar(\"Epoch_val/loss\", epoch_val_loss.value, epoch)\n",
    "\n",
    "\n",
    "\n",
    "    def eval(self, val_dataloader,epoch):\n",
    "        \"\"\"\n",
    "        Compute loss and metrics on a validation dataloader\n",
    "        @return:\n",
    "        \"\"\"\n",
    "        with torch.no_grad():\n",
    "            self.network.eval()\n",
    "            running_loss=Averager()\n",
    "            pbar = tqdm(val_dataloader, desc=f\"Validation Epoch {epoch + 1}/{self.nb_epochs}\")\n",
    "            for _, batch in enumerate(pbar):\n",
    "\n",
    "                \"\"\"\n",
    "                Training lopp\n",
    "                \"\"\"\n",
    "                \"\"\"\n",
    "                1.Forward pass\n",
    "                \"\"\"\n",
    "                batch=batch.to(DEVICE)\n",
    "                y_pred = self.network(batch)\n",
    "                \"\"\"\n",
    "                2.Loss computation and other metrics\n",
    "                \"\"\"\n",
    "                y_true = batch[:,:,-1]\n",
    "\n",
    "\n",
    "\n",
    "                loss = self.loss_fn(y_pred, y_true[:, -1:])\n",
    "\n",
    "                running_loss.send(loss.item())\n",
    "\n",
    "                pbar.set_postfix(current_loss=loss.item(), current_mean_loss=running_loss.value)\n",
    "\n",
    "        return running_loss\n",
    "\n",
    "\n",
    "\n",
    "    def run_test(self, test_dataloader):\n",
    "        \"\"\"\n",
    "        Compute loss and metrics on a validation dataloader\n",
    "        @return:\n",
    "        \"\"\"\n",
    "        assert test_dataloader.batch_size == 1, \"Batch size must be 1 for test\"\n",
    "        predictions = []\n",
    "        row_ids = []\n",
    "        with torch.no_grad():\n",
    "            self.network.eval()\n",
    "            for i, batch in enumerate(tqdm(test_dataloader,\" Running tests for submission\")):\n",
    "                batch = batch.to(DEVICE)\n",
    "                y_pred = self.network(batch).cpu().squeeze().item()\n",
    "\n",
    "                # Denormalize. MEAN_MB, STD_MB (if noramlized)\n",
    "                # y_pred = y_pred * STD_MB + MEAN_MB\n",
    "                \"\"\"\n",
    "                2.Loss computation and other metrics\n",
    "                \"\"\"\n",
    "                predictions.append(y_pred)\n",
    "\n",
    "                ##Update all microbusiness_den isty column\n",
    "                row_id=test_dataloader.dataset.test_df.loc[i,\"row_id\"]\n",
    "                row_ids.append(row_id)\n",
    "\n",
    "                test_dataloader.dataset.main_df.loc[test_dataloader.dataset.main_df[\"row_id\"]==row_id,\"microbusiness_density\"]=y_pred\n",
    "\n",
    "\n",
    "        #Merge predictions\n",
    "        predictions=np.array(predictions)\n",
    "\n",
    "\n",
    "        #Update all microbusiness_denisty column\n",
    "\n",
    "        pred_test_df = pd.DataFrame(\n",
    "            {\n",
    "                \"row_id\":row_ids,\n",
    "                 \"microbusiness_density\":predictions}\n",
    "\n",
    "                                )\n",
    "        pred_test_df.to_csv(os.path.join(self.experiment_dir,\"submission.csv\"),index=False)\n",
    "\n",
    "        return pred_test_df"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Runner"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import dataclasses\n",
    "from dataclasses import dataclass\n",
    "import argparse\n",
    "import logging\n",
    "import os\n",
    "import pickle\n",
    "import torch.utils.data\n",
    "\n",
    "\n",
    "@dataclasses.dataclass\n",
    "class Arguments:\n",
    "    reset: bool = True\n",
    "    learning_rate: float = 0.001\n",
    "    nb_epochs: int = 120\n",
    "    model_name: str = None\n",
    "    num_workers: int = 0\n",
    "    batch_size: int = 1024\n",
    "    log_level: str = \"INFO\"\n",
    "    autorun_tb: bool = False\n",
    "    use_census: bool = True\n",
    "    seq_len: int = 10\n",
    "    seq_stride: int = 1\n",
    "    hidden_dim: int = 6\n",
    "    n_hidden_layers: int = 1\n",
    "    variante: int = 2\n",
    "    use_derivative: bool = True\n",
    "\n",
    "def cli():\n",
    "    \"\"\"\n",
    "   Parsing args\n",
    "   @return:\n",
    "   \"\"\"\n",
    "    return Arguments()\n",
    "\n",
    "def main(args):\n",
    "\n",
    "    #Format the model name\n",
    "\n",
    "    variante = f\"v{args.variante}\"\n",
    "    if args.model_name is None:\n",
    "        model_name = f\"lstm_{variante}_{'ae_' if args.use_census else ''}{'dv_' if args.use_derivative else ''}ehd.{args.hidden_dim}_nl.{args.n_hidden_layers}_sl.{args.seq_len}_ss.{args.seq_stride}_lr.{args.learning_rate}_bs.{args.batch_size}\"\n",
    "    else :\n",
    "        model_name=args.model_name\n",
    "\n",
    "\n",
    "    #Only variante 2 is till supported\n",
    "    if args.variante != 2:\n",
    "        raise NotImplementedError(\"Only variante 2 is supported\")\n",
    "\n",
    "    experiment_dir = os.path.join(EXPERIMENTS_DIR, model_name)\n",
    "\n",
    "\n",
    "    NetworkClass = None\n",
    "    if args.variante == 0:\n",
    "        NetworkClass = LstmPredictor\n",
    "    elif args.variante == 1:\n",
    "        NetworkClass = LstmPredictor2\n",
    "    elif args.variante == 2:\n",
    "        NetworkClass = LstmPredictorWithAttention\n",
    "\n",
    "    network = NetworkClass(experiment_dir=experiment_dir, use_derivative=args.use_derivative, hidden_dim=4, n_hidden_layers=1, use_census=args.use_census,reset= args.reset).to(DEVICE)\n",
    "\n",
    "    #Adam optimizer\n",
    "    optimizer = torch.optim.Adam(network.parameters(), lr=args.learning_rate)\n",
    "\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=int(520*512/args.batch_size), factor=0.5, verbose=True)\n",
    "\n",
    "    criterion= SmapeCriterion().to(DEVICE)\n",
    "\n",
    "\n",
    "    logging.info(\"Training : \"+model_name)\n",
    "    trainer = TrainerLstmPredictor(network,\n",
    "                      criterion,\n",
    "                      optimizer=optimizer,\n",
    "                      scheduler=scheduler,\n",
    "                      nb_epochs= args.nb_epochs,\n",
    "                      batch_size=args.batch_size,\n",
    "                      reset=args.reset,\n",
    "                      )\n",
    "\n",
    "    # Save  the dataset according to type, seq_len_stride and use_census: using pickle\n",
    "\n",
    "    if not os.path.exists(os.path.join(ROOT_DIR,\"dataset\",\"pickle\")):\n",
    "        os.makedirs(os.path.join(ROOT_DIR,\"dataset\",\"pickle\"))\n",
    "\n",
    "    datasets_pickle_path = os.path.join(ROOT_DIR,\"dataset\",\"pickle\",f\"all_dataset_{args.seq_len}_{args.seq_stride}_{args.use_census}.pickle\")\n",
    "\n",
    "\n",
    "    if not os.path.exists(datasets_pickle_path):\n",
    "        train_dataset = MicroDensityDataset(type=DatasetType.TRAIN, seq_len=args.seq_len, stride=args.seq_stride,\n",
    "                                            use_census=args.use_census)\n",
    "        val_dataset = MicroDensityDataset(type=DatasetType.VALID, seq_len=args.seq_len, stride=args.seq_stride,\n",
    "                                          use_census=args.use_census)\n",
    "\n",
    "        train_dataset.mix_with(val_dataset,size=0.8) #Mix train and val dataset to avoid disparity between the two in terms of dates distribution\n",
    "\n",
    "        test_dataset = MicroDensityDataset(type=DatasetType.TEST, seq_len=args.seq_len, stride=args.seq_stride,\n",
    "                                           use_census=args.use_census)\n",
    "\n",
    "        with open(datasets_pickle_path,\"wb\") as f:\n",
    "            logging.info(f\"Saving datasets to {datasets_pickle_path}\")\n",
    "            pickle.dump((train_dataset,val_dataset,test_dataset),f)\n",
    "    else:\n",
    "        with open(datasets_pickle_path,\"rb\") as f:\n",
    "            logging.info(f\"Loading datasets  from {datasets_pickle_path}\")\n",
    "            train_dataset,val_dataset,test_dataset = pickle.load(f)\n",
    "\n",
    "    logging.info(f\"Nb sequences : Train {len(train_dataset)} - Val {len(val_dataset)} - Test {len(test_dataset)}\")\n",
    "\n",
    "    train_dataloader=torch.utils.data.DataLoader(train_dataset,batch_size=args.batch_size,num_workers=args.num_workers,shuffle=True,drop_last=False)\n",
    "    val_dataloader = torch.utils.data.DataLoader(val_dataset, batch_size=args.batch_size,num_workers=0,drop_last=False)\n",
    "    test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=1,num_workers=0,drop_last=False,shuffle=False)\n",
    "\n",
    "    ##Train\n",
    "    trainer.fit(train_dataloader,val_dataloader)\n",
    "\n",
    "    ##Load best model\n",
    "    trainer.network.load_state(best=True)\n",
    "    trainer.run_test(test_dataloader=test_dataloader)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    args = cli()\n",
    "    setup_logger(args)\n",
    "    main(args)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}